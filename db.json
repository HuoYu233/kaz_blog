{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/bamboo1/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/css/animate.min.css","path":"css/animate.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/medias/logo.png","path":"medias/logo.png","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/activate-power-mode.js","path":"js/activate-power-mode.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/app.js","path":"js/app.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/goTop.js","path":"js/goTop.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/local_search.js","path":"js/local_search.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/jquery3.5.1.js","path":"js/jquery3.5.1.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/ribbon.min.js","path":"js/ribbon.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue2.6.11.js","path":"js/vue2.6.11.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/wrapImage.js","path":"js/wrapImage.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/medias/cursor/Horizontal.cur","path":"medias/cursor/Horizontal.cur","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/aplayer/APlayer@1.10.1.min.css","path":"js/aplayer/APlayer@1.10.1.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/bubble/bubble.js","path":"js/bubble/bubble.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/bubble/homeBubble.js","path":"js/bubble/homeBubble.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/clipboard/clipboard.min.js","path":"js/clipboard/clipboard.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/barrager.css","path":"js/danmu/barrager.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/close.png","path":"js/danmu/close.png","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/jquery.barrager.js","path":"js/danmu/jquery.barrager.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/falling/sakura.js","path":"js/falling/sakura.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/falling/snow.js","path":"js/falling/snow.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/clicklove.js","path":"js/cursor/clicklove.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/explosion.min.js","path":"js/cursor/explosion.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/fireworks.js","path":"js/cursor/fireworks.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/text.js","path":"js/cursor/text.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getPhotoOnline/index.js","path":"js/getPhotoOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getSiteOnline/index.js","path":"js/getSiteOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getTalkOnline/index.js","path":"js/getTalkOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/issues/index.js","path":"js/issues/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/pjax@0.2.8/index.js","path":"js/pjax@0.2.8/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/font.css","path":"js/shareJs/font.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/share.min.css","path":"js/shareJs/share.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/social-share.min.js","path":"js/shareJs/social-share.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.css","path":"js/tocbot/tocbot.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.min.js","path":"js/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-coy.min.css","path":"js/prism/prism-coy.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-funky.min.css","path":"js/prism/prism-funky.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-dark.min.css","path":"js/prism/prism-dark.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-line-numbers.css","path":"js/prism/prism-line-numbers.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-okaidia.min.css","path":"js/prism/prism-okaidia.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-solarizedlight.min.css","path":"js/prism/prism-solarizedlight.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism.min.css","path":"js/prism/prism.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/utils/index.js","path":"js/utils/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/valine/index.js","path":"js/valine/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-tomorrow.min.css","path":"js/prism/prism-tomorrow.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-twilight.min.css","path":"js/prism/prism-twilight.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper.animate1.0.3.min.js","path":"js/swiper/swiper.animate1.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper.min.js","path":"js/swiper/swiper.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-seamless-scroll/index.js","path":"js/vue-seamless-scroll/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper@5.4.1.min.css","path":"js/swiper/swiper@5.4.1.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.css","path":"js/vue-typed-js/index.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.js","path":"js/vue-typed-js/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/vue-awesome-swiper.js","path":"js/swiper/vue-awesome-swiper.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/waline/waline.min.js","path":"js/waline/waline.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.eot","path":"js/shareJs/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.svg","path":"js/shareJs/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.ttf","path":"js/shareJs/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.woff","path":"js/shareJs/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"source/img/Kaz.jpg","path":"img/Kaz.jpg","modified":1,"renderable":0},{"_id":"source/img/bg.jpg","path":"img/bg.jpg","modified":1,"renderable":0},{"_id":"source/img/favicon.ico","path":"img/favicon.ico","modified":1,"renderable":0},{"_id":"source/img/transformer-notes/pic-1.png","path":"img/transformer-notes/pic-1.png","modified":1,"renderable":0},{"_id":"source/img/transformer-notes/pic-2.png","path":"img/transformer-notes/pic-2.png","modified":1,"renderable":0},{"_id":"source/img/hello-world/Kaz.jpg","path":"img/hello-world/Kaz.jpg","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-1.png","path":"img/machine-learning-notes/pic-1.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-11.png","path":"img/machine-learning-notes/pic-11.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-10.png","path":"img/machine-learning-notes/pic-10.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-12.png","path":"img/machine-learning-notes/pic-12.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-13.png","path":"img/machine-learning-notes/pic-13.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-14.png","path":"img/machine-learning-notes/pic-14.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-15.png","path":"img/machine-learning-notes/pic-15.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-16.png","path":"img/machine-learning-notes/pic-16.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-17.png","path":"img/machine-learning-notes/pic-17.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-18.png","path":"img/machine-learning-notes/pic-18.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-19.png","path":"img/machine-learning-notes/pic-19.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-2.png","path":"img/machine-learning-notes/pic-2.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-20.png","path":"img/machine-learning-notes/pic-20.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-21.png","path":"img/machine-learning-notes/pic-21.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-22.png","path":"img/machine-learning-notes/pic-22.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-24.png","path":"img/machine-learning-notes/pic-24.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-23.png","path":"img/machine-learning-notes/pic-23.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-26.png","path":"img/machine-learning-notes/pic-26.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-25.png","path":"img/machine-learning-notes/pic-25.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-27.png","path":"img/machine-learning-notes/pic-27.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-3.png","path":"img/machine-learning-notes/pic-3.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-28.png","path":"img/machine-learning-notes/pic-28.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-29.png","path":"img/machine-learning-notes/pic-29.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-30.png","path":"img/machine-learning-notes/pic-30.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-31.png","path":"img/machine-learning-notes/pic-31.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-33.png","path":"img/machine-learning-notes/pic-33.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-32.png","path":"img/machine-learning-notes/pic-32.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-35.png","path":"img/machine-learning-notes/pic-35.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-34.png","path":"img/machine-learning-notes/pic-34.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-37.png","path":"img/machine-learning-notes/pic-37.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-36.png","path":"img/machine-learning-notes/pic-36.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-38.png","path":"img/machine-learning-notes/pic-38.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-4.png","path":"img/machine-learning-notes/pic-4.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-7.jpg","path":"img/machine-learning-notes/pic-7.jpg","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-8.png","path":"img/machine-learning-notes/pic-8.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-9.png","path":"img/machine-learning-notes/pic-9.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-6.png","path":"img/machine-learning-notes/pic-6.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-5.png","path":"img/machine-learning-notes/pic-5.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/_posts/algorithm-basic.md","hash":"77812d87aa99ed356be433e28f989246b78fa721","modified":1740711826487},{"_id":"source/_posts/algorithm-data-structure.md","hash":"7086b1498dae97bf727896740ec9c29317e08af8","modified":1740033940656},{"_id":"source/_posts/ai4chem.md","hash":"d787b2f0f08d544a5a87a4cfeb4898216b25d2bb","modified":1742733082540},{"_id":"source/_posts/algorithm-dp.md","hash":"7b820bff87b39bd8ae211d74d1e70188aad9465e","modified":1740711998010},{"_id":"source/_posts/algorithm-graph.md","hash":"f2fb24bafef83ec8c1e3a3a084a0bac8c22c04f7","modified":1740712608846},{"_id":"source/_posts/algorithm-math.md","hash":"b8837cdd921be28cf6b39e68c4ce91b71462487a","modified":1741144378052},{"_id":"source/_posts/algorithm-search.md","hash":"f64e7202a8da9c39428826985838e59d0622e246","modified":1740712354855},{"_id":"source/_posts/algorithm-skills.md","hash":"cd89781466ab14e056400ee5153b05b29c0435ed","modified":1740711625621},{"_id":"source/_posts/learning-cpp-notes.md","hash":"356e332d1caf71e006ea1877020b8d683c3632f6","modified":1745855383357},{"_id":"source/_posts/dive-into-deep-learning.md","hash":"f0415b92f350fbb7dc3a91611bff4f546cd60131","modified":1740998538757},{"_id":"source/_posts/hello-world.md","hash":"662795e1685a6493db59dd5eafab94cdf24dd0ff","modified":1739795982971},{"_id":"source/log/index.md","hash":"5deeb5b2f9716755f91cfcc3e6dda56bb45e951d","modified":1740672551623},{"_id":"source/_posts/machine-learning-notes.md","hash":"94148eaa455f7f7bc90b10dff3aa48e232911498","modified":1740711414489},{"_id":"source/_posts/transformer-notes.md","hash":"c76cc8327ce2fab6ed61cf8ae02fa971bf72af2d","modified":1742023593982},{"_id":"source/img/favicon.ico","hash":"a3a1b2c8f5bcd3faf1871d57d7923d6b45f35d34","modified":1737556566666},{"_id":"source/about/index.md","hash":"0442aeb1ab3bf797155322510a9917f81128b6aa","modified":1745572219273},{"_id":"source/img/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1739458454213},{"_id":"source/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1739622620308},{"_id":"source/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1738425420819},{"_id":"source/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1740055846274},{"_id":"source/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1738425452378},{"_id":"source/_posts/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/_posts/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1739458454213},{"_id":"source/_posts/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1739622620308},{"_id":"source/_posts/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1738425420819},{"_id":"source/_posts/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1740055846274},{"_id":"source/_posts/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1738425452378},{"_id":"source/img/transformer-notes/pic-2.png","hash":"0df6ba77725900dcc7a9ad3e39790f0b8ecc967c","modified":1741693804900},{"_id":"source/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1739458598505},{"_id":"source/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1739461102564},{"_id":"source/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1739537460909},{"_id":"source/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1739619946353},{"_id":"source/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1740144875402},{"_id":"source/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1740143318932},{"_id":"source/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1738564459827},{"_id":"source/_posts/img/transformer-notes/pic-2.png","hash":"0df6ba77725900dcc7a9ad3e39790f0b8ecc967c","modified":1741693804900},{"_id":"source/_posts/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1739458598505},{"_id":"source/_posts/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1739461102564},{"_id":"source/_posts/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1739537460909},{"_id":"source/_posts/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1739619946353},{"_id":"source/_posts/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1740143318932},{"_id":"source/_posts/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1740144875402},{"_id":"source/_posts/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1738564459827},{"_id":"source/img/transformer-notes/pic-1.png","hash":"f6d66e79b8bade7a2b5366ce7fd9a7db04707af1","modified":1741692757020},{"_id":"source/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1739456947233},{"_id":"source/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1739453444035},{"_id":"source/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1739458743567},{"_id":"source/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1739459181242},{"_id":"source/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1739535680777},{"_id":"source/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1739542099791},{"_id":"source/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1739624989401},{"_id":"source/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1739888230194},{"_id":"source/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1739946176241},{"_id":"source/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1739947190822},{"_id":"source/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1738910656446},{"_id":"source/_posts/img/transformer-notes/pic-1.png","hash":"f6d66e79b8bade7a2b5366ce7fd9a7db04707af1","modified":1741692757020},{"_id":"source/_posts/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1739456947233},{"_id":"source/_posts/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1739453444035},{"_id":"source/_posts/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1739458743567},{"_id":"source/_posts/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1739459181242},{"_id":"source/_posts/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1739535680777},{"_id":"source/_posts/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1739542099791},{"_id":"source/_posts/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1739624989401},{"_id":"source/_posts/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1739888230194},{"_id":"source/_posts/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1739946176241},{"_id":"source/_posts/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1739947190822},{"_id":"source/_posts/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1738910656446},{"_id":"source/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1739449524427},{"_id":"source/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1737897790662},{"_id":"source/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1739541957085},{"_id":"source/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1739800893470},{"_id":"source/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1739624877518},{"_id":"source/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1738564587991},{"_id":"source/_posts/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1739449524427},{"_id":"source/_posts/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1737897790662},{"_id":"source/_posts/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1739541957085},{"_id":"source/_posts/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1739624877518},{"_id":"source/_posts/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1739800893470},{"_id":"source/_posts/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1738564587991},{"_id":"themes/bamboo1/source/css/_tag/ghcard.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1740671571328},{"_id":"source/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1739023761909},{"_id":"source/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1739453338824},{"_id":"source/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1739455810944},{"_id":"source/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1739543974084},{"_id":"source/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1740307931753},{"_id":"source/_posts/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1739023761909},{"_id":"source/_posts/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1739453338824},{"_id":"source/_posts/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1739455810944},{"_id":"source/_posts/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1739543974084},{"_id":"source/_posts/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1740307931753},{"_id":"themes/bamboo1/package.json","hash":"1db58d63b57e63d88e0a060e5b164db225cf2599","modified":1740671571298},{"_id":"themes/bamboo1/_config.yml","hash":"12b2d30c750c6d956262964d469bfeb43a19d54d","modified":1740672984347},{"_id":"themes/bamboo1/source/favicon.ico","hash":"801ff7b3f358b77a813787a97ef59148eec93fd8","modified":1740671571332},{"_id":"themes/bamboo1/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1740671571295},{"_id":"themes/bamboo1/README.md","hash":"2b81d4346abf63bd58483420ea54220c8d1101b3","modified":1740671571262},{"_id":"themes/bamboo1/layout/categories.ejs","hash":"b0f71816ca4c0899eb82b6fa100abf91b56508ab","modified":1740671571295},{"_id":"themes/bamboo1/LICENSE","hash":"2f9d4d3c41f055757f8c86567cfe838846446e7b","modified":1740671571262},{"_id":"themes/bamboo1/languages/zh-CN.yml","hash":"2bf66fefa219ee8152d35bb5f823ca5096fedcf2","modified":1740671571264},{"_id":"themes/bamboo1/languages/zh-TW.yml","hash":"bd3ef201b7dcbeeee54107301550d60e71d72ba7","modified":1740671571264},{"_id":"themes/bamboo1/languages/default.yml","hash":"4c604dc1344630ae5ab50edc282a3e46982884c1","modified":1740671571263},{"_id":"themes/bamboo1/layout/post.ejs","hash":"206c60b92ec1ce6332e75e85761ce0c6947f5dae","modified":1740671571297},{"_id":"themes/bamboo1/layout/tags.ejs","hash":"0c6a171fa56cf8bfa180db32f10f75c6760fd983","modified":1740671571297},{"_id":"themes/bamboo1/scripts/events/index.js","hash":"514fb117a0c526de85c0338de7f66c23abc58b48","modified":1740671571298},{"_id":"themes/bamboo1/scripts/helpers/side_archives.js","hash":"a292f0a9e9242556b83219f519e3e92a4d85e904","modified":1740671571299},{"_id":"themes/bamboo1/layout/tag.ejs","hash":"1061f8a9b51d983590a3efc536142a9b10acebf5","modified":1740671571297},{"_id":"themes/bamboo1/layout/category.ejs","hash":"d0d19ac565414123c24b312f7158dbe1c9e275f8","modified":1740671571296},{"_id":"themes/bamboo1/scripts/tag/btn.js","hash":"0e628fa28e03f60e28f257af895b2e72a0cb8449","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/btns.js","hash":"618e2f77ec244d8814f2b38c9820d1356580bbcd","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/checkbox.js","hash":"49da9041bd41c57a547d42fb7a6741757b848f1c","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/folding.js","hash":"832c55a45cfeeabcd2d317d42faaee09ee54d2a4","modified":1740671571301},{"_id":"themes/bamboo1/scripts/tag/file.js","hash":"260333b277073ba8f41472cdddb35ee3e8212267","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/getPhoto.js","hash":"e78765e6156ff261e564d8a22c6307ea98990a0d","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/gallery.js","hash":"694a6a81dd3b3aa4a37e39b35402e99322941ec1","modified":1740671571301},{"_id":"themes/bamboo1/layout/index.ejs","hash":"fea918c473fe66846b7a7f94da4617610cff3d07","modified":1740671571297},{"_id":"themes/bamboo1/layout/layout.ejs","hash":"a7dbe3f3f900e30c49f02b4d3a99803a4aba59ec","modified":1740671571297},{"_id":"themes/bamboo1/scripts/tag/getPhotoOnline.js","hash":"16478a1a0d642b92cc4f86114d185bf79cbd0bf9","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/getSiteOnline.js","hash":"2dce91bf40a1e6f856e317eba777fc29399ec2fc","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/ghcard.js","hash":"4e893d79abc1e8e1e5b3bfe08249ff32b250314d","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/image.js","hash":"faa1d83114bc255cffc18bd0ab037f08b430f515","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/getTalkOnline.js","hash":"30f63677757c051835fec668ec928bcd47f6ba66","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/inline-labels.js","hash":"eaaedc3d65384e0beb4306534ef4ed202b46da18","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/issues.js","hash":"7dcb40af462e4131f6a52d354ed3b147b4e874af","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/media.js","hash":"1d163ee349818baeb95504f82d3497da6f6556e2","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/link.js","hash":"a11fe06f20669f4b64a1a0dcc9f005a9f32e29dc","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/mermaid.js","hash":"1e69a5e4a4a5f88fdb76d0fe55ea651c14301816","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/note.js","hash":"9e990caa1fd815a760e31f1eaa02015d357fcef8","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/site.js","hash":"1cb487b1435925a55eaf957d761bc08254092c36","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/span.js","hash":"d617b5a0056c4a0c983225513c89eed6f5b56833","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/progress.js","hash":"99a10305e3924aaab05135ef25afd10d04574bfe","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/swiper.js","hash":"26a587371f7d2f6715cdb0e5f4f7b63a7f7921cd","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/tabs.js","hash":"133310460bdf70a7932b44c3ccca509b3f221e1c","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/timeline.js","hash":"da2b0d7760dea698429f370aba5cded5bb24501e","modified":1740671571306},{"_id":"themes/bamboo1/scripts/z-lazyload/index.js","hash":"58b935fb699a98f0a9ceb741d2105a977e24cf59","modified":1740671571307},{"_id":"themes/bamboo1/scripts/tag/title.js","hash":"8cfce58425366f805a5f2c88f01b76dca44f91ce","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/titleB.js","hash":"3dde507bf20477cd89e71549be8ddfc4964a76ed","modified":1740671571307},{"_id":"themes/bamboo1/source/js/activate-power-mode.js","hash":"2e14b0f48c55eaec543d96ec0eb2f16e80c20c01","modified":1740671571332},{"_id":"themes/bamboo1/source/js/app.js","hash":"57f824da5f893a0c83c80522114797f09868ccf1","modified":1740671571333},{"_id":"themes/bamboo1/source/css/style.styl","hash":"29848c643d866b6b3ae76bd7d4238c3ab7343618","modified":1740671571332},{"_id":"themes/bamboo1/source/medias/logo.png","hash":"d08165f945567a08bd74d36b1241a0b8f1618536","modified":1740671571358},{"_id":"themes/bamboo1/source/css/animate.min.css","hash":"dc47ce9b8438909921b14e766febdabf3018e3c2","modified":1740671571331},{"_id":"themes/bamboo1/source/js/goTop.js","hash":"ae548538475ddea2aae8949194935582cc0ae972","modified":1740671571340},{"_id":"themes/bamboo1/source/js/ribbon.min.js","hash":"e6136a6243e04faca95844f47c21b070ade3661a","modified":1740671571345},{"_id":"themes/bamboo1/source/js/local_search.js","hash":"131d74198aa41bdb74dc27ef3ed856bc3d752f8d","modified":1740671571342},{"_id":"themes/bamboo1/layout/_partial/dark.ejs","hash":"774216ad95828bc349310b4cad02660018ca06e9","modified":1740671571274},{"_id":"themes/bamboo1/source/js/wrapImage.js","hash":"08fa22ecfb8a93bdb96e1063e37367fcc97be29c","modified":1740671571357},{"_id":"themes/bamboo1/layout/_partial/archive.ejs","hash":"246967d57c31bd873f98603536df79c1f67c96e2","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/goTop.ejs","hash":"fa802c5b70f3ac8cf98dad040d05867891d7d4ff","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/loaded.ejs","hash":"8e29d9924d2ee35c3da7c2f3e27652112252a185","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/motto.ejs","hash":"535af08125435651591be103f8e6d98c7222907d","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/lantern.ejs","hash":"225044aa82bf305e27a00adf1a8368146e860394","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/home_widget.ejs","hash":"0882e6992cfc2fbf1e0b05acbc1463de68f77aa4","modified":1740671571277},{"_id":"themes/bamboo1/scripts/events/lib/stellar-tag-utils.js","hash":"315d9e8a8261e760e1001970e09c32a660c969e0","modified":1740671571299},{"_id":"themes/bamboo1/layout/_partial/notice.ejs","hash":"ed4cad963e1a9b747864bd3ceb76bf9e763c1150","modified":1740671571280},{"_id":"themes/bamboo1/scripts/z-lazyload/lib/process.js","hash":"48a29bdb7026c4a9c8a58190d044140a8a05a64c","modified":1740671571308},{"_id":"themes/bamboo1/layout/_partial/paginator.ejs","hash":"26655627ce5b1eb7050b5e24cc262cf3fc46c400","modified":1740671571280},{"_id":"themes/bamboo1/layout/_partial/topArticle.ejs","hash":"9b321c75dbcbc424b2392e90426127182539a86d","modified":1740671571295},{"_id":"themes/bamboo1/layout/_partial/swiper.ejs","hash":"767c0fcae79bf94fa718ba26ea4ad1fdb610fe10","modified":1740671571295},{"_id":"themes/bamboo1/source/css/_plugins/mathjax.styl","hash":"499f59db53e9c57d99bebe4722156aeca7adb8b7","modified":1740671571321},{"_id":"themes/bamboo1/layout/_partial/side.ejs","hash":"17f52c2fa6d771da94644d4fd0983d054a606384","modified":1740671571292},{"_id":"themes/bamboo1/source/css/_plugins/pjaxanimate.styl","hash":"f8c2d14c041bb87bc7f37d82ac939320e3d110bf","modified":1740671571321},{"_id":"themes/bamboo1/source/css/_defines/variable.styl","hash":"ff4fff39224138f8cce1fe82dd8e0ab4077804b3","modified":1740671571308},{"_id":"themes/bamboo1/source/medias/cursor/Horizontal.cur","hash":"c3c5e8485a67b7ab16079a96b53aff7ff52de756","modified":1740671571357},{"_id":"themes/bamboo1/source/js/aplayer/APlayer@1.10.1.min.css","hash":"7f4f8913f2d46ade2def5134e2cc8684a4b87939","modified":1740671571333},{"_id":"themes/bamboo1/source/css/_partial/about.styl","hash":"0673f15fbb3649e221da3b20ba091d03bbd1cc3e","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/base.styl","hash":"e3c5b4f828552be20a6f773f6a8c242d58a2dc23","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/archive.styl","hash":"caf5c83ba9897644582e60e29770f1bb7362ad5a","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/categories.styl","hash":"ad6d70243be366677293ff88c2eec715d2c29e9e","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/category.styl","hash":"327d6d1f71d782f69fe0b365137b0abc331ca3bb","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/copyRyght.styl","hash":"ed1377ceb86204fa6b6c7430d14a1366d9ca568e","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/custom.styl","hash":"badd12c63cb9bb5f38c829f00be2509fb546e2cd","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/comment.styl","hash":"6d70e8ec7481d11558b430ed3bbf437805b42bc0","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/danmu.styl","hash":"8aaa764bb2b1c6a49c2f6c9ee868da24a0359669","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/donate.styl","hash":"a880996ca61f96ba1280d581a132deb924c4ff62","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/footer.styl","hash":"598d193754645b22a0f1406303c1df66d95ffd9d","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/drawer.styl","hash":"0498b0cf2819b681eeeec35193e491d1d039302d","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/dark.styl","hash":"28d5269fae1cbaec4f40c21daa0378c098c7d801","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/goTop.styl","hash":"08c3dc03570ca3738f18b99ebe95c79ec3d0ce0a","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/friends.styl","hash":"f7018f99210ccab74b2d315a55ba9c4350a12fc9","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/highlight.styl","hash":"55fc39472aba296434fab0ffdc6be5684f01778a","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/header.styl","hash":"db725d2648fc740aa59360502628e6de959504a6","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/home.styl","hash":"0bd1214d90fcf7ae020ee0673da4a9fd7c225274","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/lantern.styl","hash":"04acde311d7b9f7a732340626dbe677814ab502f","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/motto.styl","hash":"cb484d25bc6f0bcda4cadffb5f1cdbe5df93919e","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/pace.styl","hash":"b666b9079262d2dcc2a7b6023f97f79c8535db0e","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/notice.styl","hash":"8fd57b791e518c14e88c36510e1132c10288b86b","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/paginator.styl","hash":"df1fd26976fd5be8418cd49a5c65ec651a680496","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/post.styl","hash":"32655204cccbf9861097efad65c1cf69dac11fd5","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/post-nav.styl","hash":"a7c7b33ad813885af485815d8c787c0fb3b6c8c8","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/posts.styl","hash":"9464726d962229149d38841c087f6207cd8c2adc","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/post-detail-header.styl","hash":"b316f4bcb9964bbfa4f6829e550578aa27d509a8","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/topArticle.styl","hash":"910e24383b1009e27c0ebf26a5958051451da47c","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_partial/transition.styl","hash":"809b40b7214cda6691b2f22ae827cbdbfaf8c303","modified":1740671571321},{"_id":"themes/bamboo1/source/css/_partial/search.styl","hash":"2f67103cd8cb9b92d1ca4f334e41c195e01c3ce3","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_partial/side.styl","hash":"2999120872bff96f84381f76ad0a65015bd3f549","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_tag/checkbox.styl","hash":"dbc18a5685879493b06016c85993d4522fe48564","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_partial/tag.styl","hash":"f2b741dddc033f1989d3c4710f339ee122900e58","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_tag/btn.styl","hash":"dbba1c1f7d374bd7c69c5b9758a61371db334d87","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_partial/tags.styl","hash":"6fc3915d4a0f5d551b23f2281df868e0399bc13d","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_tag/circle.styl","hash":"c2adc73eab52952140420c2b5fc8bf134432b695","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_tag/folding.styl","hash":"7a88c350d302c6a89ab008b6ce2a98ed6f19c007","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/gallery.styl","hash":"440ae8b7cd2802e068a82e044c35a3273eb98668","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/galleryGroup.styl","hash":"ea9f387bc1bc00b4d6d4bd34e2df9046bda3610a","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/inline-label.styl","hash":"1903a258c5829c8370c4eb53fcb60df7f7921f08","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/image.styl","hash":"ce0c9f758f0f0be385c38d65e9bf4fb708cbaf5c","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/link.styl","hash":"7181435bed445840bb61d655451494f83ac4d7e9","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/media.styl","hash":"6727008f95ad9b3146c609a2e890af009472f9e4","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/note.styl","hash":"4487702c5348bf691e329fa8a9bbb6f42808436f","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/span.styl","hash":"bede49e1edf1049d4ea2f3dd0a17787fe084b2d2","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/site-card.styl","hash":"ee95cbf6072dbe3ae11e6f73a3b38a9c09e31994","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/progress.styl","hash":"de1e1b08d23f95493ffda2a5375888e9e678891b","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/timeline.styl","hash":"ae8e4487a32606127d26dc27c74df592b2175f82","modified":1740671571331},{"_id":"themes/bamboo1/source/css/_tag/tabs.styl","hash":"ca2dac222da40e13aa3b117d55b2da74d7ce9a35","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/title.styl","hash":"e17ce9da2937d314c71b459c43de5f01441fe421","modified":1740671571331},{"_id":"themes/bamboo1/source/js/bubble/homeBubble.js","hash":"8475e7ed2004b9791b3f7ad4162b7a2b89467874","modified":1740671571334},{"_id":"themes/bamboo1/source/js/bubble/bubble.js","hash":"57f116efe2418a389913a46909e018fa4c9b9e84","modified":1740671571334},{"_id":"themes/bamboo1/source/js/danmu/barrager.css","hash":"9de985f20d314f3f1182f30d1b0666e5eb9ca9b5","modified":1740671571337},{"_id":"themes/bamboo1/source/js/clipboard/clipboard.min.js","hash":"76fd19c15b1d0a2d7afc7b66ca5f80c9061aabe2","modified":1740671571335},{"_id":"themes/bamboo1/source/js/danmu/jquery.barrager.js","hash":"72ec0d8bbd0811973152fcbb316b0dd839ffb8f3","modified":1740671571337},{"_id":"themes/bamboo1/source/js/danmu/close.png","hash":"2c3ed4345f91dc1b74a57b6dcd1e1efa9e279dbb","modified":1740671571337},{"_id":"themes/bamboo1/source/js/falling/snow.js","hash":"99222d79ff36b05200b3ff7f54f8209d8f0a364b","modified":1740671571338},{"_id":"themes/bamboo1/source/js/cursor/explosion.min.js","hash":"ed2d0a5ad306a2745b7c8180b69e36b78d4b0698","modified":1740671571336},{"_id":"themes/bamboo1/source/js/falling/sakura.js","hash":"b1566483a7d0deda2dd35db3d5a46f13aa5f1a86","modified":1740671571338},{"_id":"themes/bamboo1/source/js/cursor/fireworks.js","hash":"86ad9484e40268952b5e32c240fb04d0268f86dd","modified":1740671571336},{"_id":"themes/bamboo1/source/js/cursor/text.js","hash":"7dd898cb00b46ceda065c92f2ac092c4ef41b4e4","modified":1740671571336},{"_id":"themes/bamboo1/source/js/cursor/clicklove.js","hash":"9e8e79d69ad8338761272f86fe5cad0ad5e503cc","modified":1740671571335},{"_id":"themes/bamboo1/source/js/getPhotoOnline/index.js","hash":"3b6354c11105aba544b08ded11295d83219d59ec","modified":1740671571339},{"_id":"themes/bamboo1/source/js/getTalkOnline/index.js","hash":"ba714c7ffe4abde553d0c54ce5d528453f279c06","modified":1740671571340},{"_id":"themes/bamboo1/source/css/_tag/swiper.styl","hash":"ff02b78ba54cb71eafad141c3e4ef4a9cd9085cd","modified":1740671571330},{"_id":"themes/bamboo1/source/js/issues/index.js","hash":"f02538ab609541489396a682879ce854519487ca","modified":1740671571340},{"_id":"themes/bamboo1/source/js/getSiteOnline/index.js","hash":"733f75aff00e0a62013089cb3e869878d6fcc535","modified":1740671571339},{"_id":"themes/bamboo1/source/css/_tag/talkByJson.styl","hash":"2affd0fe4a640ab92c07198cd4df13bef1ea1575","modified":1740671571331},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.css","hash":"45e469dffa7b9ebc03f99fd09fb97274cdc5e9b4","modified":1740671571350},{"_id":"themes/bamboo1/source/js/shareJs/font.css","hash":"f6407017418989fb0ced993509543fb07c6b0b33","modified":1740671571346},{"_id":"themes/bamboo1/source/js/shareJs/share.min.css","hash":"9bd0cd6c81b60e10085cdda6aa724f147ee76599","modified":1740671571348},{"_id":"themes/bamboo1/source/js/pjax@0.2.8/index.js","hash":"efb9166635c18f09f2c7604a8b15d6ac8aae4870","modified":1740671571342},{"_id":"themes/bamboo1/source/js/shareJs/social-share.min.js","hash":"efdfa6b695ac6f0dd04cd8153d3e3a1a1edd90c2","modified":1740671571348},{"_id":"themes/bamboo1/source/js/prism/prism-funky.min.css","hash":"0220f68ccda78c2b5d1109e58f3879674c93b587","modified":1740671571344},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.min.js","hash":"bc45d3586a21f7e364cd6efe58844932c00cf11c","modified":1740671571351},{"_id":"themes/bamboo1/source/js/prism/prism-coy.min.css","hash":"fe1246de39c25eaa7ad1b0c997ee530dbdd39ad8","modified":1740671571343},{"_id":"themes/bamboo1/source/js/prism/prism-dark.min.css","hash":"a3f604a19e9a46f83a2fde49dfb45782748957ca","modified":1740671571343},{"_id":"themes/bamboo1/source/js/prism/prism-line-numbers.css","hash":"c42732535ac61ac59a4356af3d89186a3071edf1","modified":1740671571344},{"_id":"themes/bamboo1/source/js/prism/prism-okaidia.min.css","hash":"50be6cc15d883ff3fa5d0885fed47241695a986c","modified":1740671571344},{"_id":"themes/bamboo1/source/js/utils/index.js","hash":"fcea598ed253006d79f78d34cc36fdc6649639f3","modified":1740671571351},{"_id":"themes/bamboo1/source/js/prism/prism.min.css","hash":"aa405e2bcb571595c822a80f5482454c1536fa52","modified":1740671571345},{"_id":"themes/bamboo1/source/js/prism/prism-solarizedlight.min.css","hash":"927b757cd8030d12953b5c0fa6eed5de15dda8ad","modified":1740671571344},{"_id":"themes/bamboo1/source/js/swiper/swiper.animate1.0.3.min.js","hash":"6a8d6aa926e552a563356c36d52d1e0e0c83521e","modified":1740671571349},{"_id":"themes/bamboo1/source/js/prism/prism-tomorrow.min.css","hash":"7b4247bc4d3b719afe5957779d0e5c8fb716c8ea","modified":1740671571345},{"_id":"themes/bamboo1/source/js/prism/prism-twilight.min.css","hash":"ff4a6e3c4f1cb9bb59ec061656eacb750d238c15","modified":1740671571345},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.css","hash":"b9dac4cfc5f0dc8854393d670b525fb63092fd38","modified":1740671571353},{"_id":"themes/bamboo1/source/js/vue-seamless-scroll/index.js","hash":"f2aaf3f9b1ab7362f7cc158e5360cb1d62a57172","modified":1740671571353},{"_id":"themes/bamboo1/source/js/swiper/swiper@5.4.1.min.css","hash":"fd618d2bdf929821d9fa70ae377b840ffc47d756","modified":1740671571350},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.js","hash":"c8e6f4510eb5fe55015401510ce03f5307556b1a","modified":1740671571354},{"_id":"themes/bamboo1/layout/_partial/analytics/baidu-analytics.ejs","hash":"589ac42358e8f2e4b22aac4353caa0c2c462a332","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/analytics/google-analytics.ejs","hash":"cb7d5c76508fe8db43dbd4af9a691398fffccadb","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/analytics/baidu-push.ejs","hash":"e7fcc44a10565505e85417ad4416d0d5d5839523","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/footer/busuanzi.ejs","hash":"bc3d2f6abc95b329dfe0186fa0364c48aab3772e","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/card/post.ejs","hash":"37fd6f4443620ff3b2963c19fc42bd21891428b0","modified":1740671571267},{"_id":"themes/bamboo1/source/js/swiper/vue-awesome-swiper.js","hash":"e6f36537ed091a6b69945b1acf49e426426f1cf0","modified":1740671571350},{"_id":"themes/bamboo1/layout/_partial/footer/footer.ejs","hash":"3ed9b13b849c5a54dc667b9137beaa769d382980","modified":1740671571275},{"_id":"themes/bamboo1/layout/_partial/math/mermaid.ejs","hash":"2c6894abc259167170e274728467c7c7aa1ef8e5","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/meta/aplayer.ejs","hash":"33aeb95a90093ce66816676f9ac63f1f02d27852","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/head/drawer.ejs","hash":"1f78c957b472a14c13301ea6a7ec59d5ecc777e6","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/math/mathjax.ejs","hash":"dc9a1270d34448606e87e52a3b003a89f4f5b3aa","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/footer/fish.ejs","hash":"c90e55153d7e83f5c054dd98add7dd9bbb6e095f","modified":1740671571275},{"_id":"themes/bamboo1/layout/_partial/pjax/animate.ejs","hash":"8a60b1b8ec8340f712ed6539bacba62d137232cc","modified":1740671571280},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_2.ejs","hash":"d1df3a9050bae7cb7cf17d44359292999a1d0664","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/pjax/index.ejs","hash":"6ac774c816f9dcb7099612e2ef13bb0e7893476c","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_3.ejs","hash":"1ef0876d3a2f3ae43eccdf3c88dbd9c642d22d62","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_1.ejs","hash":"e82a5a888ba376080b21d4e39ac9b4fff2623d24","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/head/head.ejs","hash":"aea1a32f47edda23b22b4e3444c13cedb4f3e5a6","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/head/header.ejs","hash":"0583e814ef8af4cce35382a2b2634488e432de75","modified":1740671571277},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_7.ejs","hash":"3b6d1da24786682b5d248f5e59f8f48510a55d4d","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_6.ejs","hash":"7ce6bd6bf765d23acb30f1a054e56f265099a4dd","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_5.ejs","hash":"7db609c64eeabb8b68771097663d1ff427667e14","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_4.ejs","hash":"458f5c179670b029178c037f1feb99f566b9408e","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_8.ejs","hash":"2ab0a97498e0f35516c1db65949afd831a2800da","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/scripts/danmu.ejs","hash":"ca20ae64fbd2527f8b54fc8618a3a0502728420c","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/scripts/copy.ejs","hash":"fda84bf47a5e7c5692f682a45f8fdcfab90900b0","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/scripts/dark.ejs","hash":"25558a06394cb251a140b5a8998069f9f776f67e","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/cursor_effect.ejs","hash":"0877a20709f046f693e4536fe70354ad1b67f195","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/scripts/getSiteOnline.ejs","hash":"7735a6b702c32447e4667cbc7cab86cd28d5badf","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/getPhotoOnline.ejs","hash":"bd17b91d841055cef0fe7e4a70736f57a55e030b","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/getTalkOnline.ejs","hash":"5e5806d276dcfad3702d460d43c4ee826d9ce9ea","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/head.ejs","hash":"4536885880315fb90efd75935c133051a729d45c","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/head/search.ejs","hash":"7cbf73c577874de0b6cc89180680b1e19c5e8348","modified":1740671571277},{"_id":"themes/bamboo1/layout/_partial/scripts/inputEffects.ejs","hash":"765c69436e021412dd8bbb852ea2403c97fc6adf","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/falling.ejs","hash":"79d1c7cc0de290ac35e41312ef62fc4bf04be766","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/issues.ejs","hash":"cf5436f10fb9a2fb7238a1c528a5ed64a2345840","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/global.ejs","hash":"16851de5516d2908787ed902b778c8f03f9ae63b","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/index.ejs","hash":"00a70fce835bc65801fd407236f72315e6df5934","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/lazyload.ejs","hash":"b7e8071598a8dc70406aba8da949c5d6f5403e47","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/typed.ejs","hash":"262725f946bb3ae8957b8e3d50c1a7cd564f8866","modified":1740671571292},{"_id":"themes/bamboo1/layout/_partial/scripts/swiperTag.ejs","hash":"020d6cd78cd9ace5477a79e57e958a2f5a6d43f3","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/post/bgSwiper.ejs","hash":"77ade59920c57fda25c2be421b77adf7b0c943d9","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/scripts/toc.ejs","hash":"af49e146385d28f5cbdaa6d7a5167fbbde25a917","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/scripts/setHeader.ejs","hash":"c082c910282c3c96465b0812222874fbe87c58cd","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/post/categories.ejs","hash":"15f33099ef5f653b9ceb3e27f089b36bff50cc4f","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/post/copyright.ejs","hash":"fd3af5c33895f907b1e5daa56d8d7266549dd019","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/comment.ejs","hash":"364875140582c1b702e01b63791f97011ca24b40","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/scripts/scrollreveal.ejs","hash":"4aba6d5b506b3a8fc7d84abe0d42875e0107d64e","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/post/post-detail-header.ejs","hash":"34ded5a37233689e991bb7292dc785d626430106","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/donate.ejs","hash":"b54c1be4bf9a4b28a8c39d2835e8b4d9ee1e56ff","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/tags.ejs","hash":"f92692427de2caa48033f975f193f9a8e4b02613","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/post/post-nav.ejs","hash":"1e92a0ca46977f94ce27540ceb09ce05bc75accd","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/share.ejs","hash":"c414ae139680a2e2a50e776de4e137265fd0178d","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/post/prismjs.ejs","hash":"198a472f69517829caf2f2cb542d32996a8fed74","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/side/sideHeader.ejs","hash":"3985bd41e1310500d44d42b02863d4b16815154b","modified":1740671571292},{"_id":"themes/bamboo1/layout/_partial/side/side_tagcloud.ejs","hash":"7e716ce939cb8dd4004896adf41559d57bcfff0e","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_archives.ejs","hash":"c82b7c669aaaa2b9575cf6daf6d0a133b64acfd0","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_blogger.ejs","hash":"a2c637abf7943b273edfb69d0d8e0dc812c84127","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_category.ejs","hash":"dccd4537a94cf6e210803d6ab2789e6cd57d755e","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_webinfo.ejs","hash":"1efe51b2685b7c4ad2ab42849c5ebe59e2d20def","modified":1740671571294},{"_id":"themes/bamboo1/layout/_partial/side/side_toc.ejs","hash":"670c9c8bbc0f8cadfaa1e5caca73fd641d2fe831","modified":1740671571294},{"_id":"themes/bamboo1/layout/_partial/side/widget_library_sticky.ejs","hash":"e3929f7edf85900e7becc71e0cbe14da2333f621","modified":1740671571294},{"_id":"themes/bamboo1/layout/_partial/side/side_recent_post.ejs","hash":"7fe61d14865e97988ff5d42047abb4a4ee8af7f0","modified":1740671571293},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_1.styl","hash":"ac20f1e2c9337396d590ceae03f9845b382ad534","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_2.styl","hash":"112c765730edc8d143b503b8407046ab8deb3835","modified":1740671571317},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_4.styl","hash":"8dacae32f9ed4460546a75d313388b1b2497e097","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_7.styl","hash":"955a96e5829684f0021ef2ad6bdf42b928433ad1","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_10.styl","hash":"01bcf630c126f7fa273892245e6e6c59b654bf56","modified":1740671571323},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_6.styl","hash":"8a33ff2ed45cb2539743fd12b8ea4fc0d3873b98","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_8.styl","hash":"1826bd092b9ea3a028d41cacc993927899406deb","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_1.styl","hash":"f7fc1257c6b402b1ddec85d45ac8e665580dc14d","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_11.styl","hash":"f7b6a4ab283029f649a0ae2732fa6e7079ecc435","modified":1740671571323},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_3.styl","hash":"be27e1a9f2c0d78ab31f7a6b59341dc8c4393f88","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_5.styl","hash":"7e63625a1fc42f0403c3f933b9f642362f44ff46","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_13.styl","hash":"a5015270d9d79fa2f4ed246939d48bf4c9c7f7f6","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_12.styl","hash":"27792c767fa345b0dbe735a681c87cf790e19a8b","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_14.styl","hash":"0ad1c7d9faaf46bc201d8c7c9b34e39ae7efec48","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_15.styl","hash":"fd9b1e87dfaed5db7d9d7b9dc272a5669056c278","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_2.styl","hash":"dbf766a7086bfb35a7fabc635edeb67a32d1828f","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_5.styl","hash":"6fea986bd4c37188ce7da86b0839749ac188bd02","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_3.styl","hash":"083991d97f004f1f657c7a7649bd7b319dee652e","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_4.styl","hash":"ea0a0fbfb605d7d0592a06bb94e38f386830aa24","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_6.styl","hash":"7f6b7d34933921dbabee0937053cb288fcad9647","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_7.styl","hash":"d75280def0358da644945744da22f6a5f2abd745","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_8.styl","hash":"4ce4323bc8a183533bbb1ab9ea2bf946350e5713","modified":1740671571326},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1740671571346},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_9.styl","hash":"6cdaa72cab01a2ca483eb7092372bcfcb2dc9b25","modified":1740671571327},{"_id":"themes/bamboo1/layout/_partial/comment/gitalk/layout.ejs","hash":"8a4c57646ee0d4a4e94d568708fb85a8f9ac97e7","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/beaudar/layout.ejs","hash":"52b9a55b6e83bd9a10fc3f66a18be98e3965475b","modified":1740671571268},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1740671571347},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1740671571347},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.svg","hash":"337b4f156f6d8f4beb32c32a3db46fef361cff74","modified":1740671571347},{"_id":"themes/bamboo1/layout/_partial/comment/gitalk/script.ejs","hash":"733947cad238d89c5a5694ecf19a83b1d5648ab9","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/changyan/layout.ejs","hash":"46192143a90303d8924b3d07d28df116bc833894","modified":1740671571268},{"_id":"themes/bamboo1/layout/_partial/comment/gitment/layout.ejs","hash":"353820d6d6aade09cd21b31585afa20485008083","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/gitment/script.ejs","hash":"1e02cf43a347a612796aa188446605e213e0dd51","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/livere/layout.ejs","hash":"f88b32604056721e658c25f775866a1519e714f2","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/giscus/layout.ejs","hash":"a17970930b1064def6f5ad5f67a1afd3ed3169a0","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/changyan/script.ejs","hash":"c357b9052564e12754cf21a1cb1debd3bdfe1eac","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/beaudar/script.ejs","hash":"58b914569fbc9d5bf706674c1ea4d7a83b5540d1","modified":1740671571268},{"_id":"themes/bamboo1/layout/_partial/comment/livere/script.ejs","hash":"f545bc19b874f684bc4369ef1c6bfc4427b13b6b","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/utterance/layout.ejs","hash":"eab867c580f6184d068d5fcc545a763a2919eb16","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/twikoo/layout.ejs","hash":"bdfd72b519f3bd8f3f78fd631a8326cbd0b20a98","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/giscus/script.ejs","hash":"69ee0d2750ff779f754c8ffe6a5c44960420e2b7","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/twikoo/script.ejs","hash":"c09ef940a9d99d4e567e4891644241dd0ee40135","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/waline/layout.ejs","hash":"a82f1c7819cadca142b7f3436957ddce5adf7fa0","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/comment/utterance/script.ejs","hash":"b8206894169b3e5ad1d38b448db5b9f1e4717987","modified":1740671571273},{"_id":"themes/bamboo1/layout/_partial/comment/valine/layout.ejs","hash":"453fa08c310cef7d00a12bb7cf448aef34c7728a","modified":1740671571273},{"_id":"source/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1738910330380},{"_id":"themes/bamboo1/layout/_partial/comment/waline/script.ejs","hash":"ddfc35b05d22d2ad0f5b41444ed47793546f4b9d","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/comment/valine/script.ejs","hash":"c39459b0fb46bb1eb49823cc62375f04d3b4a48e","modified":1740671571273},{"_id":"source/_posts/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1738910330380},{"_id":"themes/bamboo1/source/js/jquery3.5.1.js","hash":"29fa5ad995e9ec866ece1d3d0b698fc556580eee","modified":1740671571341},{"_id":"source/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1739945847505},{"_id":"source/_posts/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1739945847505},{"_id":"themes/bamboo1/source/js/valine/index.js","hash":"d520897b1bd3788aacb672b5cd9ff7ab0c81fc80","modified":1740671571352},{"_id":"themes/bamboo1/source/js/swiper/swiper.min.js","hash":"674fa0bd5973cc8124d6a711c725b119c025da0c","modified":1740671571349},{"_id":"themes/bamboo1/source/js/waline/waline.min.js","hash":"3a17de5f24e0437c3c681b15f147ceef3980736f","modified":1740671571356},{"_id":"source/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1737530573994},{"_id":"source/_posts/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1737530573994},{"_id":"themes/bamboo1/source/js/vue2.6.11.js","hash":"1159f02f3f7191a5cf4c109734d0268173fab96d","modified":1740671571356},{"_id":"source/img/bg.jpg","hash":"91cb967d8e17bb6304049f800b236e59ae4cc752","modified":1736254693039},{"_id":"source/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1737530573974},{"_id":"source/_posts/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1737530573974},{"_id":"public/about/index.html","hash":"71b7abad5bb4cbf2523f2a058078d21b19bedd70","modified":1745855422998},{"_id":"public/log/index.html","hash":"cd80ce8b19e5865c7d97a610b7160e828c093731","modified":1745855422998},{"_id":"public/20235/04/26/learning-cpp-notes/index.html","hash":"e11a093e08b8b4cb3543bc047df9b9efe0fa1f35","modified":1745855422998},{"_id":"public/2025/03/11/transformer-notes/index.html","hash":"d3145b2811277c99199e9963d83cc29d7aad5e7a","modified":1745855422998},{"_id":"public/2025/02/25/ai4chem/index.html","hash":"05a54b27229fbb1d521e98cd4d26057d57c0ef80","modified":1745855422998},{"_id":"public/2024/01/22/dive-into-deep-learning/index.html","hash":"c88f63254aed53521542425f4bdcf95cbb71ce8e","modified":1745855422998},{"_id":"public/2023/11/25/machine-learning-notes/index.html","hash":"1b370e484039383499e08836f1cfb359934800a2","modified":1745855422998},{"_id":"public/2023/10/22/algorithm-skills/index.html","hash":"e9de6c2dcaf4918222e433a77cf6572378f80fbe","modified":1745855422998},{"_id":"public/2023/09/13/algorithm-dp/index.html","hash":"65088104553c530b3690437b4b137b622f42a190","modified":1745855422998},{"_id":"public/2023/09/07/algorithm-math/index.html","hash":"03a24141ff059ca827c83883ae7afdf85313e0a7","modified":1745855422998},{"_id":"public/2023/08/05/algorithm-graph/index.html","hash":"7005ca8c37bc19b7ba07f81c41f03febb5b6a5ed","modified":1745855422998},{"_id":"public/2023/08/03/algorithm-search/index.html","hash":"7ab9479e7aaf4e021b9c33b4e8af119e25379a36","modified":1745855422998},{"_id":"public/2023/07/07/algorithm-data-structure/index.html","hash":"486422051b6dc9574fcdf68029fcb3537c1c5a2a","modified":1745855422998},{"_id":"public/2023/04/27/algorithm-basic/index.html","hash":"ad7c2099fbffc0769b08a20497e05de3d9c52882","modified":1745855422998},{"_id":"public/2023/01/22/hello-world/index.html","hash":"f89d7a9893f34b3bd5e0dd5a46a9b49aef0a9e6c","modified":1745855422998},{"_id":"public/archives/index.html","hash":"a57d8450a1bb24fd7091713269a49896d351faf6","modified":1745855422998},{"_id":"public/archives/page/2/index.html","hash":"7c5b0ad4579ecb80da71cd651f9ead3a1ced5c33","modified":1745855422998},{"_id":"public/archives/2023/index.html","hash":"a215be39b2ca48af7c896cc5464359b36d66a6f8","modified":1745855422998},{"_id":"public/archives/2023/01/index.html","hash":"6845e007453971ee8820832d24a8dd1fb0404d47","modified":1745855422998},{"_id":"public/archives/2023/04/index.html","hash":"a9be77e8b4b935fe4c82fa1ae22a94078d374264","modified":1745855422998},{"_id":"public/archives/2023/07/index.html","hash":"dfb7a09afa9d758bfd20d1b8c9569c5f75ebba47","modified":1745855422998},{"_id":"public/archives/2023/08/index.html","hash":"003e14e7c9a9b23c8665924f3613b85da1e2b208","modified":1745855422998},{"_id":"public/archives/2023/09/index.html","hash":"b517a847d12683fafa600c9deac5ee9978f1a6bc","modified":1745855422998},{"_id":"public/archives/2023/10/index.html","hash":"f467a8de14060d45347685c5909228dd09e602e5","modified":1745855422998},{"_id":"public/archives/2023/11/index.html","hash":"13389ef9a9dac9126cd7b1814677ab7f19a18904","modified":1745855422998},{"_id":"public/archives/2024/index.html","hash":"96955f40b682eb728111cecd15381fe5251f5aad","modified":1745855422998},{"_id":"public/archives/2024/01/index.html","hash":"184166e28dd73ac301b90d64fcfbf7b75020dd68","modified":1745855422998},{"_id":"public/archives/2025/index.html","hash":"6256ac778e9b89bc8e4a79d719b6fb12c3892304","modified":1745855422998},{"_id":"public/archives/2025/02/index.html","hash":"33e59c2f8339117913188f7b0368e7cd1c914efc","modified":1745855422998},{"_id":"public/archives/2025/03/index.html","hash":"5a94bb2a91a02ba5dce2ce5b3f6d17baf3c4a65b","modified":1745855422998},{"_id":"public/archives/20235/index.html","hash":"9e6cd2b70ffcb5648b6497e672a5e2cde1ca0849","modified":1745855422998},{"_id":"public/archives/20235/04/index.html","hash":"a9f8eaef0f5b2cdae4a55390e81376c3bfbeb18b","modified":1745855422998},{"_id":"public/index.html","hash":"03982c367fcfebb8179d38da93ecdb7abb81e9a4","modified":1745855422998},{"_id":"public/page/2/index.html","hash":"51055bfe2b2d58122cf970b610716569597f359e","modified":1745855422998},{"_id":"public/favicon.ico","hash":"801ff7b3f358b77a813787a97ef59148eec93fd8","modified":1745855422998},{"_id":"public/medias/cursor/Horizontal.cur","hash":"c3c5e8485a67b7ab16079a96b53aff7ff52de756","modified":1745855422998},{"_id":"public/medias/logo.png","hash":"d08165f945567a08bd74d36b1241a0b8f1618536","modified":1745855422998},{"_id":"public/js/danmu/close.png","hash":"2c3ed4345f91dc1b74a57b6dcd1e1efa9e279dbb","modified":1745855422998},{"_id":"public/js/shareJs/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1745855422998},{"_id":"public/js/shareJs/fonts/iconfont.svg","hash":"337b4f156f6d8f4beb32c32a3db46fef361cff74","modified":1745855422998},{"_id":"public/js/shareJs/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1745855422998},{"_id":"public/img/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1745855422998},{"_id":"public/js/shareJs/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1745855422998},{"_id":"public/img/favicon.ico","hash":"a3a1b2c8f5bcd3faf1871d57d7923d6b45f35d34","modified":1745855422998},{"_id":"public/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1745855422998},{"_id":"public/css/style.css","hash":"9d93cfbfc4026b96d8dd9115f24b44dec3643281","modified":1745855422998},{"_id":"public/css/animate.min.css","hash":"8411c1c0418521c96d07bcca0d9dbce7e832ccc9","modified":1745855422998},{"_id":"public/js/activate-power-mode.js","hash":"3d02584da9dd820d1d9a454c5a93a2c37a8e4e42","modified":1745855422998},{"_id":"public/js/app.js","hash":"38e8d7ce69449ee7fc28db92f6be88ae26e708b2","modified":1745855422998},{"_id":"public/js/goTop.js","hash":"5bc7779f0d672c503a68b1e091fb3195df7e9815","modified":1745855422998},{"_id":"public/js/local_search.js","hash":"475dc0727cb85c22f15f86701dd93c4bf449a438","modified":1745855422998},{"_id":"public/js/jquery3.5.1.js","hash":"d2cc8d43ce1c854b1172e42b1209502ad563db83","modified":1745855422998},{"_id":"public/js/ribbon.min.js","hash":"3c8e4d717ca107f3723def1795c8ed62a5f1a8d0","modified":1745855422998},{"_id":"public/js/wrapImage.js","hash":"d29b7b5f24b1cbf342187096ee47ec29b5146e7c","modified":1745855422998},{"_id":"public/js/aplayer/APlayer@1.10.1.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1745855422998},{"_id":"public/js/bubble/bubble.js","hash":"40cbc57f98407216ba6dc412e2b75e18c036240f","modified":1745855422998},{"_id":"public/js/clipboard/clipboard.min.js","hash":"6371ec0a8e242395c7d4d008d2b98e472c9dcc52","modified":1745855422998},{"_id":"public/js/danmu/barrager.css","hash":"3691efec6dd3d554b4a3dd20ef04836459f151a8","modified":1745855422998},{"_id":"public/js/bubble/homeBubble.js","hash":"a8635136621c8c54c04462932192a94f314942cb","modified":1745855422998},{"_id":"public/js/danmu/jquery.barrager.js","hash":"305d6e93f3de102b5e1e9b1373821c849d8f54cb","modified":1745855422998},{"_id":"public/js/falling/snow.js","hash":"6f4ef88304f874ef8bb8ea54f79b5d97f5a8f2f6","modified":1745855422998},{"_id":"public/js/falling/sakura.js","hash":"ab41921e8f6ea1bedfcc348924574dc0caa20858","modified":1745855422998},{"_id":"public/js/cursor/clicklove.js","hash":"9e8e79d69ad8338761272f86fe5cad0ad5e503cc","modified":1745855422998},{"_id":"public/js/cursor/fireworks.js","hash":"6e1e9206549a6a1a4f5a8672a2dc5044a8f691bd","modified":1745855422998},{"_id":"public/js/cursor/explosion.min.js","hash":"ed2d0a5ad306a2745b7c8180b69e36b78d4b0698","modified":1745855422998},{"_id":"public/js/vue2.6.11.js","hash":"e793aa33ef33150eaba3bc02b07455a231f053ad","modified":1745855422998},{"_id":"public/js/cursor/text.js","hash":"a015017310e601f1e544cbc4b08c35b8e547c939","modified":1745855422998},{"_id":"public/js/getPhotoOnline/index.js","hash":"f513605485600561123ffae1a70a0eb35cd5c675","modified":1745855422998},{"_id":"public/js/getSiteOnline/index.js","hash":"8b93e96331bbdcbee0deb33c9aeca6b2dceacb4b","modified":1745855422998},{"_id":"public/js/issues/index.js","hash":"e5f7b37f9dd8e966c7a63b8b6da27d53510eddeb","modified":1745855422998},{"_id":"public/js/getTalkOnline/index.js","hash":"58d9601cfd851c83c2eadd4803698171cd2d8b08","modified":1745855422998},{"_id":"public/js/pjax@0.2.8/index.js","hash":"c9b1e349203e558dbe43665353e88c6eafc7dbcd","modified":1745855422998},{"_id":"public/js/shareJs/font.css","hash":"9d909397e4e94f696b7dd90a16481b50cf170362","modified":1745855422998},{"_id":"public/js/shareJs/share.min.css","hash":"573c7dddb465efd5f5a9337bd50a1ed3f8e82cff","modified":1745855422998},{"_id":"public/js/shareJs/social-share.min.js","hash":"efdfa6b695ac6f0dd04cd8153d3e3a1a1edd90c2","modified":1745855422998},{"_id":"public/js/tocbot/tocbot.min.js","hash":"bc45d3586a21f7e364cd6efe58844932c00cf11c","modified":1745855422998},{"_id":"public/js/tocbot/tocbot.css","hash":"45e469dffa7b9ebc03f99fd09fb97274cdc5e9b4","modified":1745855422998},{"_id":"public/js/prism/prism-coy.min.css","hash":"fe1246de39c25eaa7ad1b0c997ee530dbdd39ad8","modified":1745855422998},{"_id":"public/js/prism/prism-funky.min.css","hash":"0220f68ccda78c2b5d1109e58f3879674c93b587","modified":1745855422998},{"_id":"public/js/prism/prism-dark.min.css","hash":"a3f604a19e9a46f83a2fde49dfb45782748957ca","modified":1745855422998},{"_id":"public/js/prism/prism-okaidia.min.css","hash":"50be6cc15d883ff3fa5d0885fed47241695a986c","modified":1745855422998},{"_id":"public/js/prism/prism-line-numbers.css","hash":"3b64b50b73729de943ec894c1d6f19115fa81624","modified":1745855422998},{"_id":"public/js/prism/prism-solarizedlight.min.css","hash":"927b757cd8030d12953b5c0fa6eed5de15dda8ad","modified":1745855422998},{"_id":"public/js/prism/prism.min.css","hash":"aa405e2bcb571595c822a80f5482454c1536fa52","modified":1745855422998},{"_id":"public/js/utils/index.js","hash":"54c66b0a396cc3743884cdb979e5c400218613ce","modified":1745855422998},{"_id":"public/js/prism/prism-twilight.min.css","hash":"ff4a6e3c4f1cb9bb59ec061656eacb750d238c15","modified":1745855422998},{"_id":"public/js/valine/index.js","hash":"8809117760e0a7ce8dcc3f14b6421a4d415284a6","modified":1745855422998},{"_id":"public/js/swiper/swiper.animate1.0.3.min.js","hash":"0e48f180ca2f18b787e4b7b6e55ee3b0c6067691","modified":1745855422998},{"_id":"public/js/vue-seamless-scroll/index.js","hash":"f2aaf3f9b1ab7362f7cc158e5360cb1d62a57172","modified":1745855422998},{"_id":"public/js/swiper/swiper@5.4.1.min.css","hash":"de2263f82e7bf0778f31fd05c53000799f60701a","modified":1745855422998},{"_id":"public/js/prism/prism-tomorrow.min.css","hash":"7b4247bc4d3b719afe5957779d0e5c8fb716c8ea","modified":1745855422998},{"_id":"public/js/vue-typed-js/index.css","hash":"36a1d2f61d11ab328e349d6a523dd9dea2ec7ee1","modified":1745855422998},{"_id":"public/js/swiper/swiper.min.js","hash":"a2fe3c0df9196597c283b2f6ffecc1d4d8702245","modified":1745855422998},{"_id":"public/js/swiper/vue-awesome-swiper.js","hash":"b7a1ab21dfc58272009bfb5cb7ab87b79f5df573","modified":1745855422998},{"_id":"public/js/vue-typed-js/index.js","hash":"0d80f25135de943ccdfdebec23275bd82712fae1","modified":1745855422998},{"_id":"public/js/waline/waline.min.js","hash":"94f70e622e2a1ab05adb205033a9ddf371c61534","modified":1745855422998},{"_id":"public/img/transformer-notes/pic-2.png","hash":"0df6ba77725900dcc7a9ad3e39790f0b8ecc967c","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1745855422998},{"_id":"public/img/transformer-notes/pic-1.png","hash":"f6d66e79b8bade7a2b5366ce7fd9a7db04707af1","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1745855422998},{"_id":"public/img/bg.jpg","hash":"91cb967d8e17bb6304049f800b236e59ae4cc752","modified":1745855422998},{"_id":"public/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1745855422998}],"Category":[],"Data":[],"Page":[{"title":"关于","type":"about","_content":"\n## 基础信息\n\n[[Email](mailto:1981270473@qq.com)] | [[Gitee](https://gitee.com/huoyu233)] | [[Github](https://github.com/HuoYu233)] | [[Bilibili](https://space.bilibili.com/82505737)]\n\n厦门大学人工智能硕士研究生在读\n\n- Basic Skills：CET-6、Java、C/C++、Python、JavaScript\n- Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis\n\n## 项目经历\n\n- [2023.09-2024.04] 面向智慧城市的群智感知平台[[Link](https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/)]\n- [2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[[Link](https://gitee.com/huoyu233/minyipin)]\n\n## 校园经历\n\n- [2023.07-2024.04] [福州大学人机共融智能课题组](https://www.fzu-urbansensing.com/)开发组成员\n- [2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员\n\n## 荣誉奖项\n\n**[Undergraduate]**\n\n- [2025.4] 福州大学第二十二届程序设计竞赛暨校ACM/ICPC集训队选拔赛**三等奖**\n\n- [2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛**二等奖**\n- [2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(**50%**)\n- [2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛**一等奖**\n- [2024.5] COMAP’s 2024 MCM/ICM Contest **Honorable Mention**\n- [2024.4] 第十五届蓝桥杯软件赛C/C++ **A组省三等奖**\n- [2024.4] 福州大学第八届网络信息安全竞赛**三等奖**\n- [2023.10] 第六届传智杯全国IT技能大赛优秀志愿者\n- [2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖\n\n**[High School]**\n\n- [2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖\n- [2019.10] 第十届全国青少年科学影像节三等奖\n- [2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖\n","source":"about/index.md","raw":"---\ntitle: 关于\ntype: about\n---\n\n## 基础信息\n\n[[Email](mailto:1981270473@qq.com)] | [[Gitee](https://gitee.com/huoyu233)] | [[Github](https://github.com/HuoYu233)] | [[Bilibili](https://space.bilibili.com/82505737)]\n\n厦门大学人工智能硕士研究生在读\n\n- Basic Skills：CET-6、Java、C/C++、Python、JavaScript\n- Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis\n\n## 项目经历\n\n- [2023.09-2024.04] 面向智慧城市的群智感知平台[[Link](https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/)]\n- [2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[[Link](https://gitee.com/huoyu233/minyipin)]\n\n## 校园经历\n\n- [2023.07-2024.04] [福州大学人机共融智能课题组](https://www.fzu-urbansensing.com/)开发组成员\n- [2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员\n\n## 荣誉奖项\n\n**[Undergraduate]**\n\n- [2025.4] 福州大学第二十二届程序设计竞赛暨校ACM/ICPC集训队选拔赛**三等奖**\n\n- [2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛**二等奖**\n- [2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(**50%**)\n- [2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛**一等奖**\n- [2024.5] COMAP’s 2024 MCM/ICM Contest **Honorable Mention**\n- [2024.4] 第十五届蓝桥杯软件赛C/C++ **A组省三等奖**\n- [2024.4] 福州大学第八届网络信息安全竞赛**三等奖**\n- [2023.10] 第六届传智杯全国IT技能大赛优秀志愿者\n- [2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖\n\n**[High School]**\n\n- [2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖\n- [2019.10] 第十届全国青少年科学影像节三等奖\n- [2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖\n","date":"2025-04-25T09:10:19.273Z","updated":"2025-04-25T09:10:19.273Z","path":"about/index.html","comments":1,"layout":"page","_id":"cma198q960000bs993ohoe003","content":"<h2 id=\"基础信息\"><a href=\"#基础信息\" class=\"headerlink\" title=\"基础信息\"></a>基础信息</h2><p>[<a href=\"mailto:1981270473@qq.com\">Email</a>] | [<a href=\"https://gitee.com/huoyu233\">Gitee</a>] | [<a href=\"https://github.com/HuoYu233\">Github</a>] | [<a href=\"https://space.bilibili.com/82505737\">Bilibili</a>]</p>\n<p>厦门大学人工智能硕士研究生在读</p>\n<ul>\n<li>Basic Skills：CET-6、Java、C&#x2F;C++、Python、JavaScript</li>\n<li>Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis</li>\n</ul>\n<h2 id=\"项目经历\"><a href=\"#项目经历\" class=\"headerlink\" title=\"项目经历\"></a>项目经历</h2><ul>\n<li>[2023.09-2024.04] 面向智慧城市的群智感知平台[<a href=\"https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/\">Link</a>]</li>\n<li>[2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[<a href=\"https://gitee.com/huoyu233/minyipin\">Link</a>]</li>\n</ul>\n<h2 id=\"校园经历\"><a href=\"#校园经历\" class=\"headerlink\" title=\"校园经历\"></a>校园经历</h2><ul>\n<li>[2023.07-2024.04] <a href=\"https://www.fzu-urbansensing.com/\">福州大学人机共融智能课题组</a>开发组成员</li>\n<li>[2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员</li>\n</ul>\n<h2 id=\"荣誉奖项\"><a href=\"#荣誉奖项\" class=\"headerlink\" title=\"荣誉奖项\"></a>荣誉奖项</h2><p><strong>[Undergraduate]</strong></p>\n<ul>\n<li><p>[2025.4] 福州大学第二十二届程序设计竞赛暨校ACM&#x2F;ICPC集训队选拔赛<strong>三等奖</strong></p>\n</li>\n<li><p>[2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛<strong>二等奖</strong></p>\n</li>\n<li><p>[2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(<strong>50%</strong>)</p>\n</li>\n<li><p>[2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛<strong>一等奖</strong></p>\n</li>\n<li><p>[2024.5] COMAP’s 2024 MCM&#x2F;ICM Contest <strong>Honorable Mention</strong></p>\n</li>\n<li><p>[2024.4] 第十五届蓝桥杯软件赛C&#x2F;C++ <strong>A组省三等奖</strong></p>\n</li>\n<li><p>[2024.4] 福州大学第八届网络信息安全竞赛<strong>三等奖</strong></p>\n</li>\n<li><p>[2023.10] 第六届传智杯全国IT技能大赛优秀志愿者</p>\n</li>\n<li><p>[2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖</p>\n</li>\n</ul>\n<p><strong>[High School]</strong></p>\n<ul>\n<li>[2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖</li>\n<li>[2019.10] 第十届全国青少年科学影像节三等奖</li>\n<li>[2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"基础信息\"><a href=\"#基础信息\" class=\"headerlink\" title=\"基础信息\"></a>基础信息</h2><p>[<a href=\"mailto:1981270473@qq.com\">Email</a>] | [<a href=\"https://gitee.com/huoyu233\">Gitee</a>] | [<a href=\"https://github.com/HuoYu233\">Github</a>] | [<a href=\"https://space.bilibili.com/82505737\">Bilibili</a>]</p>\n<p>厦门大学人工智能硕士研究生在读</p>\n<ul>\n<li>Basic Skills：CET-6、Java、C&#x2F;C++、Python、JavaScript</li>\n<li>Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis</li>\n</ul>\n<h2 id=\"项目经历\"><a href=\"#项目经历\" class=\"headerlink\" title=\"项目经历\"></a>项目经历</h2><ul>\n<li>[2023.09-2024.04] 面向智慧城市的群智感知平台[<a href=\"https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/\">Link</a>]</li>\n<li>[2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[<a href=\"https://gitee.com/huoyu233/minyipin\">Link</a>]</li>\n</ul>\n<h2 id=\"校园经历\"><a href=\"#校园经历\" class=\"headerlink\" title=\"校园经历\"></a>校园经历</h2><ul>\n<li>[2023.07-2024.04] <a href=\"https://www.fzu-urbansensing.com/\">福州大学人机共融智能课题组</a>开发组成员</li>\n<li>[2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员</li>\n</ul>\n<h2 id=\"荣誉奖项\"><a href=\"#荣誉奖项\" class=\"headerlink\" title=\"荣誉奖项\"></a>荣誉奖项</h2><p><strong>[Undergraduate]</strong></p>\n<ul>\n<li><p>[2025.4] 福州大学第二十二届程序设计竞赛暨校ACM&#x2F;ICPC集训队选拔赛<strong>三等奖</strong></p>\n</li>\n<li><p>[2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛<strong>二等奖</strong></p>\n</li>\n<li><p>[2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(<strong>50%</strong>)</p>\n</li>\n<li><p>[2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛<strong>一等奖</strong></p>\n</li>\n<li><p>[2024.5] COMAP’s 2024 MCM&#x2F;ICM Contest <strong>Honorable Mention</strong></p>\n</li>\n<li><p>[2024.4] 第十五届蓝桥杯软件赛C&#x2F;C++ <strong>A组省三等奖</strong></p>\n</li>\n<li><p>[2024.4] 福州大学第八届网络信息安全竞赛<strong>三等奖</strong></p>\n</li>\n<li><p>[2023.10] 第六届传智杯全国IT技能大赛优秀志愿者</p>\n</li>\n<li><p>[2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖</p>\n</li>\n</ul>\n<p><strong>[High School]</strong></p>\n<ul>\n<li>[2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖</li>\n<li>[2019.10] 第十届全国青少年科学影像节三等奖</li>\n<li>[2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖</li>\n</ul>\n"},{"title":"日志","type":"log","_content":"- 2025.2.28 使用`netlify`加速网站\n- 2025.1.24 主题切换为`bamboo`\n- 2025.1.22 数据误删重建 主题采用`coder`\n- 2024.1.22 绑定域名 `kazovo.cn`\n- 2024.1.22 删除目录`tools`\n- 2023.11.10 修改主题为`mashiro`\n- 2023.10.12 修改大量文章内容\n- 2023.3.16 更新日志移动至新页面`log`下\n- 2023.3.15 修改主题为 `Kaze`\n- 2023.2.17 新增`tools`页面\n- 2023.2.17 同步多篇本地文章\n- 2023.2.17 绑定域名 `hawyior.top`\n- 2023.2.17 博客基于`Hexo`搭建框架完毕，主题采用`cactus`\n","source":"log/index.md","raw":"---\ntitle: 日志\ntype: log\n---\n- 2025.2.28 使用`netlify`加速网站\n- 2025.1.24 主题切换为`bamboo`\n- 2025.1.22 数据误删重建 主题采用`coder`\n- 2024.1.22 绑定域名 `kazovo.cn`\n- 2024.1.22 删除目录`tools`\n- 2023.11.10 修改主题为`mashiro`\n- 2023.10.12 修改大量文章内容\n- 2023.3.16 更新日志移动至新页面`log`下\n- 2023.3.15 修改主题为 `Kaze`\n- 2023.2.17 新增`tools`页面\n- 2023.2.17 同步多篇本地文章\n- 2023.2.17 绑定域名 `hawyior.top`\n- 2023.2.17 博客基于`Hexo`搭建框架完毕，主题采用`cactus`\n","date":"2025-02-27T16:09:11.623Z","updated":"2025-02-27T16:09:11.623Z","path":"log/index.html","comments":1,"layout":"page","_id":"cma198q9b0002bs994ncvanc1","content":"<ul>\n<li>2025.2.28 使用<code>netlify</code>加速网站</li>\n<li>2025.1.24 主题切换为<code>bamboo</code></li>\n<li>2025.1.22 数据误删重建 主题采用<code>coder</code></li>\n<li>2024.1.22 绑定域名 <code>kazovo.cn</code></li>\n<li>2024.1.22 删除目录<code>tools</code></li>\n<li>2023.11.10 修改主题为<code>mashiro</code></li>\n<li>2023.10.12 修改大量文章内容</li>\n<li>2023.3.16 更新日志移动至新页面<code>log</code>下</li>\n<li>2023.3.15 修改主题为 <code>Kaze</code></li>\n<li>2023.2.17 新增<code>tools</code>页面</li>\n<li>2023.2.17 同步多篇本地文章</li>\n<li>2023.2.17 绑定域名 <code>hawyior.top</code></li>\n<li>2023.2.17 博客基于<code>Hexo</code>搭建框架完毕，主题采用<code>cactus</code></li>\n</ul>\n","excerpt":"","more":"<ul>\n<li>2025.2.28 使用<code>netlify</code>加速网站</li>\n<li>2025.1.24 主题切换为<code>bamboo</code></li>\n<li>2025.1.22 数据误删重建 主题采用<code>coder</code></li>\n<li>2024.1.22 绑定域名 <code>kazovo.cn</code></li>\n<li>2024.1.22 删除目录<code>tools</code></li>\n<li>2023.11.10 修改主题为<code>mashiro</code></li>\n<li>2023.10.12 修改大量文章内容</li>\n<li>2023.3.16 更新日志移动至新页面<code>log</code>下</li>\n<li>2023.3.15 修改主题为 <code>Kaze</code></li>\n<li>2023.2.17 新增<code>tools</code>页面</li>\n<li>2023.2.17 同步多篇本地文章</li>\n<li>2023.2.17 绑定域名 <code>hawyior.top</code></li>\n<li>2023.2.17 博客基于<code>Hexo</code>搭建框架完毕，主题采用<code>cactus</code></li>\n</ul>\n"}],"Post":[{"title":"Optuna搜参优化LightGBM预测催化反应产率","mathjax":true,"date":"2025-02-25T12:46:25.000Z","img":"https://img0.baidu.com/it/u=3213989145,974537053&fm=253&fmt=auto&app=120&f=PNG?w=1023&h=362","excerpt":"RT","_content":"# 任务概述\n\n构建一个能够准确预测碳氮成键反应产率的预测模型。  \n\n通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。\n\n或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。\n\n训练集中包含19999条反应数据，测试集中包含3539条反应数据。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。\n\n**评价指标**\n\n实验真实结果与预测结果$R^2$决定系数来进行评测:\n$$\nR^2(y,\\hat{y})=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n$$\n\n\n# baseline\n\n1. **导入库**：首先，代码导入了需要用到的库，包括 `pandas`（用于数据处理和分析），`scikit-learn`（机器学习库），`rdkit`（化学信息工具）。\n2. **读取数据**：代码通过使用 `pd.read_csv` 函数从文件中读取训练集和测试集数据。\n3. **使用Morgan分子指纹建模SMILES**：\n\n   \\- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。\n\n1. **使用随机森林预测结果**：\n\n   \\- 这里直接调用`sklearn`的`RandomForestRegressor`模块实例化一个随机森林模型，并对`n_estimators`等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地`'./random_forest_model.pkl'`。\n\n1. **加载模型进行预测，并将保存结果文件到本地：**\n\n   ` pkl`文件直接使用`pickle.load()`加载，然后使用`model.predict(x)`进行预测。\n\n## SMILES\n\nSMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。\n\nSMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。\n\n在SMILES中，原子由他们的化学符号表示，=表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。\n\n（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）\n\n由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。\n\n## Morgan fingerprint\n位向量（bit vector）形式的特征，即由0,1组成的向量。\n\n分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。\n\n通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。\n\n## RDKit\n\nRDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES\n\nRDkit是化学信息学中主要的工具，是开源的。网址：http://www.rdkit.org\n支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。\n\n## 结果\n\nbaseline的$R^2 = 0.0745336043830066$，约0.08\n\n# My model\n\nSMILE表达式：字符串表示分子结构的化学信息学标准。\n\n分子指纹：它通过捕捉分子中每个原子的局部环境信息，将分子结构编码为固定长度的二进制向量或者整数向量\n\n- 对于每个原子，根据其邻居的标识符更新自己的标识符。\n- 更新规则通常基于哈希函数，将邻居的标识符组合成一个新的标识符。\n- 迭代次数等于半径 r。\n\n**具体的从SMILES表达式转成分子指纹的步骤：**\n\n在 Morgan 指纹中，每个原子的 **初始标识符** 是基于原子的局部环境计算的。具体来说，初始标识符通常由以下原子属性决定：\n\n- 原子类型（如 C、N、O 等）\n- 原子的电荷\n- 原子的度（连接的键数）\n- 原子的氢原子数\n- 其他可能的属性（如是否在环中）\n\n- 哈希函数将标识符映射到一个固定范围的整数（通常是 0 到 $2^{n}-1$，其中 n是指纹的位数）。\n- 例如，如果指纹长度是 1024 位，则哈希值的范围是 0 到 1023。\n\n**指纹生成**\n\n- 将所有原子的哈希值映射到一个固定长度的位向量中。\n- 如果某个哈希值对应的位被置为 1，则表示该特征存在于分子中。\n\nOptuna：\n\n内含优化搜索，不用穷举式搜索，速度更快。\n\n支持连续值，网格只支持离散值\n\nLightGBM：\n\n轻量级的梯度提升树\n\n通过数据压缩使得训练更快\n\n**为什么不用随机森林**\n\nGBDT在大数据集上表现比RF好，LightGBM又是在XgBoost上进行优化的\n\n**有哪些特征？**\n\n最大叶子数量，最大深度，每个节点的最小样本数，建树随机选择的特征比例，随机选择的样本比例，正则化项系数\n\n结果：\n\nBest trial:\n  R2: 0.32038588335457874\n  Params: \n    num_leaves: 250\n    learning_rate: 0.04051655617169133\n    max_depth: 17\n    min_data_in_leaf: 10\n    feature_fraction: 0.4350306415682709\n    bagging_fraction: 0.6540225841297226\n    bagging_freq: 1\n    lambda_l1: 0.000524499451410933\n    lambda_l2: 0.01259645970374942\n\nLGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题，而从实践效果来看，LGBM也确实做到了这点——LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍\n\nOptuna 相比 GridSearch 有以下优势：\n\n1. **效率更高**\n   - **GridSearch**：遍历所有参数组合，计算量大，耗时长。\n   - **Optuna**：基于贝叶斯优化等算法，智能选择有潜力的参数，减少不必要的计算。\n2. **支持复杂搜索空间**\n   - **GridSearch**：只能处理离散参数值。\n   - **Optuna**：支持连续、离散、条件参数等多种类型，适应复杂搜索空间。\n3. **动态调整搜索方向**\n   - **GridSearch**：固定搜索路径，无法根据结果调整。\n   - **Optuna**：根据历史结果动态调整搜索方向，更快找到最优解。\n\n就是要手动的交叉验证\n","source":"_posts/ai4chem.md","raw":"---\ntitle: Optuna搜参优化LightGBM预测催化反应产率 \nmathjax: true\ndate: 2025/2/25 20:46:25\nimg: https://img0.baidu.com/it/u=3213989145,974537053&fm=253&fmt=auto&app=120&f=PNG?w=1023&h=362\nexcerpt: RT\n---\n# 任务概述\n\n构建一个能够准确预测碳氮成键反应产率的预测模型。  \n\n通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。\n\n或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。\n\n训练集中包含19999条反应数据，测试集中包含3539条反应数据。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。\n\n**评价指标**\n\n实验真实结果与预测结果$R^2$决定系数来进行评测:\n$$\nR^2(y,\\hat{y})=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n$$\n\n\n# baseline\n\n1. **导入库**：首先，代码导入了需要用到的库，包括 `pandas`（用于数据处理和分析），`scikit-learn`（机器学习库），`rdkit`（化学信息工具）。\n2. **读取数据**：代码通过使用 `pd.read_csv` 函数从文件中读取训练集和测试集数据。\n3. **使用Morgan分子指纹建模SMILES**：\n\n   \\- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。\n\n1. **使用随机森林预测结果**：\n\n   \\- 这里直接调用`sklearn`的`RandomForestRegressor`模块实例化一个随机森林模型，并对`n_estimators`等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地`'./random_forest_model.pkl'`。\n\n1. **加载模型进行预测，并将保存结果文件到本地：**\n\n   ` pkl`文件直接使用`pickle.load()`加载，然后使用`model.predict(x)`进行预测。\n\n## SMILES\n\nSMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。\n\nSMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。\n\n在SMILES中，原子由他们的化学符号表示，=表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。\n\n（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）\n\n由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。\n\n## Morgan fingerprint\n位向量（bit vector）形式的特征，即由0,1组成的向量。\n\n分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。\n\n通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。\n\n## RDKit\n\nRDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES\n\nRDkit是化学信息学中主要的工具，是开源的。网址：http://www.rdkit.org\n支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。\n\n## 结果\n\nbaseline的$R^2 = 0.0745336043830066$，约0.08\n\n# My model\n\nSMILE表达式：字符串表示分子结构的化学信息学标准。\n\n分子指纹：它通过捕捉分子中每个原子的局部环境信息，将分子结构编码为固定长度的二进制向量或者整数向量\n\n- 对于每个原子，根据其邻居的标识符更新自己的标识符。\n- 更新规则通常基于哈希函数，将邻居的标识符组合成一个新的标识符。\n- 迭代次数等于半径 r。\n\n**具体的从SMILES表达式转成分子指纹的步骤：**\n\n在 Morgan 指纹中，每个原子的 **初始标识符** 是基于原子的局部环境计算的。具体来说，初始标识符通常由以下原子属性决定：\n\n- 原子类型（如 C、N、O 等）\n- 原子的电荷\n- 原子的度（连接的键数）\n- 原子的氢原子数\n- 其他可能的属性（如是否在环中）\n\n- 哈希函数将标识符映射到一个固定范围的整数（通常是 0 到 $2^{n}-1$，其中 n是指纹的位数）。\n- 例如，如果指纹长度是 1024 位，则哈希值的范围是 0 到 1023。\n\n**指纹生成**\n\n- 将所有原子的哈希值映射到一个固定长度的位向量中。\n- 如果某个哈希值对应的位被置为 1，则表示该特征存在于分子中。\n\nOptuna：\n\n内含优化搜索，不用穷举式搜索，速度更快。\n\n支持连续值，网格只支持离散值\n\nLightGBM：\n\n轻量级的梯度提升树\n\n通过数据压缩使得训练更快\n\n**为什么不用随机森林**\n\nGBDT在大数据集上表现比RF好，LightGBM又是在XgBoost上进行优化的\n\n**有哪些特征？**\n\n最大叶子数量，最大深度，每个节点的最小样本数，建树随机选择的特征比例，随机选择的样本比例，正则化项系数\n\n结果：\n\nBest trial:\n  R2: 0.32038588335457874\n  Params: \n    num_leaves: 250\n    learning_rate: 0.04051655617169133\n    max_depth: 17\n    min_data_in_leaf: 10\n    feature_fraction: 0.4350306415682709\n    bagging_fraction: 0.6540225841297226\n    bagging_freq: 1\n    lambda_l1: 0.000524499451410933\n    lambda_l2: 0.01259645970374942\n\nLGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题，而从实践效果来看，LGBM也确实做到了这点——LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍\n\nOptuna 相比 GridSearch 有以下优势：\n\n1. **效率更高**\n   - **GridSearch**：遍历所有参数组合，计算量大，耗时长。\n   - **Optuna**：基于贝叶斯优化等算法，智能选择有潜力的参数，减少不必要的计算。\n2. **支持复杂搜索空间**\n   - **GridSearch**：只能处理离散参数值。\n   - **Optuna**：支持连续、离散、条件参数等多种类型，适应复杂搜索空间。\n3. **动态调整搜索方向**\n   - **GridSearch**：固定搜索路径，无法根据结果调整。\n   - **Optuna**：根据历史结果动态调整搜索方向，更快找到最优解。\n\n就是要手动的交叉验证\n","slug":"ai4chem","published":1,"updated":"2025-03-23T12:31:22.540Z","comments":1,"layout":"post","photos":[],"_id":"cma198q980001bs998hyrhhbu","content":"<h1 id=\"任务概述\"><a href=\"#任务概述\" class=\"headerlink\" title=\"任务概述\"></a>任务概述</h1><p>构建一个能够准确预测碳氮成键反应产率的预测模型。  </p>\n<p>通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。</p>\n<p>或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。</p>\n<p>训练集中包含19999条反应数据，测试集中包含3539条反应数据。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。</p>\n<p><strong>评价指标</strong></p>\n<p>实验真实结果与预测结果$R^2$决定系数来进行评测:<br>$$<br>R^2(y,\\hat{y})&#x3D;1-\\frac{\\sum_{i&#x3D;1}^n(y_i-\\hat{y}<em>i)^2}{\\sum</em>{i&#x3D;1}^n(y_i-\\bar{y})^2}<br>$$</p>\n<h1 id=\"baseline\"><a href=\"#baseline\" class=\"headerlink\" title=\"baseline\"></a>baseline</h1><ol>\n<li><p><strong>导入库</strong>：首先，代码导入了需要用到的库，包括 <code>pandas</code>（用于数据处理和分析），<code>scikit-learn</code>（机器学习库），<code>rdkit</code>（化学信息工具）。</p>\n</li>\n<li><p><strong>读取数据</strong>：代码通过使用 <code>pd.read_csv</code> 函数从文件中读取训练集和测试集数据。</p>\n</li>\n<li><p><strong>使用Morgan分子指纹建模SMILES</strong>：</p>\n<p>- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。</p>\n</li>\n<li><p><strong>使用随机森林预测结果</strong>：</p>\n<p>- 这里直接调用<code>sklearn</code>的<code>RandomForestRegressor</code>模块实例化一个随机森林模型，并对<code>n_estimators</code>等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地<code>&#39;./random_forest_model.pkl&#39;</code>。</p>\n</li>\n<li><p><strong>加载模型进行预测，并将保存结果文件到本地：</strong></p>\n<p><code> pkl</code>文件直接使用<code>pickle.load()</code>加载，然后使用<code>model.predict(x)</code>进行预测。</p>\n</li>\n</ol>\n<h2 id=\"SMILES\"><a href=\"#SMILES\" class=\"headerlink\" title=\"SMILES\"></a>SMILES</h2><p>SMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。</p>\n<p>SMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。</p>\n<p>在SMILES中，原子由他们的化学符号表示，&#x3D;表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。</p>\n<p>（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）</p>\n<p>由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。</p>\n<h2 id=\"Morgan-fingerprint\"><a href=\"#Morgan-fingerprint\" class=\"headerlink\" title=\"Morgan fingerprint\"></a>Morgan fingerprint</h2><p>位向量（bit vector）形式的特征，即由0,1组成的向量。</p>\n<p>分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。</p>\n<p>通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。</p>\n<h2 id=\"RDKit\"><a href=\"#RDKit\" class=\"headerlink\" title=\"RDKit\"></a>RDKit</h2><p>RDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES</p>\n<p>RDkit是化学信息学中主要的工具，是开源的。网址：<a href=\"http://www.rdkit.org/\">http://www.rdkit.org</a><br>支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。</p>\n<h2 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h2><p>baseline的$R^2 &#x3D; 0.0745336043830066$，约0.08</p>\n<h1 id=\"My-model\"><a href=\"#My-model\" class=\"headerlink\" title=\"My model\"></a>My model</h1><p>SMILE表达式：字符串表示分子结构的化学信息学标准。</p>\n<p>分子指纹：它通过捕捉分子中每个原子的局部环境信息，将分子结构编码为固定长度的二进制向量或者整数向量</p>\n<ul>\n<li>对于每个原子，根据其邻居的标识符更新自己的标识符。</li>\n<li>更新规则通常基于哈希函数，将邻居的标识符组合成一个新的标识符。</li>\n<li>迭代次数等于半径 r。</li>\n</ul>\n<p><strong>具体的从SMILES表达式转成分子指纹的步骤：</strong></p>\n<p>在 Morgan 指纹中，每个原子的 <strong>初始标识符</strong> 是基于原子的局部环境计算的。具体来说，初始标识符通常由以下原子属性决定：</p>\n<ul>\n<li><p>原子类型（如 C、N、O 等）</p>\n</li>\n<li><p>原子的电荷</p>\n</li>\n<li><p>原子的度（连接的键数）</p>\n</li>\n<li><p>原子的氢原子数</p>\n</li>\n<li><p>其他可能的属性（如是否在环中）</p>\n</li>\n<li><p>哈希函数将标识符映射到一个固定范围的整数（通常是 0 到 $2^{n}-1$，其中 n是指纹的位数）。</p>\n</li>\n<li><p>例如，如果指纹长度是 1024 位，则哈希值的范围是 0 到 1023。</p>\n</li>\n</ul>\n<p><strong>指纹生成</strong></p>\n<ul>\n<li>将所有原子的哈希值映射到一个固定长度的位向量中。</li>\n<li>如果某个哈希值对应的位被置为 1，则表示该特征存在于分子中。</li>\n</ul>\n<p>Optuna：</p>\n<p>内含优化搜索，不用穷举式搜索，速度更快。</p>\n<p>支持连续值，网格只支持离散值</p>\n<p>LightGBM：</p>\n<p>轻量级的梯度提升树</p>\n<p>通过数据压缩使得训练更快</p>\n<p><strong>为什么不用随机森林</strong></p>\n<p>GBDT在大数据集上表现比RF好，LightGBM又是在XgBoost上进行优化的</p>\n<p><strong>有哪些特征？</strong></p>\n<p>最大叶子数量，最大深度，每个节点的最小样本数，建树随机选择的特征比例，随机选择的样本比例，正则化项系数</p>\n<p>结果：</p>\n<p>Best trial:<br>  R2: 0.32038588335457874<br>  Params:<br>    num_leaves: 250<br>    learning_rate: 0.04051655617169133<br>    max_depth: 17<br>    min_data_in_leaf: 10<br>    feature_fraction: 0.4350306415682709<br>    bagging_fraction: 0.6540225841297226<br>    bagging_freq: 1<br>    lambda_l1: 0.000524499451410933<br>    lambda_l2: 0.01259645970374942</p>\n<p>LGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题，而从实践效果来看，LGBM也确实做到了这点——LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍</p>\n<p>Optuna 相比 GridSearch 有以下优势：</p>\n<ol>\n<li><strong>效率更高</strong><ul>\n<li><strong>GridSearch</strong>：遍历所有参数组合，计算量大，耗时长。</li>\n<li><strong>Optuna</strong>：基于贝叶斯优化等算法，智能选择有潜力的参数，减少不必要的计算。</li>\n</ul>\n</li>\n<li><strong>支持复杂搜索空间</strong><ul>\n<li><strong>GridSearch</strong>：只能处理离散参数值。</li>\n<li><strong>Optuna</strong>：支持连续、离散、条件参数等多种类型，适应复杂搜索空间。</li>\n</ul>\n</li>\n<li><strong>动态调整搜索方向</strong><ul>\n<li><strong>GridSearch</strong>：固定搜索路径，无法根据结果调整。</li>\n<li><strong>Optuna</strong>：根据历史结果动态调整搜索方向，更快找到最优解。</li>\n</ul>\n</li>\n</ol>\n<p>就是要手动的交叉验证</p>\n","more":"<h1 id=\"任务概述\"><a href=\"#任务概述\" class=\"headerlink\" title=\"任务概述\"></a>任务概述</h1><p>构建一个能够准确预测碳氮成键反应产率的预测模型。  </p>\n<p>通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。</p>\n<p>或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。</p>\n<p>训练集中包含19999条反应数据，测试集中包含3539条反应数据。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。</p>\n<p><strong>评价指标</strong></p>\n<p>实验真实结果与预测结果$R^2$决定系数来进行评测:<br>$$<br>R^2(y,\\hat{y})&#x3D;1-\\frac{\\sum_{i&#x3D;1}^n(y_i-\\hat{y}<em>i)^2}{\\sum</em>{i&#x3D;1}^n(y_i-\\bar{y})^2}<br>$$</p>\n<h1 id=\"baseline\"><a href=\"#baseline\" class=\"headerlink\" title=\"baseline\"></a>baseline</h1><ol>\n<li><p><strong>导入库</strong>：首先，代码导入了需要用到的库，包括 <code>pandas</code>（用于数据处理和分析），<code>scikit-learn</code>（机器学习库），<code>rdkit</code>（化学信息工具）。</p>\n</li>\n<li><p><strong>读取数据</strong>：代码通过使用 <code>pd.read_csv</code> 函数从文件中读取训练集和测试集数据。</p>\n</li>\n<li><p><strong>使用Morgan分子指纹建模SMILES</strong>：</p>\n<p>- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。</p>\n</li>\n<li><p><strong>使用随机森林预测结果</strong>：</p>\n<p>- 这里直接调用<code>sklearn</code>的<code>RandomForestRegressor</code>模块实例化一个随机森林模型，并对<code>n_estimators</code>等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地<code>&#39;./random_forest_model.pkl&#39;</code>。</p>\n</li>\n<li><p><strong>加载模型进行预测，并将保存结果文件到本地：</strong></p>\n<p><code> pkl</code>文件直接使用<code>pickle.load()</code>加载，然后使用<code>model.predict(x)</code>进行预测。</p>\n</li>\n</ol>\n<h2 id=\"SMILES\"><a href=\"#SMILES\" class=\"headerlink\" title=\"SMILES\"></a>SMILES</h2><p>SMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。</p>\n<p>SMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。</p>\n<p>在SMILES中，原子由他们的化学符号表示，&#x3D;表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。</p>\n<p>（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）</p>\n<p>由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。</p>\n<h2 id=\"Morgan-fingerprint\"><a href=\"#Morgan-fingerprint\" class=\"headerlink\" title=\"Morgan fingerprint\"></a>Morgan fingerprint</h2><p>位向量（bit vector）形式的特征，即由0,1组成的向量。</p>\n<p>分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。</p>\n<p>通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。</p>\n<h2 id=\"RDKit\"><a href=\"#RDKit\" class=\"headerlink\" title=\"RDKit\"></a>RDKit</h2><p>RDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES</p>\n<p>RDkit是化学信息学中主要的工具，是开源的。网址：<a href=\"http://www.rdkit.org/\">http://www.rdkit.org</a><br>支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。</p>\n<h2 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h2><p>baseline的$R^2 &#x3D; 0.0745336043830066$，约0.08</p>\n<h1 id=\"My-model\"><a href=\"#My-model\" class=\"headerlink\" title=\"My model\"></a>My model</h1><p>SMILE表达式：字符串表示分子结构的化学信息学标准。</p>\n<p>分子指纹：它通过捕捉分子中每个原子的局部环境信息，将分子结构编码为固定长度的二进制向量或者整数向量</p>\n<ul>\n<li>对于每个原子，根据其邻居的标识符更新自己的标识符。</li>\n<li>更新规则通常基于哈希函数，将邻居的标识符组合成一个新的标识符。</li>\n<li>迭代次数等于半径 r。</li>\n</ul>\n<p><strong>具体的从SMILES表达式转成分子指纹的步骤：</strong></p>\n<p>在 Morgan 指纹中，每个原子的 <strong>初始标识符</strong> 是基于原子的局部环境计算的。具体来说，初始标识符通常由以下原子属性决定：</p>\n<ul>\n<li><p>原子类型（如 C、N、O 等）</p>\n</li>\n<li><p>原子的电荷</p>\n</li>\n<li><p>原子的度（连接的键数）</p>\n</li>\n<li><p>原子的氢原子数</p>\n</li>\n<li><p>其他可能的属性（如是否在环中）</p>\n</li>\n<li><p>哈希函数将标识符映射到一个固定范围的整数（通常是 0 到 $2^{n}-1$，其中 n是指纹的位数）。</p>\n</li>\n<li><p>例如，如果指纹长度是 1024 位，则哈希值的范围是 0 到 1023。</p>\n</li>\n</ul>\n<p><strong>指纹生成</strong></p>\n<ul>\n<li>将所有原子的哈希值映射到一个固定长度的位向量中。</li>\n<li>如果某个哈希值对应的位被置为 1，则表示该特征存在于分子中。</li>\n</ul>\n<p>Optuna：</p>\n<p>内含优化搜索，不用穷举式搜索，速度更快。</p>\n<p>支持连续值，网格只支持离散值</p>\n<p>LightGBM：</p>\n<p>轻量级的梯度提升树</p>\n<p>通过数据压缩使得训练更快</p>\n<p><strong>为什么不用随机森林</strong></p>\n<p>GBDT在大数据集上表现比RF好，LightGBM又是在XgBoost上进行优化的</p>\n<p><strong>有哪些特征？</strong></p>\n<p>最大叶子数量，最大深度，每个节点的最小样本数，建树随机选择的特征比例，随机选择的样本比例，正则化项系数</p>\n<p>结果：</p>\n<p>Best trial:<br>  R2: 0.32038588335457874<br>  Params:<br>    num_leaves: 250<br>    learning_rate: 0.04051655617169133<br>    max_depth: 17<br>    min_data_in_leaf: 10<br>    feature_fraction: 0.4350306415682709<br>    bagging_fraction: 0.6540225841297226<br>    bagging_freq: 1<br>    lambda_l1: 0.000524499451410933<br>    lambda_l2: 0.01259645970374942</p>\n<p>LGBM算法提出的核心目的是为了解决GBDT算法框架在处理海量数据时计算效率低下的问题，而从实践效果来看，LGBM也确实做到了这点——LGBM以牺牲极小的计算精度为代价，将GBDT的计算效率提升了近20倍</p>\n<p>Optuna 相比 GridSearch 有以下优势：</p>\n<ol>\n<li><strong>效率更高</strong><ul>\n<li><strong>GridSearch</strong>：遍历所有参数组合，计算量大，耗时长。</li>\n<li><strong>Optuna</strong>：基于贝叶斯优化等算法，智能选择有潜力的参数，减少不必要的计算。</li>\n</ul>\n</li>\n<li><strong>支持复杂搜索空间</strong><ul>\n<li><strong>GridSearch</strong>：只能处理离散参数值。</li>\n<li><strong>Optuna</strong>：支持连续、离散、条件参数等多种类型，适应复杂搜索空间。</li>\n</ul>\n</li>\n<li><strong>动态调整搜索方向</strong><ul>\n<li><strong>GridSearch</strong>：固定搜索路径，无法根据结果调整。</li>\n<li><strong>Optuna</strong>：根据历史结果动态调整搜索方向，更快找到最优解。</li>\n</ul>\n</li>\n</ol>\n<p>就是要手动的交叉验证</p>\n"},{"title":"Algorithm-Basic","mathjax":true,"date":"2023-04-27T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛基础算法","_content":"\n# 排序\n\n## 快速排序\n\n分治思想,时间复杂度$O(nlogn)-O(n^2) $\n\n期望时间复杂度$O(nlogn)$\n\n1. 数组中找一个值$x$作为分界点（可以是$arr\\left [ l \\right ]$ ,$arr\\left [ r \\right ]$,$arr\\left [ \\frac{l+r}{2} \\right ]$ 等等…）\n2. 调整区间，使得左边的区间所有数$\\le$x，右边区间所有数$>$x\n   - 定义两个指针分别在左右边界\n   - $i$不断右移，直到遇到$arr[i]$ $>x$，就停下\n   - $j$不断左移，直到遇到$arr[j]\\le x$，就停下\n   - 交换$arr[i]$与$arr[j]$\n3. 递归处理左右区间\n\n**模版**\n\n```java\npublic static void quick_sort(int q[], int l, int r){\n    if (l >= r) return;\n    int i = l - 1, j = r + 1, x = q[l + r >> 1];\n    while (i < j){\n        do i ++ ; while (q[i] < x);\n        do j -- ; while (q[j] > x);\n        if (i < j) {\n            int t = q[i];\n            q[i] = q[j];\n            q[j] = t;           \n        }\n    }\n    quick_sort(q, l, j);\n    quick_sort(q, j + 1, r);\n}\n```\n\n## 归并排序\n\n分治思想,$O(nlogn)$\n\n1. 确定分界点 $mid = \\frac{l+r}{2}$\n2. 递归排序$left$和$right$\n3. 归并：合二为一\n   - 双指针指向$left$和$right$的第一个元素\n   - 创建一个空数组$res$存放结果\n   - 指针比较，如果$left[i]<right[j]$，则把$left[i]$放入$res$，$i$向后移动一位，继续比较\n   - 如果$left[i]=right[j]$，则把$left[i]$放入$res$，以维持稳定\n\n**模版**\n\n```java\npublic static void merge_sort(int q[], int l, int r){\n    if (l >= r) return;\n    int mid = l + r >> 1;\n\n    merge_sort(q, l, mid);\n    merge_sort(q, mid + 1, r);\n\n    int k = 0, i = l, j = mid + 1;\n\n    while (i <= mid && j <= r)\n        if (q[i] < q[j]) tmp[k ++ ] = q[i ++ ];\n    else tmp[k ++ ] = q[j ++ ];\n\n    while (i <= mid) tmp[k ++ ] = q[i ++ ];\n    while (j <= r) tmp[k ++ ] = q[j ++ ];\n\n    for (i = l, j = 0; i <= r; i ++, j ++ ) q[i] = tmp[j];\n}\n```\n\n# 二分\n\n## 整数二分\n\n**提示信息**\n\n- 题目保证有解\n- 单调性\n- 求最大值的最小化\n\n**思路**\n\n对于区间$[l,r]$，其中一部分满足条件$check(x)=true$，另一部分不满足\n\n- 对于寻找不满足区间的边界\n\n$mid = \\frac{l+r+1}{2}$\n\n若$check(mid)=true$ 则说明边界值在$[mid,r]$\n更新语句为$l = mid$\n\n若$check(mid)=false$ 则说明边界值在$[l,mid-1]$\n更新语句为$r = mid-1$\n\n- 对于寻找满足区间的边界\n\n$mid = \\frac{l+r}{2}$\n\n若$check(mid)=true$ 则说明边界值在$[l,mid]$\n更新语句为$r = mid$\n\n若$check(mid)=false$ 则说明边界值在$[mid+1,r]$\n更新语句为$l=mid+1$\n\n**模版**\n\n```java\npublic static boolean check(int x) {/* ... */} // 检查x是否满足某种性质\n\n// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：\npublic static int bsearch_1(int l, int r)\n{\n    while (l < r)\n    {\n        int mid = l + r >> 1;\n        if (check(mid)) r = mid;    // check()判断mid是否满足性质\n        else l = mid + 1;\n    }\n    return l;\n}\n// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：\npublic static int bsearch_2(int l, int r)\n{\n    while (l < r)\n    {\n        int mid = l + r + 1 >> 1;\n        if (check(mid)) l = mid;\n        else r = mid - 1;\n    }\n    return l;\n}\n```\n\n！！如果$l=mid$ ，最开始的$mid$就要补上$+1$\n\n！！`check`函数中记得取等于号\n\n## 浮点数二分\n\n```java\npublic static boolean check(double x) {/* ... */} // 检查x是否满足某种性质\n\ndouble bsearch_3(double l, double r)\n{\n    final double eps = 1e-6;   \n    // eps 表示精度，取决于题目对精度的要求\n    //比需要保留的位数多2\n    while (r - l > eps)\n    {\n        double mid = (l + r) / 2;\n        if (check(mid)) r = mid;\n        else l = mid;\n    }\n    return l;\n}\n```\n\n**精度比需要保留的位数多-2次方**\n\n可以把$while$循环直接换成`for`100次\n\n# 高精度\n\n$C++$模版\n\n高精度加法\n\n```cpp\n// C = A + B, A >= 0, B >= 0\nvector<int> add(vector<int> &A, vector<int> &B)\n{\n    if (A.size() < B.size()) return add(B, A);\n\n    vector<int> C;\n    int t = 0;\n    for (int i = 0; i < A.size(); i ++ )\n    {\n        t += A[i];\n        if (i < B.size()) t += B[i];\n        C.push_back(t % 10);\n        t /= 10;\n    }\n\n    if (t) C.push_back(t);\n    return C;\n}\n```\n\n## 高精度减法\n\n```cpp\n// C = A - B, 满足A >= B, A >= 0, B >= 0\nvector<int> sub(vector<int> &A, vector<int> &B)\n{\n    vector<int> C;\n    for (int i = 0, t = 0; i < A.size(); i ++ )\n    {\n        t = A[i] - t;\n        if (i < B.size()) t -= B[i];\n        C.push_back((t + 10) % 10);\n        if (t < 0) t = 1;\n        else t = 0;\n    }\n\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n    return C;\n}\n```\n\n## 高精度乘低精度\n\n```cpp\n// C = A * b, A >= 0, b >= 0\nvector<int> mul(vector<int> &A, int b)\n{\n    vector<int> C;\n\n    int t = 0;\n    for (int i = 0; i < A.size() || t; i ++ )\n    {\n        if (i < A.size()) t += A[i] * b;\n        C.push_back(t % 10);\n        t /= 10;\n    }\n\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n\n    return C;\n}\n```\n\n## 高精度除以低精度\n\n```cpp\n// A / b = C ... r, A >= 0, b > 0\nvector<int> div(vector<int> &A, int b, int &r)\n{\n    vector<int> C;\n    r = 0;\n    for (int i = A.size() - 1; i >= 0; i -- )\n    {\n        r = r * 10 + A[i];\n        C.push_back(r / b);\n        r %= b;\n    }\n    reverse(C.begin(), C.end());\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n    return C;\n}\n```\n\n## 高精度乘以高精度\n\n```cpp\ncin>>a1>>b1;\nint lena=strlen(a1);\nint lenb=strlen(b1);\nfor(i=1;i<=lena;i++)a[i]=a1[lena-i]-'0';\nfor(i=1;i<=lenb;i++)b[i]=b1[lenb-i]-'0';\nfor(i=1;i<=lenb;i++)\n    for(j=1;j<=lena;j++)\n        c[i+j-1]+=a[j]*b[i];\nfor(i=1;i<lena+lenb;i++)\n    if(c[i]>9)\n    {\n        c[i+1]+=c[i]/10;\n        c[i]%=10;\n    }\nlen=lena+lenb;\nwhile(c[len]==0&&len>1)len--;\n```\n\n# 前缀和与差分\n\n一对逆运算\n\n## 一维前缀和\n\n设有一列数据${a}_1,{a}_2,...,{a}_{n-1},{a}_n$\n\n定义${S}_i=a_1+a_2+...+a_i$\n\n一般下标从1开始，$S_0=0$\n\n$S_i$的初始化: $S_i = S_{i-1}+a_i$\n\n**作用**\n\n快速地求出原数组中一段区间数的和\n\n对于区间$[l,r]$\n\n$\\sum_{i=l}^{r}a_i = S_r-S_{l-1}$\n\n## 二维前缀和\n\n对于二维数组（矩阵）$\\begin{pmatrix} a_{11}& a_{12} & ... & a_{1j}\\\\ a_{21}& a_{22} & ... & a_{2j} \\\\ ...& ... & ... & ...\\\\ a_{i1}& a_{i2} & ... & a_{ij} \\end{pmatrix}$\n\n$S_{ij}$代表$a_{ij}$左上角的所有元素和\n\n- 对于点$(i,j)$，其二维前缀和$S_{ij}$的初始化\n\n  $S_{ij}=S_{i-1,j}+S_{i,j-1}-S_{i-1,j-1}+a_{i,j}$\n\n- 设点$(x_1,y_1)$在$(x_2,y_2)$的左上角，则两点围成的矩形中所有元素和\n  $S=S_{x_2,y_2}-S_{x_2,y_1-1}-S_{x_1-1,y_2}+S_{x_1-1,y_1-1}$\n\n## 一维差分\n\n对一列数据$a_1,a_2,a_3,...,a_i$\n\n构造$b_1,b_2,b_3,...,b_i$使得$a_i=b_1+b_2+...+b_i$\n\n即$a$为$b$的前缀和，$b$就是$a$的差分\n\n$\\left\\{\\begin{matrix} b_1=a_1\\\\ b_2=a_2-a_1\\\\ b_3=a_3-a_2\\\\ ......\\\\ b_n=a_n-a_{n-1} \\end{matrix}\\right.$\n\n**作用**\n\n若要把$a_1,a_2,a_3,...,a_i$中$[l,r]$区间的$a$加$c$\n\n只需要使$b_l+=c,b_{r+1}-=c$\n\n**模版**\n\n```java\nimport java.util.Scanner;\n\npublic class Diff {\n    public static void main(String[] args) {\n\n        Scanner scanner = new Scanner(System.in);\n        // 给出n数组大小和k增加次数\n        int n = scanner.nextInt();\n        int k = scanner.nextInt();\n\n        // 搭建数组\n        int[] arr = new int[n+1];\n        int[] brr = new int[n+1];\n\n        // 为arr赋值\n        for (int i = 1; i < n+1; i++) {\n            arr[i] = scanner.nextInt();\n        }\n\n        // 为brr赋值\n        for (int i = 1; i < n+1; i++){\n            brr[i] = arr[i] - arr[i-1];\n        }\n\n        while (k-- > 0){\n            // 我们为arr的[l,r]区间加上c\n            int l = scanner.nextInt();\n            int r = scanner.nextInt();\n            int c = scanner.nextInt();\n\n            brr[l] += c;\n            brr[r+1] -= c;\n        }\n\n        // 计算输出结果即可（这里输出的需要是由b累计出来的a）\n        // 也可以使用注释代码，最后输出arr即可\n        for (int i = 1; i < n+1; i++) {\n            brr[i] += brr[i-1];\n            //arr[i] = brr[i]+arr[i-1];\n        }\n\n        // 最后输出结果\n        for (int i = 1; i < n+1; i++) {\n            System.out.println(brr[i]);\n        }\n\n    }\n}\n```\n\n## 二维差分\n\n原矩阵$a_{ij}$,差分矩阵$b_{ij}$\n\n$b_{x1,y1}+=c$\n\n$b_{x2+1,y1}-=c$\n\n$b_{x1,y2+1}-=c$\n\n$b_{x2+1,y2+1}+=c$\n\n# 其他\n\n## 双指针算法\n\n- 两个序列，两个指针\n- **一个序列，两个指针**\n\n**结构**\n\n```cpp\nfor(int i=0,j=0;i<n;i++){\n    while(j<i && check(i,j)) j++;\n    //每道题具体的逻辑\n}\n```\n\n**核心思想**\n\n复杂度由$O(n^2)$优化到$O(n)$\n\n先想出朴素做法，寻找i与j之间的关系，是否有单调性，进行双指针优化\n\n[A-B数对](https://www.luogu.com.cn/problem/P1102)\n\n## 位运算\n\n计算机以二进制表示数据，以表示电路中的正反。在二进制下，一个位只有0和1。逢二进一位。\n\n计算机中存储数据，以字节为单位，一个字节有8个位，即可以表示-128~127范围的数字。\n\n### 基础运算\n\n> 与\n\n用符号&表示，运算规律是：真真为真，真假为假，假假为假（一假即假）\n\n```bash\n1&1 //1\n1&0 //0\n0&0 //0\n```\n\n> 或\n\n用符号|表示，运算规律是：真真为真，真假为真，假假为假（一真即真）\n\n```bash\n1|1 //1\n1|0 //1\n0|0 //0\n```\n\n> 非\n\n运算符为~，取反的逻辑，运算规律：二进制位若为1，取反后为0。若为0，取反后为1\n\n```bash\n~1 //11111110\n```\n\n> 左移\n\n将二进制数向左移位操作，高位溢出则丢弃，低位补0\n\n```bash\na=11;\na<<1;\n移位前：0000 1011\n移位后：0001 0110（十进制值为22）\n```\n\n对一个数左移1位就是乘以2，左移n位就是乘以2的n次方（而左移运算比乘法快得多）\n\n> 右移\n\n右移位运算中，无符号数和有符号数的运算并不相同。对于无符号数，右移之后高位补0；对于有符号数，符号位一起移动，正数高位补0，负数高位补1\n\n```text\n无符号数\na=16;\na>>3;\n移位前：0001 0000\n移位后：0000 0010（十进制值为2）\n有符号数（正数）\nb=32;\na>>3;\n移位前：0010 0000\n移位后：0000 0100（十进制值位4）\n有符号数（负数）\nb=-32;\nb>>3;\n移位前：1010 0000\n移位后：1000 0100（十进制值为-4）\n```\n\n- n的二进制表示中第k位数字是几\n\n（k从个位开始算0,1,2…）\n\n1. 先把第k位移到最后一位`n>>k`\n\n2. 看个位是几 `x&1`\n\n   **n>>k&1**\n\n- lowbit(x)\n\n  树状数组基本操作，返回x的最后一位1\n\n  **x&(-x)**\n\n  原理：`-x=(~x+1)`\n\n## 区间合并\n\n- 按照区间左端点排序\n\n- 判断下一个区间与当前区间的关系\n\n  - 相交\n\n    - 更新右端点为两个区间的$max$\n\n  - 不相交\n\n    - 将当前区间更新为不相交的这个区间\n\n    **C++模版**\n\n```cpp\n// 将所有存在交集的区间合并\nvoid merge(vector<PII> &segs)\n{\n    vector<PII> res;\n\n    sort(segs.begin(), segs.end());\n\n    int st = -2e9, ed = -2e9;\n    for (auto seg : segs)\n        if (ed < seg.first)\n        {\n            if (st != -2e9) res.push_back({st, ed});\n            st = seg.first, ed = seg.second;\n        }\n        else ed = max(ed, seg.second);\n\n    if (st != -2e9) res.push_back({st, ed});\n\n    segs = res;\n}\n```","source":"_posts/algorithm-basic.md","raw":"---\ntitle: Algorithm-Basic\nmathjax: true\ndate: 2023/4/27 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛基础算法\n---\n\n# 排序\n\n## 快速排序\n\n分治思想,时间复杂度$O(nlogn)-O(n^2) $\n\n期望时间复杂度$O(nlogn)$\n\n1. 数组中找一个值$x$作为分界点（可以是$arr\\left [ l \\right ]$ ,$arr\\left [ r \\right ]$,$arr\\left [ \\frac{l+r}{2} \\right ]$ 等等…）\n2. 调整区间，使得左边的区间所有数$\\le$x，右边区间所有数$>$x\n   - 定义两个指针分别在左右边界\n   - $i$不断右移，直到遇到$arr[i]$ $>x$，就停下\n   - $j$不断左移，直到遇到$arr[j]\\le x$，就停下\n   - 交换$arr[i]$与$arr[j]$\n3. 递归处理左右区间\n\n**模版**\n\n```java\npublic static void quick_sort(int q[], int l, int r){\n    if (l >= r) return;\n    int i = l - 1, j = r + 1, x = q[l + r >> 1];\n    while (i < j){\n        do i ++ ; while (q[i] < x);\n        do j -- ; while (q[j] > x);\n        if (i < j) {\n            int t = q[i];\n            q[i] = q[j];\n            q[j] = t;           \n        }\n    }\n    quick_sort(q, l, j);\n    quick_sort(q, j + 1, r);\n}\n```\n\n## 归并排序\n\n分治思想,$O(nlogn)$\n\n1. 确定分界点 $mid = \\frac{l+r}{2}$\n2. 递归排序$left$和$right$\n3. 归并：合二为一\n   - 双指针指向$left$和$right$的第一个元素\n   - 创建一个空数组$res$存放结果\n   - 指针比较，如果$left[i]<right[j]$，则把$left[i]$放入$res$，$i$向后移动一位，继续比较\n   - 如果$left[i]=right[j]$，则把$left[i]$放入$res$，以维持稳定\n\n**模版**\n\n```java\npublic static void merge_sort(int q[], int l, int r){\n    if (l >= r) return;\n    int mid = l + r >> 1;\n\n    merge_sort(q, l, mid);\n    merge_sort(q, mid + 1, r);\n\n    int k = 0, i = l, j = mid + 1;\n\n    while (i <= mid && j <= r)\n        if (q[i] < q[j]) tmp[k ++ ] = q[i ++ ];\n    else tmp[k ++ ] = q[j ++ ];\n\n    while (i <= mid) tmp[k ++ ] = q[i ++ ];\n    while (j <= r) tmp[k ++ ] = q[j ++ ];\n\n    for (i = l, j = 0; i <= r; i ++, j ++ ) q[i] = tmp[j];\n}\n```\n\n# 二分\n\n## 整数二分\n\n**提示信息**\n\n- 题目保证有解\n- 单调性\n- 求最大值的最小化\n\n**思路**\n\n对于区间$[l,r]$，其中一部分满足条件$check(x)=true$，另一部分不满足\n\n- 对于寻找不满足区间的边界\n\n$mid = \\frac{l+r+1}{2}$\n\n若$check(mid)=true$ 则说明边界值在$[mid,r]$\n更新语句为$l = mid$\n\n若$check(mid)=false$ 则说明边界值在$[l,mid-1]$\n更新语句为$r = mid-1$\n\n- 对于寻找满足区间的边界\n\n$mid = \\frac{l+r}{2}$\n\n若$check(mid)=true$ 则说明边界值在$[l,mid]$\n更新语句为$r = mid$\n\n若$check(mid)=false$ 则说明边界值在$[mid+1,r]$\n更新语句为$l=mid+1$\n\n**模版**\n\n```java\npublic static boolean check(int x) {/* ... */} // 检查x是否满足某种性质\n\n// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：\npublic static int bsearch_1(int l, int r)\n{\n    while (l < r)\n    {\n        int mid = l + r >> 1;\n        if (check(mid)) r = mid;    // check()判断mid是否满足性质\n        else l = mid + 1;\n    }\n    return l;\n}\n// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：\npublic static int bsearch_2(int l, int r)\n{\n    while (l < r)\n    {\n        int mid = l + r + 1 >> 1;\n        if (check(mid)) l = mid;\n        else r = mid - 1;\n    }\n    return l;\n}\n```\n\n！！如果$l=mid$ ，最开始的$mid$就要补上$+1$\n\n！！`check`函数中记得取等于号\n\n## 浮点数二分\n\n```java\npublic static boolean check(double x) {/* ... */} // 检查x是否满足某种性质\n\ndouble bsearch_3(double l, double r)\n{\n    final double eps = 1e-6;   \n    // eps 表示精度，取决于题目对精度的要求\n    //比需要保留的位数多2\n    while (r - l > eps)\n    {\n        double mid = (l + r) / 2;\n        if (check(mid)) r = mid;\n        else l = mid;\n    }\n    return l;\n}\n```\n\n**精度比需要保留的位数多-2次方**\n\n可以把$while$循环直接换成`for`100次\n\n# 高精度\n\n$C++$模版\n\n高精度加法\n\n```cpp\n// C = A + B, A >= 0, B >= 0\nvector<int> add(vector<int> &A, vector<int> &B)\n{\n    if (A.size() < B.size()) return add(B, A);\n\n    vector<int> C;\n    int t = 0;\n    for (int i = 0; i < A.size(); i ++ )\n    {\n        t += A[i];\n        if (i < B.size()) t += B[i];\n        C.push_back(t % 10);\n        t /= 10;\n    }\n\n    if (t) C.push_back(t);\n    return C;\n}\n```\n\n## 高精度减法\n\n```cpp\n// C = A - B, 满足A >= B, A >= 0, B >= 0\nvector<int> sub(vector<int> &A, vector<int> &B)\n{\n    vector<int> C;\n    for (int i = 0, t = 0; i < A.size(); i ++ )\n    {\n        t = A[i] - t;\n        if (i < B.size()) t -= B[i];\n        C.push_back((t + 10) % 10);\n        if (t < 0) t = 1;\n        else t = 0;\n    }\n\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n    return C;\n}\n```\n\n## 高精度乘低精度\n\n```cpp\n// C = A * b, A >= 0, b >= 0\nvector<int> mul(vector<int> &A, int b)\n{\n    vector<int> C;\n\n    int t = 0;\n    for (int i = 0; i < A.size() || t; i ++ )\n    {\n        if (i < A.size()) t += A[i] * b;\n        C.push_back(t % 10);\n        t /= 10;\n    }\n\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n\n    return C;\n}\n```\n\n## 高精度除以低精度\n\n```cpp\n// A / b = C ... r, A >= 0, b > 0\nvector<int> div(vector<int> &A, int b, int &r)\n{\n    vector<int> C;\n    r = 0;\n    for (int i = A.size() - 1; i >= 0; i -- )\n    {\n        r = r * 10 + A[i];\n        C.push_back(r / b);\n        r %= b;\n    }\n    reverse(C.begin(), C.end());\n    while (C.size() > 1 && C.back() == 0) C.pop_back();\n    return C;\n}\n```\n\n## 高精度乘以高精度\n\n```cpp\ncin>>a1>>b1;\nint lena=strlen(a1);\nint lenb=strlen(b1);\nfor(i=1;i<=lena;i++)a[i]=a1[lena-i]-'0';\nfor(i=1;i<=lenb;i++)b[i]=b1[lenb-i]-'0';\nfor(i=1;i<=lenb;i++)\n    for(j=1;j<=lena;j++)\n        c[i+j-1]+=a[j]*b[i];\nfor(i=1;i<lena+lenb;i++)\n    if(c[i]>9)\n    {\n        c[i+1]+=c[i]/10;\n        c[i]%=10;\n    }\nlen=lena+lenb;\nwhile(c[len]==0&&len>1)len--;\n```\n\n# 前缀和与差分\n\n一对逆运算\n\n## 一维前缀和\n\n设有一列数据${a}_1,{a}_2,...,{a}_{n-1},{a}_n$\n\n定义${S}_i=a_1+a_2+...+a_i$\n\n一般下标从1开始，$S_0=0$\n\n$S_i$的初始化: $S_i = S_{i-1}+a_i$\n\n**作用**\n\n快速地求出原数组中一段区间数的和\n\n对于区间$[l,r]$\n\n$\\sum_{i=l}^{r}a_i = S_r-S_{l-1}$\n\n## 二维前缀和\n\n对于二维数组（矩阵）$\\begin{pmatrix} a_{11}& a_{12} & ... & a_{1j}\\\\ a_{21}& a_{22} & ... & a_{2j} \\\\ ...& ... & ... & ...\\\\ a_{i1}& a_{i2} & ... & a_{ij} \\end{pmatrix}$\n\n$S_{ij}$代表$a_{ij}$左上角的所有元素和\n\n- 对于点$(i,j)$，其二维前缀和$S_{ij}$的初始化\n\n  $S_{ij}=S_{i-1,j}+S_{i,j-1}-S_{i-1,j-1}+a_{i,j}$\n\n- 设点$(x_1,y_1)$在$(x_2,y_2)$的左上角，则两点围成的矩形中所有元素和\n  $S=S_{x_2,y_2}-S_{x_2,y_1-1}-S_{x_1-1,y_2}+S_{x_1-1,y_1-1}$\n\n## 一维差分\n\n对一列数据$a_1,a_2,a_3,...,a_i$\n\n构造$b_1,b_2,b_3,...,b_i$使得$a_i=b_1+b_2+...+b_i$\n\n即$a$为$b$的前缀和，$b$就是$a$的差分\n\n$\\left\\{\\begin{matrix} b_1=a_1\\\\ b_2=a_2-a_1\\\\ b_3=a_3-a_2\\\\ ......\\\\ b_n=a_n-a_{n-1} \\end{matrix}\\right.$\n\n**作用**\n\n若要把$a_1,a_2,a_3,...,a_i$中$[l,r]$区间的$a$加$c$\n\n只需要使$b_l+=c,b_{r+1}-=c$\n\n**模版**\n\n```java\nimport java.util.Scanner;\n\npublic class Diff {\n    public static void main(String[] args) {\n\n        Scanner scanner = new Scanner(System.in);\n        // 给出n数组大小和k增加次数\n        int n = scanner.nextInt();\n        int k = scanner.nextInt();\n\n        // 搭建数组\n        int[] arr = new int[n+1];\n        int[] brr = new int[n+1];\n\n        // 为arr赋值\n        for (int i = 1; i < n+1; i++) {\n            arr[i] = scanner.nextInt();\n        }\n\n        // 为brr赋值\n        for (int i = 1; i < n+1; i++){\n            brr[i] = arr[i] - arr[i-1];\n        }\n\n        while (k-- > 0){\n            // 我们为arr的[l,r]区间加上c\n            int l = scanner.nextInt();\n            int r = scanner.nextInt();\n            int c = scanner.nextInt();\n\n            brr[l] += c;\n            brr[r+1] -= c;\n        }\n\n        // 计算输出结果即可（这里输出的需要是由b累计出来的a）\n        // 也可以使用注释代码，最后输出arr即可\n        for (int i = 1; i < n+1; i++) {\n            brr[i] += brr[i-1];\n            //arr[i] = brr[i]+arr[i-1];\n        }\n\n        // 最后输出结果\n        for (int i = 1; i < n+1; i++) {\n            System.out.println(brr[i]);\n        }\n\n    }\n}\n```\n\n## 二维差分\n\n原矩阵$a_{ij}$,差分矩阵$b_{ij}$\n\n$b_{x1,y1}+=c$\n\n$b_{x2+1,y1}-=c$\n\n$b_{x1,y2+1}-=c$\n\n$b_{x2+1,y2+1}+=c$\n\n# 其他\n\n## 双指针算法\n\n- 两个序列，两个指针\n- **一个序列，两个指针**\n\n**结构**\n\n```cpp\nfor(int i=0,j=0;i<n;i++){\n    while(j<i && check(i,j)) j++;\n    //每道题具体的逻辑\n}\n```\n\n**核心思想**\n\n复杂度由$O(n^2)$优化到$O(n)$\n\n先想出朴素做法，寻找i与j之间的关系，是否有单调性，进行双指针优化\n\n[A-B数对](https://www.luogu.com.cn/problem/P1102)\n\n## 位运算\n\n计算机以二进制表示数据，以表示电路中的正反。在二进制下，一个位只有0和1。逢二进一位。\n\n计算机中存储数据，以字节为单位，一个字节有8个位，即可以表示-128~127范围的数字。\n\n### 基础运算\n\n> 与\n\n用符号&表示，运算规律是：真真为真，真假为假，假假为假（一假即假）\n\n```bash\n1&1 //1\n1&0 //0\n0&0 //0\n```\n\n> 或\n\n用符号|表示，运算规律是：真真为真，真假为真，假假为假（一真即真）\n\n```bash\n1|1 //1\n1|0 //1\n0|0 //0\n```\n\n> 非\n\n运算符为~，取反的逻辑，运算规律：二进制位若为1，取反后为0。若为0，取反后为1\n\n```bash\n~1 //11111110\n```\n\n> 左移\n\n将二进制数向左移位操作，高位溢出则丢弃，低位补0\n\n```bash\na=11;\na<<1;\n移位前：0000 1011\n移位后：0001 0110（十进制值为22）\n```\n\n对一个数左移1位就是乘以2，左移n位就是乘以2的n次方（而左移运算比乘法快得多）\n\n> 右移\n\n右移位运算中，无符号数和有符号数的运算并不相同。对于无符号数，右移之后高位补0；对于有符号数，符号位一起移动，正数高位补0，负数高位补1\n\n```text\n无符号数\na=16;\na>>3;\n移位前：0001 0000\n移位后：0000 0010（十进制值为2）\n有符号数（正数）\nb=32;\na>>3;\n移位前：0010 0000\n移位后：0000 0100（十进制值位4）\n有符号数（负数）\nb=-32;\nb>>3;\n移位前：1010 0000\n移位后：1000 0100（十进制值为-4）\n```\n\n- n的二进制表示中第k位数字是几\n\n（k从个位开始算0,1,2…）\n\n1. 先把第k位移到最后一位`n>>k`\n\n2. 看个位是几 `x&1`\n\n   **n>>k&1**\n\n- lowbit(x)\n\n  树状数组基本操作，返回x的最后一位1\n\n  **x&(-x)**\n\n  原理：`-x=(~x+1)`\n\n## 区间合并\n\n- 按照区间左端点排序\n\n- 判断下一个区间与当前区间的关系\n\n  - 相交\n\n    - 更新右端点为两个区间的$max$\n\n  - 不相交\n\n    - 将当前区间更新为不相交的这个区间\n\n    **C++模版**\n\n```cpp\n// 将所有存在交集的区间合并\nvoid merge(vector<PII> &segs)\n{\n    vector<PII> res;\n\n    sort(segs.begin(), segs.end());\n\n    int st = -2e9, ed = -2e9;\n    for (auto seg : segs)\n        if (ed < seg.first)\n        {\n            if (st != -2e9) res.push_back({st, ed});\n            st = seg.first, ed = seg.second;\n        }\n        else ed = max(ed, seg.second);\n\n    if (st != -2e9) res.push_back({st, ed});\n\n    segs = res;\n}\n```","slug":"algorithm-basic","published":1,"updated":"2025-02-28T03:03:46.487Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9b0003bs999jba3ws0","content":"<h1 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h1><h2 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h2><p>分治思想,时间复杂度$O(nlogn)-O(n^2) $</p>\n<p>期望时间复杂度$O(nlogn)$</p>\n<ol>\n<li>数组中找一个值$x$作为分界点（可以是$arr\\left [ l \\right ]$ ,$arr\\left [ r \\right ]$,$arr\\left [ \\frac{l+r}{2} \\right ]$ 等等…）</li>\n<li>调整区间，使得左边的区间所有数$\\le$x，右边区间所有数$&gt;$x<ul>\n<li>定义两个指针分别在左右边界</li>\n<li>$i$不断右移，直到遇到$arr[i]$ $&gt;x$，就停下</li>\n<li>$j$不断左移，直到遇到$arr[j]\\le x$，就停下</li>\n<li>交换$arr[i]$与$arr[j]$</li>\n</ul>\n</li>\n<li>递归处理左右区间</li>\n</ol>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">quick_sort</span><span class=\"params\">(<span class=\"type\">int</span> q[], <span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (l &gt;= r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> l - <span class=\"number\">1</span>, j = r + <span class=\"number\">1</span>, x = q[l + r &gt;&gt; <span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt; j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i ++ ; <span class=\"keyword\">while</span> (q[i] &lt; x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j -- ; <span class=\"keyword\">while</span> (q[j] &gt; x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; j) &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">t</span> <span class=\"operator\">=</span> q[i];</span><br><span class=\"line\">            q[i] = q[j];</span><br><span class=\"line\">            q[j] = t;           </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    quick_sort(q, l, j);</span><br><span class=\"line\">    quick_sort(q, j + <span class=\"number\">1</span>, r);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h2><p>分治思想,$O(nlogn)$</p>\n<ol>\n<li>确定分界点 $mid &#x3D; \\frac{l+r}{2}$</li>\n<li>递归排序$left$和$right$</li>\n<li>归并：合二为一<ul>\n<li>双指针指向$left$和$right$的第一个元素</li>\n<li>创建一个空数组$res$存放结果</li>\n<li>指针比较，如果$left[i]&lt;right[j]$，则把$left[i]$放入$res$，$i$向后移动一位，继续比较</li>\n<li>如果$left[i]&#x3D;right[j]$，则把$left[i]$放入$res$，以维持稳定</li>\n</ul>\n</li>\n</ol>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">merge_sort</span><span class=\"params\">(<span class=\"type\">int</span> q[], <span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (l &gt;= r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    merge_sort(q, l, mid);</span><br><span class=\"line\">    merge_sort(q, mid + <span class=\"number\">1</span>, r);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">k</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>, i = l, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt;= mid &amp;&amp; j &lt;= r)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (q[i] &lt; q[j]) tmp[k ++ ] = q[i ++ ];</span><br><span class=\"line\">    <span class=\"keyword\">else</span> tmp[k ++ ] = q[j ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt;= mid) tmp[k ++ ] = q[i ++ ];</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &lt;= r) tmp[k ++ ] = q[j ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (i = l, j = <span class=\"number\">0</span>; i &lt;= r; i ++, j ++ ) q[i] = tmp[j];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"二分\"><a href=\"#二分\" class=\"headerlink\" title=\"二分\"></a>二分</h1><h2 id=\"整数二分\"><a href=\"#整数二分\" class=\"headerlink\" title=\"整数二分\"></a>整数二分</h2><p><strong>提示信息</strong></p>\n<ul>\n<li>题目保证有解</li>\n<li>单调性</li>\n<li>求最大值的最小化</li>\n</ul>\n<p><strong>思路</strong></p>\n<p>对于区间$[l,r]$，其中一部分满足条件$check(x)&#x3D;true$，另一部分不满足</p>\n<ul>\n<li>对于寻找不满足区间的边界</li>\n</ul>\n<p>$mid &#x3D; \\frac{l+r+1}{2}$</p>\n<p>若$check(mid)&#x3D;true$ 则说明边界值在$[mid,r]$<br>更新语句为$l &#x3D; mid$</p>\n<p>若$check(mid)&#x3D;false$ 则说明边界值在$[l,mid-1]$<br>更新语句为$r &#x3D; mid-1$</p>\n<ul>\n<li>对于寻找满足区间的边界</li>\n</ul>\n<p>$mid &#x3D; \\frac{l+r}{2}$</p>\n<p>若$check(mid)&#x3D;true$ 则说明边界值在$[l,mid]$<br>更新语句为$r &#x3D; mid$</p>\n<p>若$check(mid)&#x3D;false$ 则说明边界值在$[mid+1,r]$<br>更新语句为$l&#x3D;mid+1$</p>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">boolean</span> <span class=\"title function_\">check</span><span class=\"params\">(<span class=\"type\">int</span> x)</span> &#123;<span class=\"comment\">/* ... */</span>&#125; <span class=\"comment\">// 检查x是否满足某种性质</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">bsearch_1</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (l &lt; r)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) r = mid;    <span class=\"comment\">// check()判断mid是否满足性质</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">bsearch_2</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (l &lt; r)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r + <span class=\"number\">1</span> &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) l = mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> r = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>！！如果$l&#x3D;mid$ ，最开始的$mid$就要补上$+1$</p>\n<p>！！<code>check</code>函数中记得取等于号</p>\n<h2 id=\"浮点数二分\"><a href=\"#浮点数二分\" class=\"headerlink\" title=\"浮点数二分\"></a>浮点数二分</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">boolean</span> <span class=\"title function_\">check</span><span class=\"params\">(<span class=\"type\">double</span> x)</span> &#123;<span class=\"comment\">/* ... */</span>&#125; <span class=\"comment\">// 检查x是否满足某种性质</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">double</span> <span class=\"title function_\">bsearch_3</span><span class=\"params\">(<span class=\"type\">double</span> l, <span class=\"type\">double</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> <span class=\"type\">double</span> <span class=\"variable\">eps</span> <span class=\"operator\">=</span> <span class=\"number\">1e-6</span>;   </span><br><span class=\"line\">    <span class=\"comment\">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class=\"line\">    <span class=\"comment\">//比需要保留的位数多2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (r - l &gt; eps)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">double</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> (l + r) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) r = mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>精度比需要保留的位数多-2次方</strong></p>\n<p>可以把$while$循环直接换成<code>for</code>100次</p>\n<h1 id=\"高精度\"><a href=\"#高精度\" class=\"headerlink\" title=\"高精度\"></a>高精度</h1><p>$C++$模版</p>\n<p>高精度加法</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A + B, A &gt;= 0, B &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">add</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, vector&lt;<span class=\"type\">int</span>&gt; &amp;B)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (A.<span class=\"built_in\">size</span>() &lt; B.<span class=\"built_in\">size</span>()) <span class=\"keyword\">return</span> <span class=\"built_in\">add</span>(B, A);</span><br><span class=\"line\"></span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t += A[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; B.<span class=\"built_in\">size</span>()) t += B[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (t) C.<span class=\"built_in\">push_back</span>(t);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度减法\"><a href=\"#高精度减法\" class=\"headerlink\" title=\"高精度减法\"></a>高精度减法</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A - B, 满足A &gt;= B, A &gt;= 0, B &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">sub</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, vector&lt;<span class=\"type\">int</span>&gt; &amp;B)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>, t = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t = A[i] - t;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; B.<span class=\"built_in\">size</span>()) t -= B[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>((t + <span class=\"number\">10</span>) % <span class=\"number\">10</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t &lt; <span class=\"number\">0</span>) t = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度乘低精度\"><a href=\"#高精度乘低精度\" class=\"headerlink\" title=\"高精度乘低精度\"></a>高精度乘低精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A * b, A &gt;= 0, b &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">mul</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>() || t; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; A.<span class=\"built_in\">size</span>()) t += A[i] * b;</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度除以低精度\"><a href=\"#高精度除以低精度\" class=\"headerlink\" title=\"高精度除以低精度\"></a>高精度除以低精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// A / b = C ... r, A &gt;= 0, b &gt; 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">div</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, <span class=\"type\">int</span> b, <span class=\"type\">int</span> &amp;r)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    r = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = A.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; i -- )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        r = r * <span class=\"number\">10</span> + A[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(r / b);</span><br><span class=\"line\">        r %= b;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">reverse</span>(C.<span class=\"built_in\">begin</span>(), C.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度乘以高精度\"><a href=\"#高精度乘以高精度\" class=\"headerlink\" title=\"高精度乘以高精度\"></a>高精度乘以高精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cin&gt;&gt;a1&gt;&gt;b1;</span><br><span class=\"line\"><span class=\"type\">int</span> lena=<span class=\"built_in\">strlen</span>(a1);</span><br><span class=\"line\"><span class=\"type\">int</span> lenb=<span class=\"built_in\">strlen</span>(b1);</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lena;i++)a[i]=a1[lena-i]-<span class=\"string\">&#x27;0&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lenb;i++)b[i]=b1[lenb-i]-<span class=\"string\">&#x27;0&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lenb;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=lena;j++)</span><br><span class=\"line\">        c[i+j<span class=\"number\">-1</span>]+=a[j]*b[i];</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;lena+lenb;i++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(c[i]&gt;<span class=\"number\">9</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[i<span class=\"number\">+1</span>]+=c[i]/<span class=\"number\">10</span>;</span><br><span class=\"line\">        c[i]%=<span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">len=lena+lenb;</span><br><span class=\"line\"><span class=\"keyword\">while</span>(c[len]==<span class=\"number\">0</span>&amp;&amp;len&gt;<span class=\"number\">1</span>)len--;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"前缀和与差分\"><a href=\"#前缀和与差分\" class=\"headerlink\" title=\"前缀和与差分\"></a>前缀和与差分</h1><p>一对逆运算</p>\n<h2 id=\"一维前缀和\"><a href=\"#一维前缀和\" class=\"headerlink\" title=\"一维前缀和\"></a>一维前缀和</h2><p>设有一列数据${a}_1,{a}<em>2,…,{a}</em>{n-1},{a}_n$</p>\n<p>定义${S}_i&#x3D;a_1+a_2+…+a_i$</p>\n<p>一般下标从1开始，$S_0&#x3D;0$</p>\n<p>$S_i$的初始化: $S_i &#x3D; S_{i-1}+a_i$</p>\n<p><strong>作用</strong></p>\n<p>快速地求出原数组中一段区间数的和</p>\n<p>对于区间$[l,r]$</p>\n<p>$\\sum_{i&#x3D;l}^{r}a_i &#x3D; S_r-S_{l-1}$</p>\n<h2 id=\"二维前缀和\"><a href=\"#二维前缀和\" class=\"headerlink\" title=\"二维前缀和\"></a>二维前缀和</h2><p>对于二维数组（矩阵）$\\begin{pmatrix} a_{11}&amp; a_{12} &amp; … &amp; a_{1j}\\ a_{21}&amp; a_{22} &amp; … &amp; a_{2j} \\ …&amp; … &amp; … &amp; …\\ a_{i1}&amp; a_{i2} &amp; … &amp; a_{ij} \\end{pmatrix}$</p>\n<p>$S_{ij}$代表$a_{ij}$左上角的所有元素和</p>\n<ul>\n<li><p>对于点$(i,j)$，其二维前缀和$S_{ij}$的初始化</p>\n<p>$S_{ij}&#x3D;S_{i-1,j}+S_{i,j-1}-S_{i-1,j-1}+a_{i,j}$</p>\n</li>\n<li><p>设点$(x_1,y_1)$在$(x_2,y_2)$的左上角，则两点围成的矩形中所有元素和<br>$S&#x3D;S_{x_2,y_2}-S_{x_2,y_1-1}-S_{x_1-1,y_2}+S_{x_1-1,y_1-1}$</p>\n</li>\n</ul>\n<h2 id=\"一维差分\"><a href=\"#一维差分\" class=\"headerlink\" title=\"一维差分\"></a>一维差分</h2><p>对一列数据$a_1,a_2,a_3,…,a_i$</p>\n<p>构造$b_1,b_2,b_3,…,b_i$使得$a_i&#x3D;b_1+b_2+…+b_i$</p>\n<p>即$a$为$b$的前缀和，$b$就是$a$的差分</p>\n<p>$\\left{\\begin{matrix} b_1&#x3D;a_1\\ b_2&#x3D;a_2-a_1\\ b_3&#x3D;a_3-a_2\\ ……\\ b_n&#x3D;a_n-a_{n-1} \\end{matrix}\\right.$</p>\n<p><strong>作用</strong></p>\n<p>若要把$a_1,a_2,a_3,…,a_i$中$[l,r]$区间的$a$加$c$</p>\n<p>只需要使$b_l+&#x3D;c,b_{r+1}-&#x3D;c$</p>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.Scanner;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Diff</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Scanner</span> <span class=\"variable\">scanner</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Scanner</span>(System.in);</span><br><span class=\"line\">        <span class=\"comment\">// 给出n数组大小和k增加次数</span></span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">n</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">k</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 搭建数组</span></span><br><span class=\"line\">        <span class=\"type\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[n+<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"type\">int</span>[] brr = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[n+<span class=\"number\">1</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 为arr赋值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            arr[i] = scanner.nextInt();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 为brr赋值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++)&#123;</span><br><span class=\"line\">            brr[i] = arr[i] - arr[i-<span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (k-- &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 我们为arr的[l,r]区间加上c</span></span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">r</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">c</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\"></span><br><span class=\"line\">            brr[l] += c;</span><br><span class=\"line\">            brr[r+<span class=\"number\">1</span>] -= c;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 计算输出结果即可（这里输出的需要是由b累计出来的a）</span></span><br><span class=\"line\">        <span class=\"comment\">// 也可以使用注释代码，最后输出arr即可</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            brr[i] += brr[i-<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"comment\">//arr[i] = brr[i]+arr[i-1];</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 最后输出结果</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            System.out.println(brr[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"二维差分\"><a href=\"#二维差分\" class=\"headerlink\" title=\"二维差分\"></a>二维差分</h2><p>原矩阵$a_{ij}$,差分矩阵$b_{ij}$</p>\n<p>$b_{x1,y1}+&#x3D;c$</p>\n<p>$b_{x2+1,y1}-&#x3D;c$</p>\n<p>$b_{x1,y2+1}-&#x3D;c$</p>\n<p>$b_{x2+1,y2+1}+&#x3D;c$</p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><h2 id=\"双指针算法\"><a href=\"#双指针算法\" class=\"headerlink\" title=\"双指针算法\"></a>双指针算法</h2><ul>\n<li>两个序列，两个指针</li>\n<li><strong>一个序列，两个指针</strong></li>\n</ul>\n<p><strong>结构</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;i&lt;n;i++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;i &amp;&amp; <span class=\"built_in\">check</span>(i,j)) j++;</span><br><span class=\"line\">    <span class=\"comment\">//每道题具体的逻辑</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>核心思想</strong></p>\n<p>复杂度由$O(n^2)$优化到$O(n)$</p>\n<p>先想出朴素做法，寻找i与j之间的关系，是否有单调性，进行双指针优化</p>\n<p><a href=\"https://www.luogu.com.cn/problem/P1102\">A-B数对</a></p>\n<h2 id=\"位运算\"><a href=\"#位运算\" class=\"headerlink\" title=\"位运算\"></a>位运算</h2><p>计算机以二进制表示数据，以表示电路中的正反。在二进制下，一个位只有0和1。逢二进一位。</p>\n<p>计算机中存储数据，以字节为单位，一个字节有8个位，即可以表示-128~127范围的数字。</p>\n<h3 id=\"基础运算\"><a href=\"#基础运算\" class=\"headerlink\" title=\"基础运算\"></a>基础运算</h3><blockquote>\n<p>与</p>\n</blockquote>\n<p>用符号&amp;表示，运算规律是：真真为真，真假为假，假假为假（一假即假）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1&amp;1 //1</span><br><span class=\"line\">1&amp;0 //0</span><br><span class=\"line\">0&amp;0 //0</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>或</p>\n</blockquote>\n<p>用符号|表示，运算规律是：真真为真，真假为真，假假为假（一真即真）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1|1 //1</span><br><span class=\"line\">1|0 //1</span><br><span class=\"line\">0|0 //0</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>非</p>\n</blockquote>\n<p>运算符为~，取反的逻辑，运算规律：二进制位若为1，取反后为0。若为0，取反后为1</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~1 //11111110</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>左移</p>\n</blockquote>\n<p>将二进制数向左移位操作，高位溢出则丢弃，低位补0</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=11;</span><br><span class=\"line\">a&lt;&lt;<span class=\"string\">1;</span></span><br><span class=\"line\"><span class=\"string\">移位前：0000 1011</span></span><br><span class=\"line\"><span class=\"string\">移位后：0001 0110（十进制值为22）</span></span><br></pre></td></tr></table></figure>\n\n<p>对一个数左移1位就是乘以2，左移n位就是乘以2的n次方（而左移运算比乘法快得多）</p>\n<blockquote>\n<p>右移</p>\n</blockquote>\n<p>右移位运算中，无符号数和有符号数的运算并不相同。对于无符号数，右移之后高位补0；对于有符号数，符号位一起移动，正数高位补0，负数高位补1</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">无符号数</span><br><span class=\"line\">a=16;</span><br><span class=\"line\">a&gt;&gt;3;</span><br><span class=\"line\">移位前：0001 0000</span><br><span class=\"line\">移位后：0000 0010（十进制值为2）</span><br><span class=\"line\">有符号数（正数）</span><br><span class=\"line\">b=32;</span><br><span class=\"line\">a&gt;&gt;3;</span><br><span class=\"line\">移位前：0010 0000</span><br><span class=\"line\">移位后：0000 0100（十进制值位4）</span><br><span class=\"line\">有符号数（负数）</span><br><span class=\"line\">b=-32;</span><br><span class=\"line\">b&gt;&gt;3;</span><br><span class=\"line\">移位前：1010 0000</span><br><span class=\"line\">移位后：1000 0100（十进制值为-4）</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>n的二进制表示中第k位数字是几</li>\n</ul>\n<p>（k从个位开始算0,1,2…）</p>\n<ol>\n<li><p>先把第k位移到最后一位<code>n&gt;&gt;k</code></p>\n</li>\n<li><p>看个位是几 <code>x&amp;1</code></p>\n<p><strong>n&gt;&gt;k&amp;1</strong></p>\n</li>\n</ol>\n<ul>\n<li><p>lowbit(x)</p>\n<p>树状数组基本操作，返回x的最后一位1</p>\n<p><strong>x&amp;(-x)</strong></p>\n<p>原理：<code>-x=(~x+1)</code></p>\n</li>\n</ul>\n<h2 id=\"区间合并\"><a href=\"#区间合并\" class=\"headerlink\" title=\"区间合并\"></a>区间合并</h2><ul>\n<li><p>按照区间左端点排序</p>\n</li>\n<li><p>判断下一个区间与当前区间的关系</p>\n<ul>\n<li><p>相交</p>\n<ul>\n<li>更新右端点为两个区间的$max$</li>\n</ul>\n</li>\n<li><p>不相交</p>\n<ul>\n<li>将当前区间更新为不相交的这个区间</li>\n</ul>\n<p><strong>C++模版</strong></p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 将所有存在交集的区间合并</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">merge</span><span class=\"params\">(vector&lt;PII&gt; &amp;segs)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;PII&gt; res;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(segs.<span class=\"built_in\">begin</span>(), segs.<span class=\"built_in\">end</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> st = <span class=\"number\">-2e9</span>, ed = <span class=\"number\">-2e9</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> seg : segs)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ed &lt; seg.first)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (st != <span class=\"number\">-2e9</span>) res.<span class=\"built_in\">push_back</span>(&#123;st, ed&#125;);</span><br><span class=\"line\">            st = seg.first, ed = seg.second;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> ed = <span class=\"built_in\">max</span>(ed, seg.second);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (st != <span class=\"number\">-2e9</span>) res.<span class=\"built_in\">push_back</span>(&#123;st, ed&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    segs = res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"<h1 id=\"排序\"><a href=\"#排序\" class=\"headerlink\" title=\"排序\"></a>排序</h1><h2 id=\"快速排序\"><a href=\"#快速排序\" class=\"headerlink\" title=\"快速排序\"></a>快速排序</h2><p>分治思想,时间复杂度$O(nlogn)-O(n^2) $</p>\n<p>期望时间复杂度$O(nlogn)$</p>\n<ol>\n<li>数组中找一个值$x$作为分界点（可以是$arr\\left [ l \\right ]$ ,$arr\\left [ r \\right ]$,$arr\\left [ \\frac{l+r}{2} \\right ]$ 等等…）</li>\n<li>调整区间，使得左边的区间所有数$\\le$x，右边区间所有数$&gt;$x<ul>\n<li>定义两个指针分别在左右边界</li>\n<li>$i$不断右移，直到遇到$arr[i]$ $&gt;x$，就停下</li>\n<li>$j$不断左移，直到遇到$arr[j]\\le x$，就停下</li>\n<li>交换$arr[i]$与$arr[j]$</li>\n</ul>\n</li>\n<li>递归处理左右区间</li>\n</ol>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">quick_sort</span><span class=\"params\">(<span class=\"type\">int</span> q[], <span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (l &gt;= r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> l - <span class=\"number\">1</span>, j = r + <span class=\"number\">1</span>, x = q[l + r &gt;&gt; <span class=\"number\">1</span>];</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt; j)&#123;</span><br><span class=\"line\">        <span class=\"keyword\">do</span> i ++ ; <span class=\"keyword\">while</span> (q[i] &lt; x);</span><br><span class=\"line\">        <span class=\"keyword\">do</span> j -- ; <span class=\"keyword\">while</span> (q[j] &gt; x);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; j) &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">t</span> <span class=\"operator\">=</span> q[i];</span><br><span class=\"line\">            q[i] = q[j];</span><br><span class=\"line\">            q[j] = t;           </span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    quick_sort(q, l, j);</span><br><span class=\"line\">    quick_sort(q, j + <span class=\"number\">1</span>, r);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"归并排序\"><a href=\"#归并排序\" class=\"headerlink\" title=\"归并排序\"></a>归并排序</h2><p>分治思想,$O(nlogn)$</p>\n<ol>\n<li>确定分界点 $mid &#x3D; \\frac{l+r}{2}$</li>\n<li>递归排序$left$和$right$</li>\n<li>归并：合二为一<ul>\n<li>双指针指向$left$和$right$的第一个元素</li>\n<li>创建一个空数组$res$存放结果</li>\n<li>指针比较，如果$left[i]&lt;right[j]$，则把$left[i]$放入$res$，$i$向后移动一位，继续比较</li>\n<li>如果$left[i]&#x3D;right[j]$，则把$left[i]$放入$res$，以维持稳定</li>\n</ul>\n</li>\n</ol>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">merge_sort</span><span class=\"params\">(<span class=\"type\">int</span> q[], <span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (l &gt;= r) <span class=\"keyword\">return</span>;</span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    merge_sort(q, l, mid);</span><br><span class=\"line\">    merge_sort(q, mid + <span class=\"number\">1</span>, r);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> <span class=\"variable\">k</span> <span class=\"operator\">=</span> <span class=\"number\">0</span>, i = l, j = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt;= mid &amp;&amp; j &lt;= r)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (q[i] &lt; q[j]) tmp[k ++ ] = q[i ++ ];</span><br><span class=\"line\">    <span class=\"keyword\">else</span> tmp[k ++ ] = q[j ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (i &lt;= mid) tmp[k ++ ] = q[i ++ ];</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &lt;= r) tmp[k ++ ] = q[j ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (i = l, j = <span class=\"number\">0</span>; i &lt;= r; i ++, j ++ ) q[i] = tmp[j];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"二分\"><a href=\"#二分\" class=\"headerlink\" title=\"二分\"></a>二分</h1><h2 id=\"整数二分\"><a href=\"#整数二分\" class=\"headerlink\" title=\"整数二分\"></a>整数二分</h2><p><strong>提示信息</strong></p>\n<ul>\n<li>题目保证有解</li>\n<li>单调性</li>\n<li>求最大值的最小化</li>\n</ul>\n<p><strong>思路</strong></p>\n<p>对于区间$[l,r]$，其中一部分满足条件$check(x)&#x3D;true$，另一部分不满足</p>\n<ul>\n<li>对于寻找不满足区间的边界</li>\n</ul>\n<p>$mid &#x3D; \\frac{l+r+1}{2}$</p>\n<p>若$check(mid)&#x3D;true$ 则说明边界值在$[mid,r]$<br>更新语句为$l &#x3D; mid$</p>\n<p>若$check(mid)&#x3D;false$ 则说明边界值在$[l,mid-1]$<br>更新语句为$r &#x3D; mid-1$</p>\n<ul>\n<li>对于寻找满足区间的边界</li>\n</ul>\n<p>$mid &#x3D; \\frac{l+r}{2}$</p>\n<p>若$check(mid)&#x3D;true$ 则说明边界值在$[l,mid]$<br>更新语句为$r &#x3D; mid$</p>\n<p>若$check(mid)&#x3D;false$ 则说明边界值在$[mid+1,r]$<br>更新语句为$l&#x3D;mid+1$</p>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">boolean</span> <span class=\"title function_\">check</span><span class=\"params\">(<span class=\"type\">int</span> x)</span> &#123;<span class=\"comment\">/* ... */</span>&#125; <span class=\"comment\">// 检查x是否满足某种性质</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 区间[l, r]被划分成[l, mid]和[mid + 1, r]时使用：</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">bsearch_1</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (l &lt; r)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) r = mid;    <span class=\"comment\">// check()判断mid是否满足性质</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid + <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 区间[l, r]被划分成[l, mid - 1]和[mid, r]时使用：</span></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">int</span> <span class=\"title function_\">bsearch_2</span><span class=\"params\">(<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (l &lt; r)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> l + r + <span class=\"number\">1</span> &gt;&gt; <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) l = mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> r = mid - <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>！！如果$l&#x3D;mid$ ，最开始的$mid$就要补上$+1$</p>\n<p>！！<code>check</code>函数中记得取等于号</p>\n<h2 id=\"浮点数二分\"><a href=\"#浮点数二分\" class=\"headerlink\" title=\"浮点数二分\"></a>浮点数二分</h2><figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"type\">boolean</span> <span class=\"title function_\">check</span><span class=\"params\">(<span class=\"type\">double</span> x)</span> &#123;<span class=\"comment\">/* ... */</span>&#125; <span class=\"comment\">// 检查x是否满足某种性质</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">double</span> <span class=\"title function_\">bsearch_3</span><span class=\"params\">(<span class=\"type\">double</span> l, <span class=\"type\">double</span> r)</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">final</span> <span class=\"type\">double</span> <span class=\"variable\">eps</span> <span class=\"operator\">=</span> <span class=\"number\">1e-6</span>;   </span><br><span class=\"line\">    <span class=\"comment\">// eps 表示精度，取决于题目对精度的要求</span></span><br><span class=\"line\">    <span class=\"comment\">//比需要保留的位数多2</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (r - l &gt; eps)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">double</span> <span class=\"variable\">mid</span> <span class=\"operator\">=</span> (l + r) / <span class=\"number\">2</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (check(mid)) r = mid;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> l = mid;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> l;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>精度比需要保留的位数多-2次方</strong></p>\n<p>可以把$while$循环直接换成<code>for</code>100次</p>\n<h1 id=\"高精度\"><a href=\"#高精度\" class=\"headerlink\" title=\"高精度\"></a>高精度</h1><p>$C++$模版</p>\n<p>高精度加法</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A + B, A &gt;= 0, B &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">add</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, vector&lt;<span class=\"type\">int</span>&gt; &amp;B)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (A.<span class=\"built_in\">size</span>() &lt; B.<span class=\"built_in\">size</span>()) <span class=\"keyword\">return</span> <span class=\"built_in\">add</span>(B, A);</span><br><span class=\"line\"></span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t += A[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; B.<span class=\"built_in\">size</span>()) t += B[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (t) C.<span class=\"built_in\">push_back</span>(t);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度减法\"><a href=\"#高精度减法\" class=\"headerlink\" title=\"高精度减法\"></a>高精度减法</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A - B, 满足A &gt;= B, A &gt;= 0, B &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">sub</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, vector&lt;<span class=\"type\">int</span>&gt; &amp;B)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>, t = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t = A[i] - t;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; B.<span class=\"built_in\">size</span>()) t -= B[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>((t + <span class=\"number\">10</span>) % <span class=\"number\">10</span>);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (t &lt; <span class=\"number\">0</span>) t = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度乘低精度\"><a href=\"#高精度乘低精度\" class=\"headerlink\" title=\"高精度乘低精度\"></a>高精度乘低精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// C = A * b, A &gt;= 0, b &gt;= 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">mul</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; A.<span class=\"built_in\">size</span>() || t; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &lt; A.<span class=\"built_in\">size</span>()) t += A[i] * b;</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度除以低精度\"><a href=\"#高精度除以低精度\" class=\"headerlink\" title=\"高精度除以低精度\"></a>高精度除以低精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// A / b = C ... r, A &gt;= 0, b &gt; 0</span></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">div</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; &amp;A, <span class=\"type\">int</span> b, <span class=\"type\">int</span> &amp;r)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; C;</span><br><span class=\"line\">    r = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = A.<span class=\"built_in\">size</span>() - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; i -- )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        r = r * <span class=\"number\">10</span> + A[i];</span><br><span class=\"line\">        C.<span class=\"built_in\">push_back</span>(r / b);</span><br><span class=\"line\">        r %= b;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"built_in\">reverse</span>(C.<span class=\"built_in\">begin</span>(), C.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (C.<span class=\"built_in\">size</span>() &gt; <span class=\"number\">1</span> &amp;&amp; C.<span class=\"built_in\">back</span>() == <span class=\"number\">0</span>) C.<span class=\"built_in\">pop_back</span>();</span><br><span class=\"line\">    <span class=\"keyword\">return</span> C;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"高精度乘以高精度\"><a href=\"#高精度乘以高精度\" class=\"headerlink\" title=\"高精度乘以高精度\"></a>高精度乘以高精度</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cin&gt;&gt;a1&gt;&gt;b1;</span><br><span class=\"line\"><span class=\"type\">int</span> lena=<span class=\"built_in\">strlen</span>(a1);</span><br><span class=\"line\"><span class=\"type\">int</span> lenb=<span class=\"built_in\">strlen</span>(b1);</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lena;i++)a[i]=a1[lena-i]-<span class=\"string\">&#x27;0&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lenb;i++)b[i]=b1[lenb-i]-<span class=\"string\">&#x27;0&#x27;</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;=lenb;i++)</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(j=<span class=\"number\">1</span>;j&lt;=lena;j++)</span><br><span class=\"line\">        c[i+j<span class=\"number\">-1</span>]+=a[j]*b[i];</span><br><span class=\"line\"><span class=\"keyword\">for</span>(i=<span class=\"number\">1</span>;i&lt;lena+lenb;i++)</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(c[i]&gt;<span class=\"number\">9</span>)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c[i<span class=\"number\">+1</span>]+=c[i]/<span class=\"number\">10</span>;</span><br><span class=\"line\">        c[i]%=<span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">len=lena+lenb;</span><br><span class=\"line\"><span class=\"keyword\">while</span>(c[len]==<span class=\"number\">0</span>&amp;&amp;len&gt;<span class=\"number\">1</span>)len--;</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"前缀和与差分\"><a href=\"#前缀和与差分\" class=\"headerlink\" title=\"前缀和与差分\"></a>前缀和与差分</h1><p>一对逆运算</p>\n<h2 id=\"一维前缀和\"><a href=\"#一维前缀和\" class=\"headerlink\" title=\"一维前缀和\"></a>一维前缀和</h2><p>设有一列数据${a}_1,{a}<em>2,…,{a}</em>{n-1},{a}_n$</p>\n<p>定义${S}_i&#x3D;a_1+a_2+…+a_i$</p>\n<p>一般下标从1开始，$S_0&#x3D;0$</p>\n<p>$S_i$的初始化: $S_i &#x3D; S_{i-1}+a_i$</p>\n<p><strong>作用</strong></p>\n<p>快速地求出原数组中一段区间数的和</p>\n<p>对于区间$[l,r]$</p>\n<p>$\\sum_{i&#x3D;l}^{r}a_i &#x3D; S_r-S_{l-1}$</p>\n<h2 id=\"二维前缀和\"><a href=\"#二维前缀和\" class=\"headerlink\" title=\"二维前缀和\"></a>二维前缀和</h2><p>对于二维数组（矩阵）$\\begin{pmatrix} a_{11}&amp; a_{12} &amp; … &amp; a_{1j}\\ a_{21}&amp; a_{22} &amp; … &amp; a_{2j} \\ …&amp; … &amp; … &amp; …\\ a_{i1}&amp; a_{i2} &amp; … &amp; a_{ij} \\end{pmatrix}$</p>\n<p>$S_{ij}$代表$a_{ij}$左上角的所有元素和</p>\n<ul>\n<li><p>对于点$(i,j)$，其二维前缀和$S_{ij}$的初始化</p>\n<p>$S_{ij}&#x3D;S_{i-1,j}+S_{i,j-1}-S_{i-1,j-1}+a_{i,j}$</p>\n</li>\n<li><p>设点$(x_1,y_1)$在$(x_2,y_2)$的左上角，则两点围成的矩形中所有元素和<br>$S&#x3D;S_{x_2,y_2}-S_{x_2,y_1-1}-S_{x_1-1,y_2}+S_{x_1-1,y_1-1}$</p>\n</li>\n</ul>\n<h2 id=\"一维差分\"><a href=\"#一维差分\" class=\"headerlink\" title=\"一维差分\"></a>一维差分</h2><p>对一列数据$a_1,a_2,a_3,…,a_i$</p>\n<p>构造$b_1,b_2,b_3,…,b_i$使得$a_i&#x3D;b_1+b_2+…+b_i$</p>\n<p>即$a$为$b$的前缀和，$b$就是$a$的差分</p>\n<p>$\\left{\\begin{matrix} b_1&#x3D;a_1\\ b_2&#x3D;a_2-a_1\\ b_3&#x3D;a_3-a_2\\ ……\\ b_n&#x3D;a_n-a_{n-1} \\end{matrix}\\right.$</p>\n<p><strong>作用</strong></p>\n<p>若要把$a_1,a_2,a_3,…,a_i$中$[l,r]$区间的$a$加$c$</p>\n<p>只需要使$b_l+&#x3D;c,b_{r+1}-&#x3D;c$</p>\n<p><strong>模版</strong></p>\n<figure class=\"highlight java\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> java.util.Scanner;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">public</span> <span class=\"keyword\">class</span> <span class=\"title class_\">Diff</span> &#123;</span><br><span class=\"line\">    <span class=\"keyword\">public</span> <span class=\"keyword\">static</span> <span class=\"keyword\">void</span> <span class=\"title function_\">main</span><span class=\"params\">(String[] args)</span> &#123;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">Scanner</span> <span class=\"variable\">scanner</span> <span class=\"operator\">=</span> <span class=\"keyword\">new</span> <span class=\"title class_\">Scanner</span>(System.in);</span><br><span class=\"line\">        <span class=\"comment\">// 给出n数组大小和k增加次数</span></span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">n</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">        <span class=\"type\">int</span> <span class=\"variable\">k</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 搭建数组</span></span><br><span class=\"line\">        <span class=\"type\">int</span>[] arr = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[n+<span class=\"number\">1</span>];</span><br><span class=\"line\">        <span class=\"type\">int</span>[] brr = <span class=\"keyword\">new</span> <span class=\"title class_\">int</span>[n+<span class=\"number\">1</span>];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 为arr赋值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            arr[i] = scanner.nextInt();</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 为brr赋值</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++)&#123;</span><br><span class=\"line\">            brr[i] = arr[i] - arr[i-<span class=\"number\">1</span>];</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">while</span> (k-- &gt; <span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">            <span class=\"comment\">// 我们为arr的[l,r]区间加上c</span></span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">l</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">r</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\">            <span class=\"type\">int</span> <span class=\"variable\">c</span> <span class=\"operator\">=</span> scanner.nextInt();</span><br><span class=\"line\"></span><br><span class=\"line\">            brr[l] += c;</span><br><span class=\"line\">            brr[r+<span class=\"number\">1</span>] -= c;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 计算输出结果即可（这里输出的需要是由b累计出来的a）</span></span><br><span class=\"line\">        <span class=\"comment\">// 也可以使用注释代码，最后输出arr即可</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            brr[i] += brr[i-<span class=\"number\">1</span>];</span><br><span class=\"line\">            <span class=\"comment\">//arr[i] = brr[i]+arr[i-1];</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 最后输出结果</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> <span class=\"variable\">i</span> <span class=\"operator\">=</span> <span class=\"number\">1</span>; i &lt; n+<span class=\"number\">1</span>; i++) &#123;</span><br><span class=\"line\">            System.out.println(brr[i]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"二维差分\"><a href=\"#二维差分\" class=\"headerlink\" title=\"二维差分\"></a>二维差分</h2><p>原矩阵$a_{ij}$,差分矩阵$b_{ij}$</p>\n<p>$b_{x1,y1}+&#x3D;c$</p>\n<p>$b_{x2+1,y1}-&#x3D;c$</p>\n<p>$b_{x1,y2+1}-&#x3D;c$</p>\n<p>$b_{x2+1,y2+1}+&#x3D;c$</p>\n<h1 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h1><h2 id=\"双指针算法\"><a href=\"#双指针算法\" class=\"headerlink\" title=\"双指针算法\"></a>双指针算法</h2><ul>\n<li>两个序列，两个指针</li>\n<li><strong>一个序列，两个指针</strong></li>\n</ul>\n<p><strong>结构</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">0</span>,j=<span class=\"number\">0</span>;i&lt;n;i++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(j&lt;i &amp;&amp; <span class=\"built_in\">check</span>(i,j)) j++;</span><br><span class=\"line\">    <span class=\"comment\">//每道题具体的逻辑</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>核心思想</strong></p>\n<p>复杂度由$O(n^2)$优化到$O(n)$</p>\n<p>先想出朴素做法，寻找i与j之间的关系，是否有单调性，进行双指针优化</p>\n<p><a href=\"https://www.luogu.com.cn/problem/P1102\">A-B数对</a></p>\n<h2 id=\"位运算\"><a href=\"#位运算\" class=\"headerlink\" title=\"位运算\"></a>位运算</h2><p>计算机以二进制表示数据，以表示电路中的正反。在二进制下，一个位只有0和1。逢二进一位。</p>\n<p>计算机中存储数据，以字节为单位，一个字节有8个位，即可以表示-128~127范围的数字。</p>\n<h3 id=\"基础运算\"><a href=\"#基础运算\" class=\"headerlink\" title=\"基础运算\"></a>基础运算</h3><blockquote>\n<p>与</p>\n</blockquote>\n<p>用符号&amp;表示，运算规律是：真真为真，真假为假，假假为假（一假即假）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1&amp;1 //1</span><br><span class=\"line\">1&amp;0 //0</span><br><span class=\"line\">0&amp;0 //0</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>或</p>\n</blockquote>\n<p>用符号|表示，运算规律是：真真为真，真假为真，假假为假（一真即真）</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">1|1 //1</span><br><span class=\"line\">1|0 //1</span><br><span class=\"line\">0|0 //0</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>非</p>\n</blockquote>\n<p>运算符为~，取反的逻辑，运算规律：二进制位若为1，取反后为0。若为0，取反后为1</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">~1 //11111110</span><br></pre></td></tr></table></figure>\n\n<blockquote>\n<p>左移</p>\n</blockquote>\n<p>将二进制数向左移位操作，高位溢出则丢弃，低位补0</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a=11;</span><br><span class=\"line\">a&lt;&lt;<span class=\"string\">1;</span></span><br><span class=\"line\"><span class=\"string\">移位前：0000 1011</span></span><br><span class=\"line\"><span class=\"string\">移位后：0001 0110（十进制值为22）</span></span><br></pre></td></tr></table></figure>\n\n<p>对一个数左移1位就是乘以2，左移n位就是乘以2的n次方（而左移运算比乘法快得多）</p>\n<blockquote>\n<p>右移</p>\n</blockquote>\n<p>右移位运算中，无符号数和有符号数的运算并不相同。对于无符号数，右移之后高位补0；对于有符号数，符号位一起移动，正数高位补0，负数高位补1</p>\n<figure class=\"highlight text\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">无符号数</span><br><span class=\"line\">a=16;</span><br><span class=\"line\">a&gt;&gt;3;</span><br><span class=\"line\">移位前：0001 0000</span><br><span class=\"line\">移位后：0000 0010（十进制值为2）</span><br><span class=\"line\">有符号数（正数）</span><br><span class=\"line\">b=32;</span><br><span class=\"line\">a&gt;&gt;3;</span><br><span class=\"line\">移位前：0010 0000</span><br><span class=\"line\">移位后：0000 0100（十进制值位4）</span><br><span class=\"line\">有符号数（负数）</span><br><span class=\"line\">b=-32;</span><br><span class=\"line\">b&gt;&gt;3;</span><br><span class=\"line\">移位前：1010 0000</span><br><span class=\"line\">移位后：1000 0100（十进制值为-4）</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>n的二进制表示中第k位数字是几</li>\n</ul>\n<p>（k从个位开始算0,1,2…）</p>\n<ol>\n<li><p>先把第k位移到最后一位<code>n&gt;&gt;k</code></p>\n</li>\n<li><p>看个位是几 <code>x&amp;1</code></p>\n<p><strong>n&gt;&gt;k&amp;1</strong></p>\n</li>\n</ol>\n<ul>\n<li><p>lowbit(x)</p>\n<p>树状数组基本操作，返回x的最后一位1</p>\n<p><strong>x&amp;(-x)</strong></p>\n<p>原理：<code>-x=(~x+1)</code></p>\n</li>\n</ul>\n<h2 id=\"区间合并\"><a href=\"#区间合并\" class=\"headerlink\" title=\"区间合并\"></a>区间合并</h2><ul>\n<li><p>按照区间左端点排序</p>\n</li>\n<li><p>判断下一个区间与当前区间的关系</p>\n<ul>\n<li><p>相交</p>\n<ul>\n<li>更新右端点为两个区间的$max$</li>\n</ul>\n</li>\n<li><p>不相交</p>\n<ul>\n<li>将当前区间更新为不相交的这个区间</li>\n</ul>\n<p><strong>C++模版</strong></p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 将所有存在交集的区间合并</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">merge</span><span class=\"params\">(vector&lt;PII&gt; &amp;segs)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;PII&gt; res;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(segs.<span class=\"built_in\">begin</span>(), segs.<span class=\"built_in\">end</span>());</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> st = <span class=\"number\">-2e9</span>, ed = <span class=\"number\">-2e9</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"keyword\">auto</span> seg : segs)</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (ed &lt; seg.first)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (st != <span class=\"number\">-2e9</span>) res.<span class=\"built_in\">push_back</span>(&#123;st, ed&#125;);</span><br><span class=\"line\">            st = seg.first, ed = seg.second;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> ed = <span class=\"built_in\">max</span>(ed, seg.second);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (st != <span class=\"number\">-2e9</span>) res.<span class=\"built_in\">push_back</span>(&#123;st, ed&#125;);</span><br><span class=\"line\"></span><br><span class=\"line\">    segs = res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Algorithm-Data-Structure","mathjax":true,"date":"2023-07-07T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛基础数据结构","_content":"\n## 链表与邻接表\n\n由于用`结构体+指针`比较慢，一般在面试题使用，在这里使用**数组**模拟链表\n\n- 单链表\n\n`e[N]`：储存链表结点的值\n\n`ne[N]`：储存结点的下一个结点下标，其中空结点下标为-1\n\n```cpp\n// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点\nint head, e[N], ne[N], idx;\n\n// 初始化\nvoid init()\n{\n    head = -1;\n    idx = 0;\n}\n\n// 在链表头插入一个数a\nvoid insert(int a)\n{\n    e[idx] = a, ne[idx] = head, head = idx ++ ;\n}\n\n//在a插到下标是k的结点后面\nvoid insert(int a,int k)\n{\n    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;\n}\n\n// 将头结点删除，需要保证头结点存在\nvoid remove()\n{\n    head = ne[head];\n}\n\n// 将下标为k的结点的后一个点删除\nvoid remove(k)\n{\n    ne[k] = ne[ne[k]];\n}\n```\n\n- 双链表\n\n作用：优化某些问题\n\n```cpp\n// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点\nint e[N], l[N], r[N], idx;\n\n// 初始化\nvoid init()\n{\n    //0是左端点，1是右端点\n    r[0] = 1, l[1] = 0;\n    idx = 2;\n}\n\n// 在节点a的右边插入一个数x\nvoid insert(int a, int x)\n{\n    e[idx] = x;\n    l[idx] = a, r[idx] = r[a];\n    l[r[a]] = idx, r[a] = idx ++ ;\n}\n\n// 删除节点a\nvoid remove(int a)\n{\n    l[r[a]] = l[a];\n    r[l[a]] = r[a];\n}\n```\n\n- 邻接表\n\nN个单链表，用于存储树和图\n\n## 栈\n\n先进后出(FILO)\n\n```cpp\n// tt表示栈顶\nint stk[N], tt = 0;\n\n// 向栈顶插入一个数\nstk[ ++ tt] = x;\n\n// 从栈顶弹出一个数\ntt -- ;\n\n// 栈顶的值\nstk[tt];\n\n// 判断栈是否为空，如果 tt > 0，则表示不为空\nif (tt > 0)\n{\n\tnot empty\n}else\n{\n    empty\n}\n```\n\n## 队列\n\n先进先出(FIFO)\n\n```cpp\n// hh 表示队头，tt表示队尾\nint q[N], hh = 0, tt = -1;\n\n// 向队尾插入一个数\nq[ ++ tt] = x;\n\n// 从队头弹出一个数\nhh ++ ;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果 hh <= tt，则表示不为空\nif (hh <= tt)\n{\n\n}\n```\n\n循环队列\n\n```cpp\n// hh 表示队头，tt表示队尾的后一个位置\nint q[N], hh = 0, tt = 0;\n\n// 向队尾插入一个数\nq[tt ++ ] = x;\nif (tt == N) tt = 0;\n\n// 从队头弹出一个数\nhh ++ ;\nif (hh == N) hh = 0;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果hh != tt，则表示不为空\nif (hh != tt)\n{\n\n}\n```\n\n## 单调栈\n\n题型：求给定序列每一个数左/右边离他最近的比他大/小的数\n\n```cpp\nint tt = 0;\nfor (int i = 1; i <= n; i ++ )\n{\n    while (tt && check(stk[tt], i)) tt -- ;\n    stk[ ++ tt] = i;\n}\n```\n\n## 单调队列\n\n题型：求滑动窗口的最大/小值\n\n```cpp\nint hh = 0, tt = -1;\nfor (int i = 0; i < n; i ++ )\n{\n    while (hh <= tt && check_out(q[hh])) hh ++ ;  // 判断队头是否滑出窗口\n    while (hh <= tt && check(q[tt], i)) tt -- ;\n    q[ ++ tt] = i;\n}\n```\n\n## KMP\n\n对于字符串$s$,判断是否包含模式串$t$\n\n```cpp\n// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度\n//求模式串的Next数组：\nfor (int i = 2, j = 0; i <= m; i ++ )\n{\n    while (j && p[i] != p[j + 1]) j = ne[j];\n    if (p[i] == p[j + 1]) j ++ ;\n    ne[i] = j;\n}\n\n// 匹配\nfor (int i = 1, j = 0; i <= n; i ++ )\n{\n    while (j && s[i] != p[j + 1]) j = ne[j];\n    if (s[i] == p[j + 1]) j ++ ;\n    if (j == m)\n    {\n        j = ne[j];\n        // 匹配成功后的逻辑\n    }\n}\n```\n\n```cpp\ns.find(t) != s.npos\n```\n\n## Trie树\n\n快速存储和查找字符串集合的数据结构\n\n```cpp\nint son[N][26], cnt[N], idx;\n// 0号点既是根节点，又是空节点\n// son[][]存储树中每个节点的子节点\n// cnt[]存储以每个节点结尾的单词数量\n\n// 插入一个字符串\nvoid insert(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) son[p][u] = ++ idx;\n        p = son[p][u];\n    }\n    cnt[p] ++ ;\n}\n\n// 查询字符串出现的次数\nint query(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) return 0;\n        p = son[p][u];\n    }\n    return cnt[p];\n}\n```\n\n[最大异或对](https://www.acwing.com/problem/content/145/)\n\n[前缀统计](https://www.acwing.com/problem/content/144/)\n\n## 并查集\n\n1. 将两个集合合并\n2. 询问两个元素是否在一个集合当中\n\n近乎$O(1)$\n\n基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点\n\n- 如何判断是树根？\n\n$p[x] = x$\n\n- 如何求$x$的集合编号\n\n```cpp\nwhile(p[x]!=x) x = p[x]\n```\n\n- 如何合并两个区间\n\n设p[x]为x集合编号，p[y]是y集合编号。p[x]=y\n\n优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根\n\n```cpp\n(1)朴素并查集：\n\n    int p[N]; //存储每个点的祖宗节点\n\n    // 返回x的祖宗节点+路径压缩\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n\n\n(2)维护size的并查集：\n\n    int p[N], size[N];\n    //p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        size[i] = 1;\n    }\n\n    // 合并a和b所在的两个集合：\n    size[find(b)] += size[find(a)];\n    p[find(a)] = find(b);\n\n\n(3)维护到祖宗节点距离的并查集：\n\n    int p[N], d[N];\n    //p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x)\n        {\n            int u = find(p[x]);\n            d[x] += d[p[x]];\n            p[x] = u;\n        }\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        d[i] = 0;\n    }\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n    d[find(a)] = distance; // 根据具体问题，初始化find(a)的偏移量\n```\n\n## C++ STL\n\n```cpp\nvector, 变长数组，倍增的思想\n    size()  返回元素个数\n    empty()  返回是否为空\n    clear()  清空\n    front()/back()\n    push_back()/pop_back()\n    begin()/end()\n    []\n    支持比较运算，按字典序\n\npair<int, int>\n    first, 第一个元素\n    second, 第二个元素\n    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）\n\nstring，字符串\n    size()/length()  返回字符串长度\n    empty()\n    clear()\n    substr(起始下标，(子串长度))  返回子串\n    c_str()  返回字符串所在字符数组的起始地址\n\nqueue, 队列\n    size()\n    empty()\n    push()  向队尾插入一个元素\n    front()  返回队头元素\n    back()  返回队尾元素\n    pop()  弹出队头元素\n\npriority_queue, 优先队列，默认是大根堆\n    size()\n    empty()\n    push()  插入一个元素\n    top()  返回堆顶元素\n    pop()  弹出堆顶元素\n    定义成小根堆的方式：priority_queue<int, vector<int>, greater<int>> q;\n\nstack, 栈\n    size()\n    empty()\n    push()  向栈顶插入一个元素\n    top()  返回栈顶元素\n    pop()  弹出栈顶元素\n\ndeque, 双端队列\n    size()\n    empty()\n    clear()\n    front()/back()\n    push_back()/pop_back()\n    push_front()/pop_front()\n    begin()/end()\n    []\n\nset, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列\n    size()\n    empty()\n    clear()\n    begin()/end()\n    ++, -- 返回前驱和后继，时间复杂度 O(logn)\n\n    set/multiset\n        insert()  插入一个数\n        find()  查找一个数\n        count()  返回某一个数的个数\n        erase()\n            (1) 输入是一个数x，删除所有x   O(k + logn)\n            (2) 输入一个迭代器，删除这个迭代器\n        lower_bound()/upper_bound()\n            lower_bound(x)  返回大于等于x的最小的数的迭代器\n            upper_bound(x)  返回大于x的最小的数的迭代器\n    map/multimap\n        insert()  插入的数是一个pair\n        erase()  输入的参数是pair或者迭代器\n        find()\n        []  注意multimap不支持此操作。 时间复杂度是 O(logn)\n        lower_bound()/upper_bound()\n\nunordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表\n    和上面类似，增删改查的时间复杂度是 O(1)\n    不支持 lower_bound()/upper_bound()， 迭代器的++，--\n\nbitset, 圧位\n    bitset<10000> s;\n    ~, &, |, ^\n    >>, <<\n    ==, !=\n    []\n\n    count()  返回有多少个1\n\n    any()  判断是否至少有一个1\n    none()  判断是否全为0\n\n    set()  把所有位置成1\n    set(k, v)  将第k位变成v\n    reset()  把所有位变成0\n    flip()  等价于~\n    flip(k) 把第k位取反\n```","source":"_posts/algorithm-data-structure.md","raw":"---\ntitle: Algorithm-Data-Structure\nmathjax: true\ndate: 2023/07/07 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛基础数据结构\n---\n\n## 链表与邻接表\n\n由于用`结构体+指针`比较慢，一般在面试题使用，在这里使用**数组**模拟链表\n\n- 单链表\n\n`e[N]`：储存链表结点的值\n\n`ne[N]`：储存结点的下一个结点下标，其中空结点下标为-1\n\n```cpp\n// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点\nint head, e[N], ne[N], idx;\n\n// 初始化\nvoid init()\n{\n    head = -1;\n    idx = 0;\n}\n\n// 在链表头插入一个数a\nvoid insert(int a)\n{\n    e[idx] = a, ne[idx] = head, head = idx ++ ;\n}\n\n//在a插到下标是k的结点后面\nvoid insert(int a,int k)\n{\n    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;\n}\n\n// 将头结点删除，需要保证头结点存在\nvoid remove()\n{\n    head = ne[head];\n}\n\n// 将下标为k的结点的后一个点删除\nvoid remove(k)\n{\n    ne[k] = ne[ne[k]];\n}\n```\n\n- 双链表\n\n作用：优化某些问题\n\n```cpp\n// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点\nint e[N], l[N], r[N], idx;\n\n// 初始化\nvoid init()\n{\n    //0是左端点，1是右端点\n    r[0] = 1, l[1] = 0;\n    idx = 2;\n}\n\n// 在节点a的右边插入一个数x\nvoid insert(int a, int x)\n{\n    e[idx] = x;\n    l[idx] = a, r[idx] = r[a];\n    l[r[a]] = idx, r[a] = idx ++ ;\n}\n\n// 删除节点a\nvoid remove(int a)\n{\n    l[r[a]] = l[a];\n    r[l[a]] = r[a];\n}\n```\n\n- 邻接表\n\nN个单链表，用于存储树和图\n\n## 栈\n\n先进后出(FILO)\n\n```cpp\n// tt表示栈顶\nint stk[N], tt = 0;\n\n// 向栈顶插入一个数\nstk[ ++ tt] = x;\n\n// 从栈顶弹出一个数\ntt -- ;\n\n// 栈顶的值\nstk[tt];\n\n// 判断栈是否为空，如果 tt > 0，则表示不为空\nif (tt > 0)\n{\n\tnot empty\n}else\n{\n    empty\n}\n```\n\n## 队列\n\n先进先出(FIFO)\n\n```cpp\n// hh 表示队头，tt表示队尾\nint q[N], hh = 0, tt = -1;\n\n// 向队尾插入一个数\nq[ ++ tt] = x;\n\n// 从队头弹出一个数\nhh ++ ;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果 hh <= tt，则表示不为空\nif (hh <= tt)\n{\n\n}\n```\n\n循环队列\n\n```cpp\n// hh 表示队头，tt表示队尾的后一个位置\nint q[N], hh = 0, tt = 0;\n\n// 向队尾插入一个数\nq[tt ++ ] = x;\nif (tt == N) tt = 0;\n\n// 从队头弹出一个数\nhh ++ ;\nif (hh == N) hh = 0;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果hh != tt，则表示不为空\nif (hh != tt)\n{\n\n}\n```\n\n## 单调栈\n\n题型：求给定序列每一个数左/右边离他最近的比他大/小的数\n\n```cpp\nint tt = 0;\nfor (int i = 1; i <= n; i ++ )\n{\n    while (tt && check(stk[tt], i)) tt -- ;\n    stk[ ++ tt] = i;\n}\n```\n\n## 单调队列\n\n题型：求滑动窗口的最大/小值\n\n```cpp\nint hh = 0, tt = -1;\nfor (int i = 0; i < n; i ++ )\n{\n    while (hh <= tt && check_out(q[hh])) hh ++ ;  // 判断队头是否滑出窗口\n    while (hh <= tt && check(q[tt], i)) tt -- ;\n    q[ ++ tt] = i;\n}\n```\n\n## KMP\n\n对于字符串$s$,判断是否包含模式串$t$\n\n```cpp\n// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度\n//求模式串的Next数组：\nfor (int i = 2, j = 0; i <= m; i ++ )\n{\n    while (j && p[i] != p[j + 1]) j = ne[j];\n    if (p[i] == p[j + 1]) j ++ ;\n    ne[i] = j;\n}\n\n// 匹配\nfor (int i = 1, j = 0; i <= n; i ++ )\n{\n    while (j && s[i] != p[j + 1]) j = ne[j];\n    if (s[i] == p[j + 1]) j ++ ;\n    if (j == m)\n    {\n        j = ne[j];\n        // 匹配成功后的逻辑\n    }\n}\n```\n\n```cpp\ns.find(t) != s.npos\n```\n\n## Trie树\n\n快速存储和查找字符串集合的数据结构\n\n```cpp\nint son[N][26], cnt[N], idx;\n// 0号点既是根节点，又是空节点\n// son[][]存储树中每个节点的子节点\n// cnt[]存储以每个节点结尾的单词数量\n\n// 插入一个字符串\nvoid insert(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) son[p][u] = ++ idx;\n        p = son[p][u];\n    }\n    cnt[p] ++ ;\n}\n\n// 查询字符串出现的次数\nint query(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) return 0;\n        p = son[p][u];\n    }\n    return cnt[p];\n}\n```\n\n[最大异或对](https://www.acwing.com/problem/content/145/)\n\n[前缀统计](https://www.acwing.com/problem/content/144/)\n\n## 并查集\n\n1. 将两个集合合并\n2. 询问两个元素是否在一个集合当中\n\n近乎$O(1)$\n\n基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点\n\n- 如何判断是树根？\n\n$p[x] = x$\n\n- 如何求$x$的集合编号\n\n```cpp\nwhile(p[x]!=x) x = p[x]\n```\n\n- 如何合并两个区间\n\n设p[x]为x集合编号，p[y]是y集合编号。p[x]=y\n\n优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根\n\n```cpp\n(1)朴素并查集：\n\n    int p[N]; //存储每个点的祖宗节点\n\n    // 返回x的祖宗节点+路径压缩\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n\n\n(2)维护size的并查集：\n\n    int p[N], size[N];\n    //p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        size[i] = 1;\n    }\n\n    // 合并a和b所在的两个集合：\n    size[find(b)] += size[find(a)];\n    p[find(a)] = find(b);\n\n\n(3)维护到祖宗节点距离的并查集：\n\n    int p[N], d[N];\n    //p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x)\n        {\n            int u = find(p[x]);\n            d[x] += d[p[x]];\n            p[x] = u;\n        }\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        d[i] = 0;\n    }\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n    d[find(a)] = distance; // 根据具体问题，初始化find(a)的偏移量\n```\n\n## C++ STL\n\n```cpp\nvector, 变长数组，倍增的思想\n    size()  返回元素个数\n    empty()  返回是否为空\n    clear()  清空\n    front()/back()\n    push_back()/pop_back()\n    begin()/end()\n    []\n    支持比较运算，按字典序\n\npair<int, int>\n    first, 第一个元素\n    second, 第二个元素\n    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）\n\nstring，字符串\n    size()/length()  返回字符串长度\n    empty()\n    clear()\n    substr(起始下标，(子串长度))  返回子串\n    c_str()  返回字符串所在字符数组的起始地址\n\nqueue, 队列\n    size()\n    empty()\n    push()  向队尾插入一个元素\n    front()  返回队头元素\n    back()  返回队尾元素\n    pop()  弹出队头元素\n\npriority_queue, 优先队列，默认是大根堆\n    size()\n    empty()\n    push()  插入一个元素\n    top()  返回堆顶元素\n    pop()  弹出堆顶元素\n    定义成小根堆的方式：priority_queue<int, vector<int>, greater<int>> q;\n\nstack, 栈\n    size()\n    empty()\n    push()  向栈顶插入一个元素\n    top()  返回栈顶元素\n    pop()  弹出栈顶元素\n\ndeque, 双端队列\n    size()\n    empty()\n    clear()\n    front()/back()\n    push_back()/pop_back()\n    push_front()/pop_front()\n    begin()/end()\n    []\n\nset, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列\n    size()\n    empty()\n    clear()\n    begin()/end()\n    ++, -- 返回前驱和后继，时间复杂度 O(logn)\n\n    set/multiset\n        insert()  插入一个数\n        find()  查找一个数\n        count()  返回某一个数的个数\n        erase()\n            (1) 输入是一个数x，删除所有x   O(k + logn)\n            (2) 输入一个迭代器，删除这个迭代器\n        lower_bound()/upper_bound()\n            lower_bound(x)  返回大于等于x的最小的数的迭代器\n            upper_bound(x)  返回大于x的最小的数的迭代器\n    map/multimap\n        insert()  插入的数是一个pair\n        erase()  输入的参数是pair或者迭代器\n        find()\n        []  注意multimap不支持此操作。 时间复杂度是 O(logn)\n        lower_bound()/upper_bound()\n\nunordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表\n    和上面类似，增删改查的时间复杂度是 O(1)\n    不支持 lower_bound()/upper_bound()， 迭代器的++，--\n\nbitset, 圧位\n    bitset<10000> s;\n    ~, &, |, ^\n    >>, <<\n    ==, !=\n    []\n\n    count()  返回有多少个1\n\n    any()  判断是否至少有一个1\n    none()  判断是否全为0\n\n    set()  把所有位置成1\n    set(k, v)  将第k位变成v\n    reset()  把所有位变成0\n    flip()  等价于~\n    flip(k) 把第k位取反\n```","slug":"algorithm-data-structure","published":1,"updated":"2025-02-20T06:45:40.656Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9c0004bs9965mmg8xc","content":"<h2 id=\"链表与邻接表\"><a href=\"#链表与邻接表\" class=\"headerlink\" title=\"链表与邻接表\"></a>链表与邻接表</h2><p>由于用<code>结构体+指针</code>比较慢，一般在面试题使用，在这里使用<strong>数组</strong>模拟链表</p>\n<ul>\n<li>单链表</li>\n</ul>\n<p><code>e[N]</code>：储存链表结点的值</p>\n<p><code>ne[N]</code>：储存结点的下一个结点下标，其中空结点下标为-1</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> head, e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = <span class=\"number\">-1</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在链表头插入一个数a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = head, head = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//在a插到下标是k的结点后面</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将头结点删除，需要保证头结点存在</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = ne[head];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将下标为k的结点的后一个点删除</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ne[k] = ne[ne[k]];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双链表</li>\n</ul>\n<p>作用：优化某些问题</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> e[N], l[N], r[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//0是左端点，1是右端点</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">1</span>, l[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在节点a的右边插入一个数x</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = x;</span><br><span class=\"line\">    l[idx] = a, r[idx] = r[a];</span><br><span class=\"line\">    l[r[a]] = idx, r[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 删除节点a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    l[r[a]] = l[a];</span><br><span class=\"line\">    r[l[a]] = r[a];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>邻接表</li>\n</ul>\n<p>N个单链表，用于存储树和图</p>\n<h2 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h2><p>先进后出(FILO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tt表示栈顶</span></span><br><span class=\"line\"><span class=\"type\">int</span> stk[N], tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向栈顶插入一个数</span></span><br><span class=\"line\">stk[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从栈顶弹出一个数</span></span><br><span class=\"line\">tt -- ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 栈顶的值</span></span><br><span class=\"line\">stk[tt];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断栈是否为空，如果 tt &gt; 0，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">not</span> empty</span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    empty</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"队列\"><a href=\"#队列\" class=\"headerlink\" title=\"队列\"></a>队列</h2><p>先进先出(FIFO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果 hh &lt;= tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh &lt;= tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>循环队列</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾的后一个位置</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[tt ++ ] = x;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt == N) tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh == N) hh = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果hh != tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh != tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调栈\"><a href=\"#单调栈\" class=\"headerlink\" title=\"单调栈\"></a>单调栈</h2><p>题型：求给定序列每一个数左&#x2F;右边离他最近的比他大&#x2F;小的数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (tt &amp;&amp; <span class=\"built_in\">check</span>(stk[tt], i)) tt -- ;</span><br><span class=\"line\">    stk[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调队列\"><a href=\"#单调队列\" class=\"headerlink\" title=\"单调队列\"></a>单调队列</h2><p>题型：求滑动窗口的最大&#x2F;小值</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check_out</span>(q[hh])) hh ++ ;  <span class=\"comment\">// 判断队头是否滑出窗口</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check</span>(q[tt], i)) tt -- ;</span><br><span class=\"line\">    q[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><p>对于字符串$s$,判断是否包含模式串$t$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度</span></span><br><span class=\"line\"><span class=\"comment\">//求模式串的Next数组：</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>, j = <span class=\"number\">0</span>; i &lt;= m; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; p[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    ne[i] = j;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 匹配</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>, j = <span class=\"number\">0</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; s[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (j == m)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        j = ne[j];</span><br><span class=\"line\">        <span class=\"comment\">// 匹配成功后的逻辑</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">s.<span class=\"built_in\">find</span>(t) != s.npos</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Trie树\"><a href=\"#Trie树\" class=\"headerlink\" title=\"Trie树\"></a>Trie树</h2><p>快速存储和查找字符串集合的数据结构</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> son[N][<span class=\"number\">26</span>], cnt[N], idx;</span><br><span class=\"line\"><span class=\"comment\">// 0号点既是根节点，又是空节点</span></span><br><span class=\"line\"><span class=\"comment\">// son[][]存储树中每个节点的子节点</span></span><br><span class=\"line\"><span class=\"comment\">// cnt[]存储以每个节点结尾的单词数量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 插入一个字符串</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) son[p][u] = ++ idx;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cnt[p] ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 查询字符串出现的次数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">query</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cnt[p];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/145/\">最大异或对</a></p>\n<p><a href=\"https://www.acwing.com/problem/content/144/\">前缀统计</a></p>\n<h2 id=\"并查集\"><a href=\"#并查集\" class=\"headerlink\" title=\"并查集\"></a>并查集</h2><ol>\n<li>将两个集合合并</li>\n<li>询问两个元素是否在一个集合当中</li>\n</ol>\n<p>近乎$O(1)$</p>\n<p>基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点</p>\n<ul>\n<li>如何判断是树根？</li>\n</ul>\n<p>$p[x] &#x3D; x$</p>\n<ul>\n<li>如何求$x$的集合编号</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(p[x]!=x) x = p[x]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何合并两个区间</li>\n</ul>\n<p>设p[x]为x集合编号，p[y]是y集合编号。p[x]&#x3D;y</p>\n<p>优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">1</span>)朴素并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N]; <span class=\"comment\">//存储每个点的祖宗节点</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点+路径压缩</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">2</span>)维护size的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], size[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        size[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    size[<span class=\"built_in\">find</span>(b)] += size[<span class=\"built_in\">find</span>(a)];</span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">3</span>)维护到祖宗节点距离的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], d[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> u = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">            d[x] += d[p[x]];</span><br><span class=\"line\">            p[x] = u;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        d[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">    d[<span class=\"built_in\">find</span>(a)] = distance; <span class=\"comment\">// 根据具体问题，初始化find(a)的偏移量</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"C-STL\"><a href=\"#C-STL\" class=\"headerlink\" title=\"C++ STL\"></a>C++ STL</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vector, 变长数组，倍增的思想</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()  返回元素个数</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()  返回是否为空</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()  清空</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\">    支持比较运算，按字典序</span><br><span class=\"line\"></span><br><span class=\"line\">pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt;</span><br><span class=\"line\">    first, 第一个元素</span><br><span class=\"line\">    second, 第二个元素</span><br><span class=\"line\">    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）</span><br><span class=\"line\"></span><br><span class=\"line\">string，字符串</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()/<span class=\"built_in\">length</span>()  返回字符串长度</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">substr</span>(起始下标，(子串长度))  返回子串</span><br><span class=\"line\">    <span class=\"built_in\">c_str</span>()  返回字符串所在字符数组的起始地址</span><br><span class=\"line\"></span><br><span class=\"line\">queue, 队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向队尾插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()  返回队头元素</span><br><span class=\"line\">    <span class=\"built_in\">back</span>()  返回队尾元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出队头元素</span><br><span class=\"line\"></span><br><span class=\"line\">priority_queue, 优先队列，默认是大根堆</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回堆顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出堆顶元素</span><br><span class=\"line\">    定义成小根堆的方式：priority_queue&lt;<span class=\"type\">int</span>, vector&lt;<span class=\"type\">int</span>&gt;, greater&lt;<span class=\"type\">int</span>&gt;&gt; q;</span><br><span class=\"line\"></span><br><span class=\"line\">stack, 栈</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向栈顶插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回栈顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出栈顶元素</span><br><span class=\"line\"></span><br><span class=\"line\">deque, 双端队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_front</span>()/<span class=\"built_in\">pop_front</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    ++, -- 返回前驱和后继，时间复杂度 <span class=\"built_in\">O</span>(logn)</span><br><span class=\"line\"></span><br><span class=\"line\">    set/<span class=\"function\">multiset</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span>  查找一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">count</span><span class=\"params\">()</span>  返回某一个数的个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">1</span>)</span> 输入是一个数x，删除所有x   <span class=\"title\">O</span><span class=\"params\">(k + logn)</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">2</span>)</span> 输入一个迭代器，删除这个迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">lower_bound</span><span class=\"params\">(x)</span>  返回大于等于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">upper_bound</span><span class=\"params\">(x)</span>  返回大于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">    map/multimap</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入的数是一个pair</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span>  输入的参数是pair或者迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        []  注意multimap不支持此操作。 时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(logn)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">unordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表</span></span><br><span class=\"line\"><span class=\"function\">    和上面类似，增删改查的时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(<span class=\"number\">1</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    不支持 <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span>， 迭代器的++，--</span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">bitset, 圧位</span></span><br><span class=\"line\"><span class=\"function\">    bitset&lt;10000&gt; s</span>;</span><br><span class=\"line\">    ~, &amp;, |, ^</span><br><span class=\"line\">    &gt;&gt;, &lt;&lt;</span><br><span class=\"line\">    ==, !=</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">count</span>()  返回有多少个<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">any</span>()  判断是否至少有一个<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">none</span>()  判断是否全为<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">set</span>()  把所有位置成<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">set</span>(k, v)  将第k位变成v</span><br><span class=\"line\">    <span class=\"built_in\">reset</span>()  把所有位变成<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"built_in\">flip</span>()  等价于~</span><br><span class=\"line\">    <span class=\"built_in\">flip</span>(k) 把第k位取反</span><br></pre></td></tr></table></figure>","more":"<h2 id=\"链表与邻接表\"><a href=\"#链表与邻接表\" class=\"headerlink\" title=\"链表与邻接表\"></a>链表与邻接表</h2><p>由于用<code>结构体+指针</code>比较慢，一般在面试题使用，在这里使用<strong>数组</strong>模拟链表</p>\n<ul>\n<li>单链表</li>\n</ul>\n<p><code>e[N]</code>：储存链表结点的值</p>\n<p><code>ne[N]</code>：储存结点的下一个结点下标，其中空结点下标为-1</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> head, e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = <span class=\"number\">-1</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在链表头插入一个数a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = head, head = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//在a插到下标是k的结点后面</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将头结点删除，需要保证头结点存在</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = ne[head];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将下标为k的结点的后一个点删除</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ne[k] = ne[ne[k]];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双链表</li>\n</ul>\n<p>作用：优化某些问题</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> e[N], l[N], r[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//0是左端点，1是右端点</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">1</span>, l[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在节点a的右边插入一个数x</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = x;</span><br><span class=\"line\">    l[idx] = a, r[idx] = r[a];</span><br><span class=\"line\">    l[r[a]] = idx, r[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 删除节点a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    l[r[a]] = l[a];</span><br><span class=\"line\">    r[l[a]] = r[a];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>邻接表</li>\n</ul>\n<p>N个单链表，用于存储树和图</p>\n<h2 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h2><p>先进后出(FILO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tt表示栈顶</span></span><br><span class=\"line\"><span class=\"type\">int</span> stk[N], tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向栈顶插入一个数</span></span><br><span class=\"line\">stk[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从栈顶弹出一个数</span></span><br><span class=\"line\">tt -- ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 栈顶的值</span></span><br><span class=\"line\">stk[tt];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断栈是否为空，如果 tt &gt; 0，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">not</span> empty</span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    empty</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"队列\"><a href=\"#队列\" class=\"headerlink\" title=\"队列\"></a>队列</h2><p>先进先出(FIFO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果 hh &lt;= tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh &lt;= tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>循环队列</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾的后一个位置</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[tt ++ ] = x;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt == N) tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh == N) hh = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果hh != tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh != tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调栈\"><a href=\"#单调栈\" class=\"headerlink\" title=\"单调栈\"></a>单调栈</h2><p>题型：求给定序列每一个数左&#x2F;右边离他最近的比他大&#x2F;小的数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (tt &amp;&amp; <span class=\"built_in\">check</span>(stk[tt], i)) tt -- ;</span><br><span class=\"line\">    stk[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调队列\"><a href=\"#单调队列\" class=\"headerlink\" title=\"单调队列\"></a>单调队列</h2><p>题型：求滑动窗口的最大&#x2F;小值</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check_out</span>(q[hh])) hh ++ ;  <span class=\"comment\">// 判断队头是否滑出窗口</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check</span>(q[tt], i)) tt -- ;</span><br><span class=\"line\">    q[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><p>对于字符串$s$,判断是否包含模式串$t$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度</span></span><br><span class=\"line\"><span class=\"comment\">//求模式串的Next数组：</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>, j = <span class=\"number\">0</span>; i &lt;= m; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; p[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    ne[i] = j;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 匹配</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>, j = <span class=\"number\">0</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; s[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (j == m)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        j = ne[j];</span><br><span class=\"line\">        <span class=\"comment\">// 匹配成功后的逻辑</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">s.<span class=\"built_in\">find</span>(t) != s.npos</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Trie树\"><a href=\"#Trie树\" class=\"headerlink\" title=\"Trie树\"></a>Trie树</h2><p>快速存储和查找字符串集合的数据结构</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> son[N][<span class=\"number\">26</span>], cnt[N], idx;</span><br><span class=\"line\"><span class=\"comment\">// 0号点既是根节点，又是空节点</span></span><br><span class=\"line\"><span class=\"comment\">// son[][]存储树中每个节点的子节点</span></span><br><span class=\"line\"><span class=\"comment\">// cnt[]存储以每个节点结尾的单词数量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 插入一个字符串</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) son[p][u] = ++ idx;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cnt[p] ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 查询字符串出现的次数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">query</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cnt[p];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/145/\">最大异或对</a></p>\n<p><a href=\"https://www.acwing.com/problem/content/144/\">前缀统计</a></p>\n<h2 id=\"并查集\"><a href=\"#并查集\" class=\"headerlink\" title=\"并查集\"></a>并查集</h2><ol>\n<li>将两个集合合并</li>\n<li>询问两个元素是否在一个集合当中</li>\n</ol>\n<p>近乎$O(1)$</p>\n<p>基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点</p>\n<ul>\n<li>如何判断是树根？</li>\n</ul>\n<p>$p[x] &#x3D; x$</p>\n<ul>\n<li>如何求$x$的集合编号</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(p[x]!=x) x = p[x]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何合并两个区间</li>\n</ul>\n<p>设p[x]为x集合编号，p[y]是y集合编号。p[x]&#x3D;y</p>\n<p>优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">1</span>)朴素并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N]; <span class=\"comment\">//存储每个点的祖宗节点</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点+路径压缩</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">2</span>)维护size的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], size[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        size[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    size[<span class=\"built_in\">find</span>(b)] += size[<span class=\"built_in\">find</span>(a)];</span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">3</span>)维护到祖宗节点距离的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], d[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> u = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">            d[x] += d[p[x]];</span><br><span class=\"line\">            p[x] = u;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        d[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">    d[<span class=\"built_in\">find</span>(a)] = distance; <span class=\"comment\">// 根据具体问题，初始化find(a)的偏移量</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"C-STL\"><a href=\"#C-STL\" class=\"headerlink\" title=\"C++ STL\"></a>C++ STL</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vector, 变长数组，倍增的思想</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()  返回元素个数</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()  返回是否为空</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()  清空</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\">    支持比较运算，按字典序</span><br><span class=\"line\"></span><br><span class=\"line\">pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt;</span><br><span class=\"line\">    first, 第一个元素</span><br><span class=\"line\">    second, 第二个元素</span><br><span class=\"line\">    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）</span><br><span class=\"line\"></span><br><span class=\"line\">string，字符串</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()/<span class=\"built_in\">length</span>()  返回字符串长度</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">substr</span>(起始下标，(子串长度))  返回子串</span><br><span class=\"line\">    <span class=\"built_in\">c_str</span>()  返回字符串所在字符数组的起始地址</span><br><span class=\"line\"></span><br><span class=\"line\">queue, 队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向队尾插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()  返回队头元素</span><br><span class=\"line\">    <span class=\"built_in\">back</span>()  返回队尾元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出队头元素</span><br><span class=\"line\"></span><br><span class=\"line\">priority_queue, 优先队列，默认是大根堆</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回堆顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出堆顶元素</span><br><span class=\"line\">    定义成小根堆的方式：priority_queue&lt;<span class=\"type\">int</span>, vector&lt;<span class=\"type\">int</span>&gt;, greater&lt;<span class=\"type\">int</span>&gt;&gt; q;</span><br><span class=\"line\"></span><br><span class=\"line\">stack, 栈</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向栈顶插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回栈顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出栈顶元素</span><br><span class=\"line\"></span><br><span class=\"line\">deque, 双端队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_front</span>()/<span class=\"built_in\">pop_front</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    ++, -- 返回前驱和后继，时间复杂度 <span class=\"built_in\">O</span>(logn)</span><br><span class=\"line\"></span><br><span class=\"line\">    set/<span class=\"function\">multiset</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span>  查找一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">count</span><span class=\"params\">()</span>  返回某一个数的个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">1</span>)</span> 输入是一个数x，删除所有x   <span class=\"title\">O</span><span class=\"params\">(k + logn)</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">2</span>)</span> 输入一个迭代器，删除这个迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">lower_bound</span><span class=\"params\">(x)</span>  返回大于等于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">upper_bound</span><span class=\"params\">(x)</span>  返回大于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">    map/multimap</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入的数是一个pair</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span>  输入的参数是pair或者迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        []  注意multimap不支持此操作。 时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(logn)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">unordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表</span></span><br><span class=\"line\"><span class=\"function\">    和上面类似，增删改查的时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(<span class=\"number\">1</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    不支持 <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span>， 迭代器的++，--</span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">bitset, 圧位</span></span><br><span class=\"line\"><span class=\"function\">    bitset&lt;10000&gt; s</span>;</span><br><span class=\"line\">    ~, &amp;, |, ^</span><br><span class=\"line\">    &gt;&gt;, &lt;&lt;</span><br><span class=\"line\">    ==, !=</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">count</span>()  返回有多少个<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">any</span>()  判断是否至少有一个<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">none</span>()  判断是否全为<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">set</span>()  把所有位置成<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">set</span>(k, v)  将第k位变成v</span><br><span class=\"line\">    <span class=\"built_in\">reset</span>()  把所有位变成<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"built_in\">flip</span>()  等价于~</span><br><span class=\"line\">    <span class=\"built_in\">flip</span>(k) 把第k位取反</span><br></pre></td></tr></table></figure>"},{"title":"Algorithm-DP","mathjax":true,"date":"2023-09-13T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛中的动态规划与背包问题","_content":"\n### 动态规划\n\n- 状态表示\n\n  $f[i,j]$\n\n  - 集合\n  - 属性（Max，Min，Cnt）\n\n- 状态计算\n\n  - 集合的划分\n\n### 线性DP\n\n**[数字三角形](https://www.luogu.com.cn/problem/P1216)**\n\n$O(n^2)$\n\n$f[i,j]$表示到坐标为$[i,j]$的路径的和最大值\n\n$f[1][1] = a[1][1]$\n\n$f[i][j] = max(f[i-1][j-1]+a[i][j], f[i-1][j]+a[i][j])$\n\n**[最长上升子序列](https://www.luogu.com.cn/problem/B3637)**\n\n**朴素**：$O(n^2)$\n\n$f[i]$表示以第$i$个数结尾的序列中上升子序列长度的最大值\n\n遍历$a_i$所有可能的前一个数$a_j$($a_j<a_i$且$0 \\le j \\le i-1$)\n\n$f[i] = max(f[j]+1,f[i]),j \\in [0,i-1]$\n\n如果要保存最长序列：$g[i]$保存从哪一步$j$转移过来\n\n代码：https://www.luogu.com.cn/record/124595657\n\n**优化**：$O(nlogn)$\n\n用一个q数组储存长度为i的序列的结尾数字的最小值\n\n可以证明$q_i>q_{i-1}>...>q_2>q_1$，即数组严格单调递增\n\n对于$a_i$，二分找到最大的$q_k<=a_i$，$f[i] = k+1$，更新$q_k = a_i$\n\n代码：https://www.luogu.com.cn/record/133704642\n\n**[最长公共子序列](https://www.luogu.com.cn/problem/P1439)**\n\n**朴素**：$O(n^2)$\n\n$f[i][j]$表示所有在第一个序列的前i个字母中出现，且在第二个序列的前j个字母中出现的子序列的最大值\n\n集合划分依据：是否选择$a[i],b[j]$\n\n分为四个集合：选择$a[i],b[j]$ ; 选择$a[i]$ 不选择$b[j]$ ; 不选择$a[j]$选择$b[j]$ ; 都不选择$a[i],b[j]$\n\n分别表示为 $f[i-1][j-1] , f[i,j-1] , f[i-1][j] , f[i-1][j-1]+1$\n\n其中第二三种情况**包含**上面对应的集合（由于是求Max，所以有重复不影响结果）\n\n且第二三种集合也包含第一个集合，所以只要对后三种集合求最大值即可\n\n$f[i,j] = max(f[i-1,j],f[i,j-1])$\n\n当$a[i]==b[j]$时,$f[i,j] = max(f[i,j],f[i-1,j-1]+1)$\n\n**优化**：$O(nlogn)$\n\n**[编辑距离](https://www.luogu.com.cn/problem/P2758)**\n\n$f[i,j]$所有将$a[1-i]$变成$b[1-j]$的操作方式的最小步数\n\n区间划分，①删除最后一个数、②增加最后一个数、③修改最后一个数\n\n① $f[i-1,j]+1$\n\n②$f[i,j-1]+1$\n\n③$f[i-1,j-1]+1$ （如果$a[i]==b[j]$则不需要加一，即不需要进行修改操作）\n\n### 区间DP\n\n**[石子合并](https://www.acwing.com/problem/content/284/)**\n\n$f[i,j]$表示将第$i$堆石子到第$j$堆石子合并成一堆石子的方式的代价最小值/最大值\n\n$O(n^3)$\n\n```cpp\nfor(int len=2;len<=n;len++){\n    for(int i=1;i+len-1<=n;i++){\n        int l = i,r = i+len-1;\n        f_max[l][r] = -1e8,f_min[l][r] = 1e8;\n        for(int k=l;k<r;k++){\n            f_max[l][r] = max(f_max[l][r],f_max[l][k]+f_max[k+1][r]+s[r]-s[l-1]);\n            f_min[l][r] = min(f_min[l][r],f_min[l][k]+f_min[k+1][r]+s[r]-s[l-1]);\n        }\n    }\n}\n```\n\n### 计数类DP\n\n### 数位统计DP\n\n**[计数问题](https://www.acwing.com/problem/content/340/)**\n\n设$n=abcdefg$，枚举第$i$位是 $x \\in [0,9]$\n\n举例$x=1,i=4$的情况：\n\n设数字为$xxx1yyy$\n\n- 当$abc>xxx,xxx \\in [000,abc-1], y \\in [000,999]$，则共有$abc * 1000$个\n\n- 当$abc<xxx$，则共有0个\n\n- 当\n\n  $abc=xxx$\n\n  - 当$d<1$，无解\n  - 当$d=1$，$yyy \\in [000,efg]$,则有$efg+1$种\n  - 当$d>1$，$yyy \\in [000,999]$,有1000种\n\n**当x=0时，注意前导0，即对于第一种情况，$xxx \\in [001,abc-1]$**，即有$(abc-1)*1000$情况\n\n**[圆形数字](https://www.acwing.com/problem/content/341/)**\n\n### 状态压缩DP\n\n**[蒙德里安的梦想](https://www.acwing.com/problem/content/293/)**\n\n$f[i][j]$表示第i列，上一列横着摆的数量j,其中j是一个二进制数。\n\n**[最短Hamilton路径](https://www.acwing.com/problem/content/93/)**\n\n$f[i][j]$表示从0号点走到j号点，走过的所有点是i的所有路径(二进制数i表示某个点是否已经走过了)的最小路径长度\n\n### 树形DP\n\n**[没有上司的舞会](https://www.acwing.com/problem/content/287/)**\n\n$f[u][0]$表示所有以u为根的子树中选择，并且不选u这个点的方案的最大值\n\n$f[u][1]$表示所有以u为根的子树中选择，并且选u这个点的方案的最大值\n\n设点u的子节点$s_1,s_2,s_3....s_i$\n\n$f[u][0] = \\sum_{1}^{i}max(f[s_i][0],f[s_i][1])$\n\n$f[u][1] = \\sum_{1}^{i}f[s_i][0]$\n\n找出根节点，递归求最大值即可\n\n### 记忆化搜索\n\n**[滑雪](https://www.luogu.com.cn/problem/P1434)**\n\n用$s[i][j]$表示从(i,j)点出发能走的最长距离。\n\n每次搜索一次记忆一次即可。\n\n举例\n\n```none\n3 3 \n1 1 3\n2 3 4\n1 1 1\n```\n\n先去找(1,1)的最长距离，很明显为1\n\n接着找(1,2)的最长距离，很明显为1\n\n接着找(1,3)的最长距离，为2((1,3)->(1,2))\n\n然后找(2,1)的最长距离，为2((2,1)->(1,1))\n\n然后是(2,2)的最长距离，如果没有记忆化，那么搜索过程为：(2,2)->(2,1)->(1,1)\n\n但是（2,1）之前已经搜过了，再去搜就是浪费时间，之前搜索已经知道(2,1)的值为2，那么搜索过程就是缩短为：(2,2)->(2,1),即为3\n\n### 背包问题\n\n给定$n$个物品和容量$v$的背包，每个物品都有体积$v_i$和价值$w_i$，求当$\\sum_{i=1}^{n} v_i \\le v$时最大的$w$是多少\n\n#### 01背包问题\n\n每个物品只能用0/1次\n\n$f[i,j] = max(f[i-1,j],f[i-1,j-v_i]+w_i)$\n\n[01背包问题](https://www.acwing.com/problem/content/2/)\n\n[采药](https://www.luogu.com.cn/problem/P1048)\n\n#### 完全背包问题\n\n物品可以无限次使用\n\n$f[i,j] = Max(f[i-1,j-v_i \\times k]+w[i] \\times k)$\n\n$k \\subseteq [0,\\frac{j}{v_i}]$\n\n即$f[i,j] = Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,....,f[i-1][j-kv_i]+kw_i)$\n\n$f[i,j-v_i] = Max(f[i-1][j-v_i],f[i-1][j-2v_i]+w_i,...,f[i-1][j-kv_i]+(k-1)w_i)$\n\n$f[i][j]$的后$k$项等于$f[i][j-v_i]+w_i$\n\n得\n\n$f[i,j] = Max(f[i-1,j],f[i,j-v_i]+w_i)$\n\n[完全背包问题](https://www.acwing.com/problem/content/3/)\n\n#### 多重背包物品\n\n每个物品的个数不一致\n\n朴素做法\n\n$f[i,j] = Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,....,f[i-1][j-kv_i]+kw_i)$\n\n$k \\subseteq [0,s_i]$\n\n三重循环即可\n\n[多重背包问题1](https://www.acwing.com/problem/content/4/)\n\n**优化：二进制优化**\n\n```cpp\nfor(int i=1;i<=n;i++){\n    int a,b,s;\n    cin>>a>>b>>s;\n    //v w s;\n    int k = 1;\n    while(k<=s){\n        cnt++;\n        v[cnt] = a*k;\n        w[cnt] = b*k;\n        s-=k;\n        k*=2;\n    }\n    if(s>0){\n        cnt++;\n        v[cnt] = a*s;\n        w[cnt] = b*s;\n    }\n}\n```\n\n对物品进行二进制分组，组数为$cnt$，转化为01背包问题求解\n\n```cpp\nn = cnt;\nfor(int i=1;i<=n;i++){\n    for(int j=0;j<=m;j++){\n        f[i][j] = f[i-1][j];\n        if(j>=v[i]) f[i][j] = max(f[i][j],f[i-1][j-v[i]]+w[i]);\n    }\n}\ncout<<f[n][m]<<endl;\n```\n\n[多重背包问题2](https://www.acwing.com/problem/content/5/)\n\n#### 分组背包问题\n\n有$N$组，每一组只能选其中一种物品\n\n$f[i][j] = Max(f[i-1,j],f[i-1,j-v_{i,k}]+w_{i,k})$\n\n[分组背包问题](https://www.acwing.com/problem/content/9/)\n\n\n\n","source":"_posts/algorithm-dp.md","raw":"---\ntitle: Algorithm-DP\nmathjax: true\ndate: 2023/09/13 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛中的动态规划与背包问题\n---\n\n### 动态规划\n\n- 状态表示\n\n  $f[i,j]$\n\n  - 集合\n  - 属性（Max，Min，Cnt）\n\n- 状态计算\n\n  - 集合的划分\n\n### 线性DP\n\n**[数字三角形](https://www.luogu.com.cn/problem/P1216)**\n\n$O(n^2)$\n\n$f[i,j]$表示到坐标为$[i,j]$的路径的和最大值\n\n$f[1][1] = a[1][1]$\n\n$f[i][j] = max(f[i-1][j-1]+a[i][j], f[i-1][j]+a[i][j])$\n\n**[最长上升子序列](https://www.luogu.com.cn/problem/B3637)**\n\n**朴素**：$O(n^2)$\n\n$f[i]$表示以第$i$个数结尾的序列中上升子序列长度的最大值\n\n遍历$a_i$所有可能的前一个数$a_j$($a_j<a_i$且$0 \\le j \\le i-1$)\n\n$f[i] = max(f[j]+1,f[i]),j \\in [0,i-1]$\n\n如果要保存最长序列：$g[i]$保存从哪一步$j$转移过来\n\n代码：https://www.luogu.com.cn/record/124595657\n\n**优化**：$O(nlogn)$\n\n用一个q数组储存长度为i的序列的结尾数字的最小值\n\n可以证明$q_i>q_{i-1}>...>q_2>q_1$，即数组严格单调递增\n\n对于$a_i$，二分找到最大的$q_k<=a_i$，$f[i] = k+1$，更新$q_k = a_i$\n\n代码：https://www.luogu.com.cn/record/133704642\n\n**[最长公共子序列](https://www.luogu.com.cn/problem/P1439)**\n\n**朴素**：$O(n^2)$\n\n$f[i][j]$表示所有在第一个序列的前i个字母中出现，且在第二个序列的前j个字母中出现的子序列的最大值\n\n集合划分依据：是否选择$a[i],b[j]$\n\n分为四个集合：选择$a[i],b[j]$ ; 选择$a[i]$ 不选择$b[j]$ ; 不选择$a[j]$选择$b[j]$ ; 都不选择$a[i],b[j]$\n\n分别表示为 $f[i-1][j-1] , f[i,j-1] , f[i-1][j] , f[i-1][j-1]+1$\n\n其中第二三种情况**包含**上面对应的集合（由于是求Max，所以有重复不影响结果）\n\n且第二三种集合也包含第一个集合，所以只要对后三种集合求最大值即可\n\n$f[i,j] = max(f[i-1,j],f[i,j-1])$\n\n当$a[i]==b[j]$时,$f[i,j] = max(f[i,j],f[i-1,j-1]+1)$\n\n**优化**：$O(nlogn)$\n\n**[编辑距离](https://www.luogu.com.cn/problem/P2758)**\n\n$f[i,j]$所有将$a[1-i]$变成$b[1-j]$的操作方式的最小步数\n\n区间划分，①删除最后一个数、②增加最后一个数、③修改最后一个数\n\n① $f[i-1,j]+1$\n\n②$f[i,j-1]+1$\n\n③$f[i-1,j-1]+1$ （如果$a[i]==b[j]$则不需要加一，即不需要进行修改操作）\n\n### 区间DP\n\n**[石子合并](https://www.acwing.com/problem/content/284/)**\n\n$f[i,j]$表示将第$i$堆石子到第$j$堆石子合并成一堆石子的方式的代价最小值/最大值\n\n$O(n^3)$\n\n```cpp\nfor(int len=2;len<=n;len++){\n    for(int i=1;i+len-1<=n;i++){\n        int l = i,r = i+len-1;\n        f_max[l][r] = -1e8,f_min[l][r] = 1e8;\n        for(int k=l;k<r;k++){\n            f_max[l][r] = max(f_max[l][r],f_max[l][k]+f_max[k+1][r]+s[r]-s[l-1]);\n            f_min[l][r] = min(f_min[l][r],f_min[l][k]+f_min[k+1][r]+s[r]-s[l-1]);\n        }\n    }\n}\n```\n\n### 计数类DP\n\n### 数位统计DP\n\n**[计数问题](https://www.acwing.com/problem/content/340/)**\n\n设$n=abcdefg$，枚举第$i$位是 $x \\in [0,9]$\n\n举例$x=1,i=4$的情况：\n\n设数字为$xxx1yyy$\n\n- 当$abc>xxx,xxx \\in [000,abc-1], y \\in [000,999]$，则共有$abc * 1000$个\n\n- 当$abc<xxx$，则共有0个\n\n- 当\n\n  $abc=xxx$\n\n  - 当$d<1$，无解\n  - 当$d=1$，$yyy \\in [000,efg]$,则有$efg+1$种\n  - 当$d>1$，$yyy \\in [000,999]$,有1000种\n\n**当x=0时，注意前导0，即对于第一种情况，$xxx \\in [001,abc-1]$**，即有$(abc-1)*1000$情况\n\n**[圆形数字](https://www.acwing.com/problem/content/341/)**\n\n### 状态压缩DP\n\n**[蒙德里安的梦想](https://www.acwing.com/problem/content/293/)**\n\n$f[i][j]$表示第i列，上一列横着摆的数量j,其中j是一个二进制数。\n\n**[最短Hamilton路径](https://www.acwing.com/problem/content/93/)**\n\n$f[i][j]$表示从0号点走到j号点，走过的所有点是i的所有路径(二进制数i表示某个点是否已经走过了)的最小路径长度\n\n### 树形DP\n\n**[没有上司的舞会](https://www.acwing.com/problem/content/287/)**\n\n$f[u][0]$表示所有以u为根的子树中选择，并且不选u这个点的方案的最大值\n\n$f[u][1]$表示所有以u为根的子树中选择，并且选u这个点的方案的最大值\n\n设点u的子节点$s_1,s_2,s_3....s_i$\n\n$f[u][0] = \\sum_{1}^{i}max(f[s_i][0],f[s_i][1])$\n\n$f[u][1] = \\sum_{1}^{i}f[s_i][0]$\n\n找出根节点，递归求最大值即可\n\n### 记忆化搜索\n\n**[滑雪](https://www.luogu.com.cn/problem/P1434)**\n\n用$s[i][j]$表示从(i,j)点出发能走的最长距离。\n\n每次搜索一次记忆一次即可。\n\n举例\n\n```none\n3 3 \n1 1 3\n2 3 4\n1 1 1\n```\n\n先去找(1,1)的最长距离，很明显为1\n\n接着找(1,2)的最长距离，很明显为1\n\n接着找(1,3)的最长距离，为2((1,3)->(1,2))\n\n然后找(2,1)的最长距离，为2((2,1)->(1,1))\n\n然后是(2,2)的最长距离，如果没有记忆化，那么搜索过程为：(2,2)->(2,1)->(1,1)\n\n但是（2,1）之前已经搜过了，再去搜就是浪费时间，之前搜索已经知道(2,1)的值为2，那么搜索过程就是缩短为：(2,2)->(2,1),即为3\n\n### 背包问题\n\n给定$n$个物品和容量$v$的背包，每个物品都有体积$v_i$和价值$w_i$，求当$\\sum_{i=1}^{n} v_i \\le v$时最大的$w$是多少\n\n#### 01背包问题\n\n每个物品只能用0/1次\n\n$f[i,j] = max(f[i-1,j],f[i-1,j-v_i]+w_i)$\n\n[01背包问题](https://www.acwing.com/problem/content/2/)\n\n[采药](https://www.luogu.com.cn/problem/P1048)\n\n#### 完全背包问题\n\n物品可以无限次使用\n\n$f[i,j] = Max(f[i-1,j-v_i \\times k]+w[i] \\times k)$\n\n$k \\subseteq [0,\\frac{j}{v_i}]$\n\n即$f[i,j] = Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,....,f[i-1][j-kv_i]+kw_i)$\n\n$f[i,j-v_i] = Max(f[i-1][j-v_i],f[i-1][j-2v_i]+w_i,...,f[i-1][j-kv_i]+(k-1)w_i)$\n\n$f[i][j]$的后$k$项等于$f[i][j-v_i]+w_i$\n\n得\n\n$f[i,j] = Max(f[i-1,j],f[i,j-v_i]+w_i)$\n\n[完全背包问题](https://www.acwing.com/problem/content/3/)\n\n#### 多重背包物品\n\n每个物品的个数不一致\n\n朴素做法\n\n$f[i,j] = Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,....,f[i-1][j-kv_i]+kw_i)$\n\n$k \\subseteq [0,s_i]$\n\n三重循环即可\n\n[多重背包问题1](https://www.acwing.com/problem/content/4/)\n\n**优化：二进制优化**\n\n```cpp\nfor(int i=1;i<=n;i++){\n    int a,b,s;\n    cin>>a>>b>>s;\n    //v w s;\n    int k = 1;\n    while(k<=s){\n        cnt++;\n        v[cnt] = a*k;\n        w[cnt] = b*k;\n        s-=k;\n        k*=2;\n    }\n    if(s>0){\n        cnt++;\n        v[cnt] = a*s;\n        w[cnt] = b*s;\n    }\n}\n```\n\n对物品进行二进制分组，组数为$cnt$，转化为01背包问题求解\n\n```cpp\nn = cnt;\nfor(int i=1;i<=n;i++){\n    for(int j=0;j<=m;j++){\n        f[i][j] = f[i-1][j];\n        if(j>=v[i]) f[i][j] = max(f[i][j],f[i-1][j-v[i]]+w[i]);\n    }\n}\ncout<<f[n][m]<<endl;\n```\n\n[多重背包问题2](https://www.acwing.com/problem/content/5/)\n\n#### 分组背包问题\n\n有$N$组，每一组只能选其中一种物品\n\n$f[i][j] = Max(f[i-1,j],f[i-1,j-v_{i,k}]+w_{i,k})$\n\n[分组背包问题](https://www.acwing.com/problem/content/9/)\n\n\n\n","slug":"algorithm-dp","published":1,"updated":"2025-02-28T03:06:38.010Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9c0005bs99gyie7rqn","content":"<h3 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h3><ul>\n<li><p>状态表示</p>\n<p>$f[i,j]$</p>\n<ul>\n<li>集合</li>\n<li>属性（Max，Min，Cnt）</li>\n</ul>\n</li>\n<li><p>状态计算</p>\n<ul>\n<li>集合的划分</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"线性DP\"><a href=\"#线性DP\" class=\"headerlink\" title=\"线性DP\"></a>线性DP</h3><p><strong><a href=\"https://www.luogu.com.cn/problem/P1216\">数字三角形</a></strong></p>\n<p>$O(n^2)$</p>\n<p>$f[i,j]$表示到坐标为$[i,j]$的路径的和最大值</p>\n<p>$f[1][1] &#x3D; a[1][1]$</p>\n<p>$f[i][j] &#x3D; max(f[i-1][j-1]+a[i][j], f[i-1][j]+a[i][j])$</p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/B3637\">最长上升子序列</a></strong></p>\n<p><strong>朴素</strong>：$O(n^2)$</p>\n<p>$f[i]$表示以第$i$个数结尾的序列中上升子序列长度的最大值</p>\n<p>遍历$a_i$所有可能的前一个数$a_j$($a_j&lt;a_i$且$0 \\le j \\le i-1$)</p>\n<p>$f[i] &#x3D; max(f[j]+1,f[i]),j \\in [0,i-1]$</p>\n<p>如果要保存最长序列：$g[i]$保存从哪一步$j$转移过来</p>\n<p>代码：<a href=\"https://www.luogu.com.cn/record/124595657\">https://www.luogu.com.cn/record/124595657</a></p>\n<p><strong>优化</strong>：$O(nlogn)$</p>\n<p>用一个q数组储存长度为i的序列的结尾数字的最小值</p>\n<p>可以证明$q_i&gt;q_{i-1}&gt;…&gt;q_2&gt;q_1$，即数组严格单调递增</p>\n<p>对于$a_i$，二分找到最大的$q_k&lt;&#x3D;a_i$，$f[i] &#x3D; k+1$，更新$q_k &#x3D; a_i$</p>\n<p>代码：<a href=\"https://www.luogu.com.cn/record/133704642\">https://www.luogu.com.cn/record/133704642</a></p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/P1439\">最长公共子序列</a></strong></p>\n<p><strong>朴素</strong>：$O(n^2)$</p>\n<p>$f[i][j]$表示所有在第一个序列的前i个字母中出现，且在第二个序列的前j个字母中出现的子序列的最大值</p>\n<p>集合划分依据：是否选择$a[i],b[j]$</p>\n<p>分为四个集合：选择$a[i],b[j]$ ; 选择$a[i]$ 不选择$b[j]$ ; 不选择$a[j]$选择$b[j]$ ; 都不选择$a[i],b[j]$</p>\n<p>分别表示为 $f[i-1][j-1] , f[i,j-1] , f[i-1][j] , f[i-1][j-1]+1$</p>\n<p>其中第二三种情况<strong>包含</strong>上面对应的集合（由于是求Max，所以有重复不影响结果）</p>\n<p>且第二三种集合也包含第一个集合，所以只要对后三种集合求最大值即可</p>\n<p>$f[i,j] &#x3D; max(f[i-1,j],f[i,j-1])$</p>\n<p>当$a[i]&#x3D;&#x3D;b[j]$时,$f[i,j] &#x3D; max(f[i,j],f[i-1,j-1]+1)$</p>\n<p><strong>优化</strong>：$O(nlogn)$</p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/P2758\">编辑距离</a></strong></p>\n<p>$f[i,j]$所有将$a[1-i]$变成$b[1-j]$的操作方式的最小步数</p>\n<p>区间划分，①删除最后一个数、②增加最后一个数、③修改最后一个数</p>\n<p>① $f[i-1,j]+1$</p>\n<p>②$f[i,j-1]+1$</p>\n<p>③$f[i-1,j-1]+1$ （如果$a[i]&#x3D;&#x3D;b[j]$则不需要加一，即不需要进行修改操作）</p>\n<h3 id=\"区间DP\"><a href=\"#区间DP\" class=\"headerlink\" title=\"区间DP\"></a>区间DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/284/\">石子合并</a></strong></p>\n<p>$f[i,j]$表示将第$i$堆石子到第$j$堆石子合并成一堆石子的方式的代价最小值&#x2F;最大值</p>\n<p>$O(n^3)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> len=<span class=\"number\">2</span>;len&lt;=n;len++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i+len<span class=\"number\">-1</span>&lt;=n;i++)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> l = i,r = i+len<span class=\"number\">-1</span>;</span><br><span class=\"line\">        f_max[l][r] = <span class=\"number\">-1e8</span>,f_min[l][r] = <span class=\"number\">1e8</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> k=l;k&lt;r;k++)&#123;</span><br><span class=\"line\">            f_max[l][r] = <span class=\"built_in\">max</span>(f_max[l][r],f_max[l][k]+f_max[k<span class=\"number\">+1</span>][r]+s[r]-s[l<span class=\"number\">-1</span>]);</span><br><span class=\"line\">            f_min[l][r] = <span class=\"built_in\">min</span>(f_min[l][r],f_min[l][k]+f_min[k<span class=\"number\">+1</span>][r]+s[r]-s[l<span class=\"number\">-1</span>]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"计数类DP\"><a href=\"#计数类DP\" class=\"headerlink\" title=\"计数类DP\"></a>计数类DP</h3><h3 id=\"数位统计DP\"><a href=\"#数位统计DP\" class=\"headerlink\" title=\"数位统计DP\"></a>数位统计DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/340/\">计数问题</a></strong></p>\n<p>设$n&#x3D;abcdefg$，枚举第$i$位是 $x \\in [0,9]$</p>\n<p>举例$x&#x3D;1,i&#x3D;4$的情况：</p>\n<p>设数字为$xxx1yyy$</p>\n<ul>\n<li><p>当$abc&gt;xxx,xxx \\in [000,abc-1], y \\in [000,999]$，则共有$abc * 1000$个</p>\n</li>\n<li><p>当$abc&lt;xxx$，则共有0个</p>\n</li>\n<li><p>当</p>\n<p>$abc&#x3D;xxx$</p>\n<ul>\n<li>当$d&lt;1$，无解</li>\n<li>当$d&#x3D;1$，$yyy \\in [000,efg]$,则有$efg+1$种</li>\n<li>当$d&gt;1$，$yyy \\in [000,999]$,有1000种</li>\n</ul>\n</li>\n</ul>\n<p>**当x&#x3D;0时，注意前导0，即对于第一种情况，$xxx \\in [001,abc-1]$**，即有$(abc-1)*1000$情况</p>\n<p><strong><a href=\"https://www.acwing.com/problem/content/341/\">圆形数字</a></strong></p>\n<h3 id=\"状态压缩DP\"><a href=\"#状态压缩DP\" class=\"headerlink\" title=\"状态压缩DP\"></a>状态压缩DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/293/\">蒙德里安的梦想</a></strong></p>\n<p>$f[i][j]$表示第i列，上一列横着摆的数量j,其中j是一个二进制数。</p>\n<p><strong><a href=\"https://www.acwing.com/problem/content/93/\">最短Hamilton路径</a></strong></p>\n<p>$f[i][j]$表示从0号点走到j号点，走过的所有点是i的所有路径(二进制数i表示某个点是否已经走过了)的最小路径长度</p>\n<h3 id=\"树形DP\"><a href=\"#树形DP\" class=\"headerlink\" title=\"树形DP\"></a>树形DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/287/\">没有上司的舞会</a></strong></p>\n<p>$f[u][0]$表示所有以u为根的子树中选择，并且不选u这个点的方案的最大值</p>\n<p>$f[u][1]$表示所有以u为根的子树中选择，并且选u这个点的方案的最大值</p>\n<p>设点u的子节点$s_1,s_2,s_3….s_i$</p>\n<p>$f[u][0] &#x3D; \\sum_{1}^{i}max(f[s_i][0],f[s_i][1])$</p>\n<p>$f[u][1] &#x3D; \\sum_{1}^{i}f[s_i][0]$</p>\n<p>找出根节点，递归求最大值即可</p>\n<h3 id=\"记忆化搜索\"><a href=\"#记忆化搜索\" class=\"headerlink\" title=\"记忆化搜索\"></a>记忆化搜索</h3><p><strong><a href=\"https://www.luogu.com.cn/problem/P1434\">滑雪</a></strong></p>\n<p>用$s[i][j]$表示从(i,j)点出发能走的最长距离。</p>\n<p>每次搜索一次记忆一次即可。</p>\n<p>举例</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">3 3 </span><br><span class=\"line\">1 1 3</span><br><span class=\"line\">2 3 4</span><br><span class=\"line\">1 1 1</span><br></pre></td></tr></table></figure>\n\n<p>先去找(1,1)的最长距离，很明显为1</p>\n<p>接着找(1,2)的最长距离，很明显为1</p>\n<p>接着找(1,3)的最长距离，为2((1,3)-&gt;(1,2))</p>\n<p>然后找(2,1)的最长距离，为2((2,1)-&gt;(1,1))</p>\n<p>然后是(2,2)的最长距离，如果没有记忆化，那么搜索过程为：(2,2)-&gt;(2,1)-&gt;(1,1)</p>\n<p>但是（2,1）之前已经搜过了，再去搜就是浪费时间，之前搜索已经知道(2,1)的值为2，那么搜索过程就是缩短为：(2,2)-&gt;(2,1),即为3</p>\n<h3 id=\"背包问题\"><a href=\"#背包问题\" class=\"headerlink\" title=\"背包问题\"></a>背包问题</h3><p>给定$n$个物品和容量$v$的背包，每个物品都有体积$v_i$和价值$w_i$，求当$\\sum_{i&#x3D;1}^{n} v_i \\le v$时最大的$w$是多少</p>\n<h4 id=\"01背包问题\"><a href=\"#01背包问题\" class=\"headerlink\" title=\"01背包问题\"></a>01背包问题</h4><p>每个物品只能用0&#x2F;1次</p>\n<p>$f[i,j] &#x3D; max(f[i-1,j],f[i-1,j-v_i]+w_i)$</p>\n<p><a href=\"https://www.acwing.com/problem/content/2/\">01背包问题</a></p>\n<p><a href=\"https://www.luogu.com.cn/problem/P1048\">采药</a></p>\n<h4 id=\"完全背包问题\"><a href=\"#完全背包问题\" class=\"headerlink\" title=\"完全背包问题\"></a>完全背包问题</h4><p>物品可以无限次使用</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j-v_i \\times k]+w[i] \\times k)$</p>\n<p>$k \\subseteq [0,\\frac{j}{v_i}]$</p>\n<p>即$f[i,j] &#x3D; Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,….,f[i-1][j-kv_i]+kw_i)$</p>\n<p>$f[i,j-v_i] &#x3D; Max(f[i-1][j-v_i],f[i-1][j-2v_i]+w_i,…,f[i-1][j-kv_i]+(k-1)w_i)$</p>\n<p>$f[i][j]$的后$k$项等于$f[i][j-v_i]+w_i$</p>\n<p>得</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j],f[i,j-v_i]+w_i)$</p>\n<p><a href=\"https://www.acwing.com/problem/content/3/\">完全背包问题</a></p>\n<h4 id=\"多重背包物品\"><a href=\"#多重背包物品\" class=\"headerlink\" title=\"多重背包物品\"></a>多重背包物品</h4><p>每个物品的个数不一致</p>\n<p>朴素做法</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,….,f[i-1][j-kv_i]+kw_i)$</p>\n<p>$k \\subseteq [0,s_i]$</p>\n<p>三重循环即可</p>\n<p><a href=\"https://www.acwing.com/problem/content/4/\">多重背包问题1</a></p>\n<p><strong>优化：二进制优化</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a,b,s;</span><br><span class=\"line\">    cin&gt;&gt;a&gt;&gt;b&gt;&gt;s;</span><br><span class=\"line\">    <span class=\"comment\">//v w s;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> k = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(k&lt;=s)&#123;</span><br><span class=\"line\">        cnt++;</span><br><span class=\"line\">        v[cnt] = a*k;</span><br><span class=\"line\">        w[cnt] = b*k;</span><br><span class=\"line\">        s-=k;</span><br><span class=\"line\">        k*=<span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(s&gt;<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        cnt++;</span><br><span class=\"line\">        v[cnt] = a*s;</span><br><span class=\"line\">        w[cnt] = b*s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>对物品进行二进制分组，组数为$cnt$，转化为01背包问题求解</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n = cnt;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;=m;j++)&#123;</span><br><span class=\"line\">        f[i][j] = f[i<span class=\"number\">-1</span>][j];</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j&gt;=v[i]) f[i][j] = <span class=\"built_in\">max</span>(f[i][j],f[i<span class=\"number\">-1</span>][j-v[i]]+w[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">cout&lt;&lt;f[n][m]&lt;&lt;endl;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/5/\">多重背包问题2</a></p>\n<h4 id=\"分组背包问题\"><a href=\"#分组背包问题\" class=\"headerlink\" title=\"分组背包问题\"></a>分组背包问题</h4><p>有$N$组，每一组只能选其中一种物品</p>\n<p>$f[i][j] &#x3D; Max(f[i-1,j],f[i-1,j-v_{i,k}]+w_{i,k})$</p>\n<p><a href=\"https://www.acwing.com/problem/content/9/\">分组背包问题</a></p>\n","more":"<h3 id=\"动态规划\"><a href=\"#动态规划\" class=\"headerlink\" title=\"动态规划\"></a>动态规划</h3><ul>\n<li><p>状态表示</p>\n<p>$f[i,j]$</p>\n<ul>\n<li>集合</li>\n<li>属性（Max，Min，Cnt）</li>\n</ul>\n</li>\n<li><p>状态计算</p>\n<ul>\n<li>集合的划分</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"线性DP\"><a href=\"#线性DP\" class=\"headerlink\" title=\"线性DP\"></a>线性DP</h3><p><strong><a href=\"https://www.luogu.com.cn/problem/P1216\">数字三角形</a></strong></p>\n<p>$O(n^2)$</p>\n<p>$f[i,j]$表示到坐标为$[i,j]$的路径的和最大值</p>\n<p>$f[1][1] &#x3D; a[1][1]$</p>\n<p>$f[i][j] &#x3D; max(f[i-1][j-1]+a[i][j], f[i-1][j]+a[i][j])$</p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/B3637\">最长上升子序列</a></strong></p>\n<p><strong>朴素</strong>：$O(n^2)$</p>\n<p>$f[i]$表示以第$i$个数结尾的序列中上升子序列长度的最大值</p>\n<p>遍历$a_i$所有可能的前一个数$a_j$($a_j&lt;a_i$且$0 \\le j \\le i-1$)</p>\n<p>$f[i] &#x3D; max(f[j]+1,f[i]),j \\in [0,i-1]$</p>\n<p>如果要保存最长序列：$g[i]$保存从哪一步$j$转移过来</p>\n<p>代码：<a href=\"https://www.luogu.com.cn/record/124595657\">https://www.luogu.com.cn/record/124595657</a></p>\n<p><strong>优化</strong>：$O(nlogn)$</p>\n<p>用一个q数组储存长度为i的序列的结尾数字的最小值</p>\n<p>可以证明$q_i&gt;q_{i-1}&gt;…&gt;q_2&gt;q_1$，即数组严格单调递增</p>\n<p>对于$a_i$，二分找到最大的$q_k&lt;&#x3D;a_i$，$f[i] &#x3D; k+1$，更新$q_k &#x3D; a_i$</p>\n<p>代码：<a href=\"https://www.luogu.com.cn/record/133704642\">https://www.luogu.com.cn/record/133704642</a></p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/P1439\">最长公共子序列</a></strong></p>\n<p><strong>朴素</strong>：$O(n^2)$</p>\n<p>$f[i][j]$表示所有在第一个序列的前i个字母中出现，且在第二个序列的前j个字母中出现的子序列的最大值</p>\n<p>集合划分依据：是否选择$a[i],b[j]$</p>\n<p>分为四个集合：选择$a[i],b[j]$ ; 选择$a[i]$ 不选择$b[j]$ ; 不选择$a[j]$选择$b[j]$ ; 都不选择$a[i],b[j]$</p>\n<p>分别表示为 $f[i-1][j-1] , f[i,j-1] , f[i-1][j] , f[i-1][j-1]+1$</p>\n<p>其中第二三种情况<strong>包含</strong>上面对应的集合（由于是求Max，所以有重复不影响结果）</p>\n<p>且第二三种集合也包含第一个集合，所以只要对后三种集合求最大值即可</p>\n<p>$f[i,j] &#x3D; max(f[i-1,j],f[i,j-1])$</p>\n<p>当$a[i]&#x3D;&#x3D;b[j]$时,$f[i,j] &#x3D; max(f[i,j],f[i-1,j-1]+1)$</p>\n<p><strong>优化</strong>：$O(nlogn)$</p>\n<p><strong><a href=\"https://www.luogu.com.cn/problem/P2758\">编辑距离</a></strong></p>\n<p>$f[i,j]$所有将$a[1-i]$变成$b[1-j]$的操作方式的最小步数</p>\n<p>区间划分，①删除最后一个数、②增加最后一个数、③修改最后一个数</p>\n<p>① $f[i-1,j]+1$</p>\n<p>②$f[i,j-1]+1$</p>\n<p>③$f[i-1,j-1]+1$ （如果$a[i]&#x3D;&#x3D;b[j]$则不需要加一，即不需要进行修改操作）</p>\n<h3 id=\"区间DP\"><a href=\"#区间DP\" class=\"headerlink\" title=\"区间DP\"></a>区间DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/284/\">石子合并</a></strong></p>\n<p>$f[i,j]$表示将第$i$堆石子到第$j$堆石子合并成一堆石子的方式的代价最小值&#x2F;最大值</p>\n<p>$O(n^3)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> len=<span class=\"number\">2</span>;len&lt;=n;len++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i+len<span class=\"number\">-1</span>&lt;=n;i++)&#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> l = i,r = i+len<span class=\"number\">-1</span>;</span><br><span class=\"line\">        f_max[l][r] = <span class=\"number\">-1e8</span>,f_min[l][r] = <span class=\"number\">1e8</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span>(<span class=\"type\">int</span> k=l;k&lt;r;k++)&#123;</span><br><span class=\"line\">            f_max[l][r] = <span class=\"built_in\">max</span>(f_max[l][r],f_max[l][k]+f_max[k<span class=\"number\">+1</span>][r]+s[r]-s[l<span class=\"number\">-1</span>]);</span><br><span class=\"line\">            f_min[l][r] = <span class=\"built_in\">min</span>(f_min[l][r],f_min[l][k]+f_min[k<span class=\"number\">+1</span>][r]+s[r]-s[l<span class=\"number\">-1</span>]);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"计数类DP\"><a href=\"#计数类DP\" class=\"headerlink\" title=\"计数类DP\"></a>计数类DP</h3><h3 id=\"数位统计DP\"><a href=\"#数位统计DP\" class=\"headerlink\" title=\"数位统计DP\"></a>数位统计DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/340/\">计数问题</a></strong></p>\n<p>设$n&#x3D;abcdefg$，枚举第$i$位是 $x \\in [0,9]$</p>\n<p>举例$x&#x3D;1,i&#x3D;4$的情况：</p>\n<p>设数字为$xxx1yyy$</p>\n<ul>\n<li><p>当$abc&gt;xxx,xxx \\in [000,abc-1], y \\in [000,999]$，则共有$abc * 1000$个</p>\n</li>\n<li><p>当$abc&lt;xxx$，则共有0个</p>\n</li>\n<li><p>当</p>\n<p>$abc&#x3D;xxx$</p>\n<ul>\n<li>当$d&lt;1$，无解</li>\n<li>当$d&#x3D;1$，$yyy \\in [000,efg]$,则有$efg+1$种</li>\n<li>当$d&gt;1$，$yyy \\in [000,999]$,有1000种</li>\n</ul>\n</li>\n</ul>\n<p>**当x&#x3D;0时，注意前导0，即对于第一种情况，$xxx \\in [001,abc-1]$**，即有$(abc-1)*1000$情况</p>\n<p><strong><a href=\"https://www.acwing.com/problem/content/341/\">圆形数字</a></strong></p>\n<h3 id=\"状态压缩DP\"><a href=\"#状态压缩DP\" class=\"headerlink\" title=\"状态压缩DP\"></a>状态压缩DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/293/\">蒙德里安的梦想</a></strong></p>\n<p>$f[i][j]$表示第i列，上一列横着摆的数量j,其中j是一个二进制数。</p>\n<p><strong><a href=\"https://www.acwing.com/problem/content/93/\">最短Hamilton路径</a></strong></p>\n<p>$f[i][j]$表示从0号点走到j号点，走过的所有点是i的所有路径(二进制数i表示某个点是否已经走过了)的最小路径长度</p>\n<h3 id=\"树形DP\"><a href=\"#树形DP\" class=\"headerlink\" title=\"树形DP\"></a>树形DP</h3><p><strong><a href=\"https://www.acwing.com/problem/content/287/\">没有上司的舞会</a></strong></p>\n<p>$f[u][0]$表示所有以u为根的子树中选择，并且不选u这个点的方案的最大值</p>\n<p>$f[u][1]$表示所有以u为根的子树中选择，并且选u这个点的方案的最大值</p>\n<p>设点u的子节点$s_1,s_2,s_3….s_i$</p>\n<p>$f[u][0] &#x3D; \\sum_{1}^{i}max(f[s_i][0],f[s_i][1])$</p>\n<p>$f[u][1] &#x3D; \\sum_{1}^{i}f[s_i][0]$</p>\n<p>找出根节点，递归求最大值即可</p>\n<h3 id=\"记忆化搜索\"><a href=\"#记忆化搜索\" class=\"headerlink\" title=\"记忆化搜索\"></a>记忆化搜索</h3><p><strong><a href=\"https://www.luogu.com.cn/problem/P1434\">滑雪</a></strong></p>\n<p>用$s[i][j]$表示从(i,j)点出发能走的最长距离。</p>\n<p>每次搜索一次记忆一次即可。</p>\n<p>举例</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">3 3 </span><br><span class=\"line\">1 1 3</span><br><span class=\"line\">2 3 4</span><br><span class=\"line\">1 1 1</span><br></pre></td></tr></table></figure>\n\n<p>先去找(1,1)的最长距离，很明显为1</p>\n<p>接着找(1,2)的最长距离，很明显为1</p>\n<p>接着找(1,3)的最长距离，为2((1,3)-&gt;(1,2))</p>\n<p>然后找(2,1)的最长距离，为2((2,1)-&gt;(1,1))</p>\n<p>然后是(2,2)的最长距离，如果没有记忆化，那么搜索过程为：(2,2)-&gt;(2,1)-&gt;(1,1)</p>\n<p>但是（2,1）之前已经搜过了，再去搜就是浪费时间，之前搜索已经知道(2,1)的值为2，那么搜索过程就是缩短为：(2,2)-&gt;(2,1),即为3</p>\n<h3 id=\"背包问题\"><a href=\"#背包问题\" class=\"headerlink\" title=\"背包问题\"></a>背包问题</h3><p>给定$n$个物品和容量$v$的背包，每个物品都有体积$v_i$和价值$w_i$，求当$\\sum_{i&#x3D;1}^{n} v_i \\le v$时最大的$w$是多少</p>\n<h4 id=\"01背包问题\"><a href=\"#01背包问题\" class=\"headerlink\" title=\"01背包问题\"></a>01背包问题</h4><p>每个物品只能用0&#x2F;1次</p>\n<p>$f[i,j] &#x3D; max(f[i-1,j],f[i-1,j-v_i]+w_i)$</p>\n<p><a href=\"https://www.acwing.com/problem/content/2/\">01背包问题</a></p>\n<p><a href=\"https://www.luogu.com.cn/problem/P1048\">采药</a></p>\n<h4 id=\"完全背包问题\"><a href=\"#完全背包问题\" class=\"headerlink\" title=\"完全背包问题\"></a>完全背包问题</h4><p>物品可以无限次使用</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j-v_i \\times k]+w[i] \\times k)$</p>\n<p>$k \\subseteq [0,\\frac{j}{v_i}]$</p>\n<p>即$f[i,j] &#x3D; Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,….,f[i-1][j-kv_i]+kw_i)$</p>\n<p>$f[i,j-v_i] &#x3D; Max(f[i-1][j-v_i],f[i-1][j-2v_i]+w_i,…,f[i-1][j-kv_i]+(k-1)w_i)$</p>\n<p>$f[i][j]$的后$k$项等于$f[i][j-v_i]+w_i$</p>\n<p>得</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j],f[i,j-v_i]+w_i)$</p>\n<p><a href=\"https://www.acwing.com/problem/content/3/\">完全背包问题</a></p>\n<h4 id=\"多重背包物品\"><a href=\"#多重背包物品\" class=\"headerlink\" title=\"多重背包物品\"></a>多重背包物品</h4><p>每个物品的个数不一致</p>\n<p>朴素做法</p>\n<p>$f[i,j] &#x3D; Max(f[i-1,j],f[i-1,j-v_i]+w_i,f[i-1,j-2v_i]+2w_i,….,f[i-1][j-kv_i]+kw_i)$</p>\n<p>$k \\subseteq [0,s_i]$</p>\n<p>三重循环即可</p>\n<p><a href=\"https://www.acwing.com/problem/content/4/\">多重背包问题1</a></p>\n<p><strong>优化：二进制优化</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a,b,s;</span><br><span class=\"line\">    cin&gt;&gt;a&gt;&gt;b&gt;&gt;s;</span><br><span class=\"line\">    <span class=\"comment\">//v w s;</span></span><br><span class=\"line\">    <span class=\"type\">int</span> k = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span>(k&lt;=s)&#123;</span><br><span class=\"line\">        cnt++;</span><br><span class=\"line\">        v[cnt] = a*k;</span><br><span class=\"line\">        w[cnt] = b*k;</span><br><span class=\"line\">        s-=k;</span><br><span class=\"line\">        k*=<span class=\"number\">2</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span>(s&gt;<span class=\"number\">0</span>)&#123;</span><br><span class=\"line\">        cnt++;</span><br><span class=\"line\">        v[cnt] = a*s;</span><br><span class=\"line\">        w[cnt] = b*s;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>对物品进行二进制分组，组数为$cnt$，转化为01背包问题求解</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">n = cnt;</span><br><span class=\"line\"><span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=<span class=\"number\">1</span>;i&lt;=n;i++)&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> j=<span class=\"number\">0</span>;j&lt;=m;j++)&#123;</span><br><span class=\"line\">        f[i][j] = f[i<span class=\"number\">-1</span>][j];</span><br><span class=\"line\">        <span class=\"keyword\">if</span>(j&gt;=v[i]) f[i][j] = <span class=\"built_in\">max</span>(f[i][j],f[i<span class=\"number\">-1</span>][j-v[i]]+w[i]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">cout&lt;&lt;f[n][m]&lt;&lt;endl;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/5/\">多重背包问题2</a></p>\n<h4 id=\"分组背包问题\"><a href=\"#分组背包问题\" class=\"headerlink\" title=\"分组背包问题\"></a>分组背包问题</h4><p>有$N$组，每一组只能选其中一种物品</p>\n<p>$f[i][j] &#x3D; Max(f[i-1,j],f[i-1,j-v_{i,k}]+w_{i,k})$</p>\n<p><a href=\"https://www.acwing.com/problem/content/9/\">分组背包问题</a></p>\n"},{"title":"Algorithm-Graph","mathjax":true,"date":"2023-08-05T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛中的图论，包括最小生成树和最短路问题","_content":"\n### 最短路\n\n$n$为点数，$m$为边数\n\n若$m$与$n^2$同一级别为稠密图，与$n$同一级别为稀疏图\n\n稠密图使用邻接矩阵储存，稀疏图用邻接表储存\n\n- 单源最短路\n  - 所有边权都是正数\n    - 朴素$dijkstra$算法 $O(n^2+m)$\n    - 堆优化版$dijkstra$算法 $O(mlogn)$\n  - 存在负权边\n    - $Bellman-Ford$算法 $O(nm)$\n    - $SPFA$算法 一般$O(m)$，最坏$O(nm)$\n- 多源汇最短路\n  - $floyd$算法 $O(n^3)$\n\n#### 朴素dijkstra算法\n\n1. 初始化距离，$dist[1]=0,dist[i]=+\\infty $，st数组：当前已经确定最短路径的点\n2. 循环每一个点，找到不在st中的最短距离点t，t加入到st中，用t更新其他点的距离\n\n```cpp\nint g[N][N];  // 存储每条边\nint dist[N];  // 存储1号点到每个点的最短距离\nbool st[N];   // 存储每个点的最短路是否已经确定\n\n// 求1号点到n号点的最短路，如果不存在则返回-1\nint dijkstra()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    for (int i = 0; i < n - 1; i ++ )\n    {\n        int t = -1;     // 在还未确定最短路的点中，寻找距离最小的点\n        for (int j = 1; j <= n; j ++ )\n            if (!st[j] && (t == -1 || dist[t] > dist[j]))\n                t = j;\n\n        // 用t更新其他点的距离\n        for (int j = 1; j <= n; j ++ )\n            dist[j] = min(dist[j], dist[t] + g[t][j]);\n\n        st[t] = true;\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n一般边数m比较多，所以使用邻接矩阵`g[a][b]`存储\n\n#### 堆优化版dijkstra算法\n\n将寻找距离最小的点的时间复杂度降低\n\n堆可以使用手写堆或**优先队列**\n\n```cpp\ntypedef pair<int, int> PII;\n\nint n;      // 点的数量\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N];        // 存储所有点到1号点的距离\nbool st[N];     // 存储每个点的最短距离是否已确定\nvoid add(int a, int b, int c){\n    e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++;\n}\n// 求1号点到n号点的最短距离，如果不存在，则返回-1\nint dijkstra()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n    priority_queue<PII, vector<PII>, greater<PII>> heap;\n    heap.push({0, 1});      // first存储距离，second存储节点编号\n\n    while (heap.size())\n    {\n        auto t = heap.top();\n        heap.pop();\n\n        int ver = t.second, distance = t.first;\n\n        if (st[ver]) continue;\n        st[ver] = true;\n\n        for (int i = h[ver]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > distance + w[i])\n            {\n                dist[j] = distance + w[i];\n                heap.push({dist[j], j});\n            }\n        }\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n#### Bellman-Ford算法\n\n迭代n次，每次遍历所有边，对dist[b]进行更新\n\n```cpp\nint n, m;       // n表示点数，m表示边数\nint dist[N];        // dist[x]存储1到x的最短路距离\n\nstruct Edge     // 边，a表示出点，b表示入点，w表示边的权重\n{\n    int a, b, w;\n}edges[M];\n\n// 求1到n的最短路距离，如果无法从1走到n，则返回-1。\nint bellman_ford()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    // 如果第n次迭代仍然会松弛三角不等式，就说明存在一条长度是n+1的最短路径，由抽屉原理，路径中至少存在两个相同的点，说明图中存在负权回路。\n    for (int i = 0; i < n; i ++ )\n    {\n        for (int j = 0; j < m; j ++ )\n        {\n            int a = edges[j].a, b = edges[j].b, w = edges[j].w;\n            if (dist[b] > dist[a] + w)\n                dist[b] = dist[a] + w;\n        }\n    }\n\n    if (dist[n] > 0x3f3f3f3f / 2) return -1;\n    return dist[n];\n}\n//遍历完后满足三角不等式\ndist[b] <= dist[a] + w\n```\n\n可以用于找负环，时间复杂度比较高\n\n#### SPFA算法\n\n队列优化的Bellman-Ford算法\n\n```cpp\nint n;      // 总点数\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N];        // 存储每个点到1号点的最短距离\nbool st[N];     // 存储每个点是否在队列中\n\n// 求1号点到n号点的最短路距离，如果从1号点无法走到n号点则返回-1\nint spfa()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    queue<int> q;\n    q.push(1);\n    st[1] = true;\n\n    while (q.size())\n    {\n        auto t = q.front();\n        q.pop();\n\n        st[t] = false;\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > dist[t] + w[i])\n            {\n                dist[j] = dist[t] + w[i];\n                if (!st[j])     // 如果队列中已存在j，则不需要将j重复插入\n                {\n                    q.push(j);\n                    st[j] = true;\n                }\n            }\n        }\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n推荐使用，只要不被卡或者存在负环\n\n**判断负环**\n\n在进行更新dist[j] = dist[t] + w时，同时维护cnt[x] (1号点到x号点经过的边数)\n\ncnt[x] = cnt[t]+1;\n\n若某个cnt[x] 大于等于n，则说明存在负环\n\n```cpp\nint n;      // 总点数\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N], cnt[N];        // dist[x]存储1号点到x的最短距离，cnt[x]存储1到x的最短路中经过的点数\nbool st[N];     // 存储每个点是否在队列中\n\n// 如果存在负环，则返回true，否则返回false。\nbool spfa()\n{\n    // 不需要初始化dist数组\n    // 原理：如果某条最短路径上有n个点（除了自己），那么加上自己之后一共有n+1个点，由抽屉原理一定有两个点相同，所以存在环。\n\n    queue<int> q;\n    for (int i = 1; i <= n; i ++ )\n    {\n        q.push(i);\n        st[i] = true;\n    }\n\n    while (q.size())\n    {\n        auto t = q.front();\n        q.pop();\n\n        st[t] = false;\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > dist[t] + w[i])\n            {\n                dist[j] = dist[t] + w[i];\n                cnt[j] = cnt[t] + 1;\n                if (cnt[j] >= n) return true;       // 如果从1号点到x的最短路中包含至少n个点（不包括自己），则说明存在环\n                if (!st[j])\n                {\n                    q.push(j);\n                    st[j] = true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```\n\n#### Floyd算法\n\n基于动态规划\n\n```cpp\n初始化：\n    for (int i = 1; i <= n; i ++ )\n        for (int j = 1; j <= n; j ++ )\n            if (i == j) d[i][j] = 0;\n            else d[i][j] = INF;\n\n// 算法结束后，d[a][b]表示a到b的最短距离\nvoid floyd()\n{\n    for (int k = 1; k <= n; k ++ )\n        for (int i = 1; i <= n; i ++ )\n            for (int j = 1; j <= n; j ++ )\n                d[i][j] = min(d[i][j], d[i][k] + d[k][j]);\n}\n```\n\n### 最小生成树\n\n#### Prim算法\n\n处理稠密图\n\n**朴素Prim算法** $O(n^2)$\n\n类似dijkstra，找出距离集合最短的点，加入集合，更新其他点到集合的距离\n\n```cpp\nint n;      // n表示点数\nint g[N][N];        // 邻接矩阵，存储所有边\nint dist[N];        // 存储其他点到当前最小生成树的距离\nbool st[N];     // 存储每个点是否已经在生成树中\n\n\n// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和\nint prim()\n{\n    memset(dist, 0x3f, sizeof dist);\n\n    int res = 0;\n    for (int i = 0; i < n; i ++ )\n    {\n        int t = -1;\n        for (int j = 1; j <= n; j ++ )\n            if (!st[j] && (t == -1 || dist[t] > dist[j]))\n                t = j;\n\n        if (i && dist[t] == INF) return INF;\n\n        if (i) res += dist[t];\n        st[t] = true;\n\n        for (int j = 1; j <= n; j ++ ) dist[j] = min(dist[j], g[t][j]);\n    }\n\n    return res;\n}\n```\n\n**堆优化Prim** $O(mlogn)$ （不常用）\n\n#### Kruskal算法\n\n$O(mlogm)$ 处理稀疏图\n\n1. 将所有边按照权重排序\n2. 枚举每条边a->b,权重c （如果ab不联通，则将边ab加入集合中）\n\n```cpp\nint n, m;       // n是点数，m是边数\nint p[N];       // 并查集的父节点数组\n\nstruct Edge     // 存储边\n{\n    int a, b, w;\n\n    bool operator< (const Edge &W)const\n    {\n        return w < W.w;\n    }\n}edges[M];\n\nint find(int x)     // 并查集核心操作\n{\n    if (p[x] != x) p[x] = find(p[x]);\n    return p[x];\n}\n\nint kruskal()\n{\n    sort(edges, edges + m);\n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;    // 初始化并查集\n\n    int res = 0, cnt = 0;\n    for (int i = 0; i < m; i ++ )\n    {\n        int a = edges[i].a, b = edges[i].b, w = edges[i].w;\n\n        a = find(a), b = find(b);\n        if (a != b)     // 如果两个连通块不连通，则将这两个连通块合并\n        {\n            p[a] = b;\n            res += w;\n            cnt ++ ;\n        }\n    }\n\n    if (cnt < n - 1) return INF;\n    return res;\n}\n```","source":"_posts/algorithm-graph.md","raw":"---\ntitle: Algorithm-Graph\nmathjax: true\ndate: 2023/08/05 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛中的图论，包括最小生成树和最短路问题\n---\n\n### 最短路\n\n$n$为点数，$m$为边数\n\n若$m$与$n^2$同一级别为稠密图，与$n$同一级别为稀疏图\n\n稠密图使用邻接矩阵储存，稀疏图用邻接表储存\n\n- 单源最短路\n  - 所有边权都是正数\n    - 朴素$dijkstra$算法 $O(n^2+m)$\n    - 堆优化版$dijkstra$算法 $O(mlogn)$\n  - 存在负权边\n    - $Bellman-Ford$算法 $O(nm)$\n    - $SPFA$算法 一般$O(m)$，最坏$O(nm)$\n- 多源汇最短路\n  - $floyd$算法 $O(n^3)$\n\n#### 朴素dijkstra算法\n\n1. 初始化距离，$dist[1]=0,dist[i]=+\\infty $，st数组：当前已经确定最短路径的点\n2. 循环每一个点，找到不在st中的最短距离点t，t加入到st中，用t更新其他点的距离\n\n```cpp\nint g[N][N];  // 存储每条边\nint dist[N];  // 存储1号点到每个点的最短距离\nbool st[N];   // 存储每个点的最短路是否已经确定\n\n// 求1号点到n号点的最短路，如果不存在则返回-1\nint dijkstra()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    for (int i = 0; i < n - 1; i ++ )\n    {\n        int t = -1;     // 在还未确定最短路的点中，寻找距离最小的点\n        for (int j = 1; j <= n; j ++ )\n            if (!st[j] && (t == -1 || dist[t] > dist[j]))\n                t = j;\n\n        // 用t更新其他点的距离\n        for (int j = 1; j <= n; j ++ )\n            dist[j] = min(dist[j], dist[t] + g[t][j]);\n\n        st[t] = true;\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n一般边数m比较多，所以使用邻接矩阵`g[a][b]`存储\n\n#### 堆优化版dijkstra算法\n\n将寻找距离最小的点的时间复杂度降低\n\n堆可以使用手写堆或**优先队列**\n\n```cpp\ntypedef pair<int, int> PII;\n\nint n;      // 点的数量\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N];        // 存储所有点到1号点的距离\nbool st[N];     // 存储每个点的最短距离是否已确定\nvoid add(int a, int b, int c){\n    e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++;\n}\n// 求1号点到n号点的最短距离，如果不存在，则返回-1\nint dijkstra()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n    priority_queue<PII, vector<PII>, greater<PII>> heap;\n    heap.push({0, 1});      // first存储距离，second存储节点编号\n\n    while (heap.size())\n    {\n        auto t = heap.top();\n        heap.pop();\n\n        int ver = t.second, distance = t.first;\n\n        if (st[ver]) continue;\n        st[ver] = true;\n\n        for (int i = h[ver]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > distance + w[i])\n            {\n                dist[j] = distance + w[i];\n                heap.push({dist[j], j});\n            }\n        }\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n#### Bellman-Ford算法\n\n迭代n次，每次遍历所有边，对dist[b]进行更新\n\n```cpp\nint n, m;       // n表示点数，m表示边数\nint dist[N];        // dist[x]存储1到x的最短路距离\n\nstruct Edge     // 边，a表示出点，b表示入点，w表示边的权重\n{\n    int a, b, w;\n}edges[M];\n\n// 求1到n的最短路距离，如果无法从1走到n，则返回-1。\nint bellman_ford()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    // 如果第n次迭代仍然会松弛三角不等式，就说明存在一条长度是n+1的最短路径，由抽屉原理，路径中至少存在两个相同的点，说明图中存在负权回路。\n    for (int i = 0; i < n; i ++ )\n    {\n        for (int j = 0; j < m; j ++ )\n        {\n            int a = edges[j].a, b = edges[j].b, w = edges[j].w;\n            if (dist[b] > dist[a] + w)\n                dist[b] = dist[a] + w;\n        }\n    }\n\n    if (dist[n] > 0x3f3f3f3f / 2) return -1;\n    return dist[n];\n}\n//遍历完后满足三角不等式\ndist[b] <= dist[a] + w\n```\n\n可以用于找负环，时间复杂度比较高\n\n#### SPFA算法\n\n队列优化的Bellman-Ford算法\n\n```cpp\nint n;      // 总点数\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N];        // 存储每个点到1号点的最短距离\nbool st[N];     // 存储每个点是否在队列中\n\n// 求1号点到n号点的最短路距离，如果从1号点无法走到n号点则返回-1\nint spfa()\n{\n    memset(dist, 0x3f, sizeof dist);\n    dist[1] = 0;\n\n    queue<int> q;\n    q.push(1);\n    st[1] = true;\n\n    while (q.size())\n    {\n        auto t = q.front();\n        q.pop();\n\n        st[t] = false;\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > dist[t] + w[i])\n            {\n                dist[j] = dist[t] + w[i];\n                if (!st[j])     // 如果队列中已存在j，则不需要将j重复插入\n                {\n                    q.push(j);\n                    st[j] = true;\n                }\n            }\n        }\n    }\n\n    if (dist[n] == 0x3f3f3f3f) return -1;\n    return dist[n];\n}\n```\n\n推荐使用，只要不被卡或者存在负环\n\n**判断负环**\n\n在进行更新dist[j] = dist[t] + w时，同时维护cnt[x] (1号点到x号点经过的边数)\n\ncnt[x] = cnt[t]+1;\n\n若某个cnt[x] 大于等于n，则说明存在负环\n\n```cpp\nint n;      // 总点数\nint h[N], w[N], e[N], ne[N], idx;       // 邻接表存储所有边\nint dist[N], cnt[N];        // dist[x]存储1号点到x的最短距离，cnt[x]存储1到x的最短路中经过的点数\nbool st[N];     // 存储每个点是否在队列中\n\n// 如果存在负环，则返回true，否则返回false。\nbool spfa()\n{\n    // 不需要初始化dist数组\n    // 原理：如果某条最短路径上有n个点（除了自己），那么加上自己之后一共有n+1个点，由抽屉原理一定有两个点相同，所以存在环。\n\n    queue<int> q;\n    for (int i = 1; i <= n; i ++ )\n    {\n        q.push(i);\n        st[i] = true;\n    }\n\n    while (q.size())\n    {\n        auto t = q.front();\n        q.pop();\n\n        st[t] = false;\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            if (dist[j] > dist[t] + w[i])\n            {\n                dist[j] = dist[t] + w[i];\n                cnt[j] = cnt[t] + 1;\n                if (cnt[j] >= n) return true;       // 如果从1号点到x的最短路中包含至少n个点（不包括自己），则说明存在环\n                if (!st[j])\n                {\n                    q.push(j);\n                    st[j] = true;\n                }\n            }\n        }\n    }\n\n    return false;\n}\n```\n\n#### Floyd算法\n\n基于动态规划\n\n```cpp\n初始化：\n    for (int i = 1; i <= n; i ++ )\n        for (int j = 1; j <= n; j ++ )\n            if (i == j) d[i][j] = 0;\n            else d[i][j] = INF;\n\n// 算法结束后，d[a][b]表示a到b的最短距离\nvoid floyd()\n{\n    for (int k = 1; k <= n; k ++ )\n        for (int i = 1; i <= n; i ++ )\n            for (int j = 1; j <= n; j ++ )\n                d[i][j] = min(d[i][j], d[i][k] + d[k][j]);\n}\n```\n\n### 最小生成树\n\n#### Prim算法\n\n处理稠密图\n\n**朴素Prim算法** $O(n^2)$\n\n类似dijkstra，找出距离集合最短的点，加入集合，更新其他点到集合的距离\n\n```cpp\nint n;      // n表示点数\nint g[N][N];        // 邻接矩阵，存储所有边\nint dist[N];        // 存储其他点到当前最小生成树的距离\nbool st[N];     // 存储每个点是否已经在生成树中\n\n\n// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和\nint prim()\n{\n    memset(dist, 0x3f, sizeof dist);\n\n    int res = 0;\n    for (int i = 0; i < n; i ++ )\n    {\n        int t = -1;\n        for (int j = 1; j <= n; j ++ )\n            if (!st[j] && (t == -1 || dist[t] > dist[j]))\n                t = j;\n\n        if (i && dist[t] == INF) return INF;\n\n        if (i) res += dist[t];\n        st[t] = true;\n\n        for (int j = 1; j <= n; j ++ ) dist[j] = min(dist[j], g[t][j]);\n    }\n\n    return res;\n}\n```\n\n**堆优化Prim** $O(mlogn)$ （不常用）\n\n#### Kruskal算法\n\n$O(mlogm)$ 处理稀疏图\n\n1. 将所有边按照权重排序\n2. 枚举每条边a->b,权重c （如果ab不联通，则将边ab加入集合中）\n\n```cpp\nint n, m;       // n是点数，m是边数\nint p[N];       // 并查集的父节点数组\n\nstruct Edge     // 存储边\n{\n    int a, b, w;\n\n    bool operator< (const Edge &W)const\n    {\n        return w < W.w;\n    }\n}edges[M];\n\nint find(int x)     // 并查集核心操作\n{\n    if (p[x] != x) p[x] = find(p[x]);\n    return p[x];\n}\n\nint kruskal()\n{\n    sort(edges, edges + m);\n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;    // 初始化并查集\n\n    int res = 0, cnt = 0;\n    for (int i = 0; i < m; i ++ )\n    {\n        int a = edges[i].a, b = edges[i].b, w = edges[i].w;\n\n        a = find(a), b = find(b);\n        if (a != b)     // 如果两个连通块不连通，则将这两个连通块合并\n        {\n            p[a] = b;\n            res += w;\n            cnt ++ ;\n        }\n    }\n\n    if (cnt < n - 1) return INF;\n    return res;\n}\n```","slug":"algorithm-graph","published":1,"updated":"2025-02-28T03:16:48.846Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9d0006bs99ec1h5ebg","content":"<h3 id=\"最短路\"><a href=\"#最短路\" class=\"headerlink\" title=\"最短路\"></a>最短路</h3><p>$n$为点数，$m$为边数</p>\n<p>若$m$与$n^2$同一级别为稠密图，与$n$同一级别为稀疏图</p>\n<p>稠密图使用邻接矩阵储存，稀疏图用邻接表储存</p>\n<ul>\n<li>单源最短路<ul>\n<li>所有边权都是正数<ul>\n<li>朴素$dijkstra$算法 $O(n^2+m)$</li>\n<li>堆优化版$dijkstra$算法 $O(mlogn)$</li>\n</ul>\n</li>\n<li>存在负权边<ul>\n<li>$Bellman-Ford$算法 $O(nm)$</li>\n<li>$SPFA$算法 一般$O(m)$，最坏$O(nm)$</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>多源汇最短路<ul>\n<li>$floyd$算法 $O(n^3)$</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"朴素dijkstra算法\"><a href=\"#朴素dijkstra算法\" class=\"headerlink\" title=\"朴素dijkstra算法\"></a>朴素dijkstra算法</h4><ol>\n<li>初始化距离，$dist[1]&#x3D;0,dist[i]&#x3D;+\\infty $，st数组：当前已经确定最短路径的点</li>\n<li>循环每一个点，找到不在st中的最短距离点t，t加入到st中，用t更新其他点的距离</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> g[N][N];  <span class=\"comment\">// 存储每条边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];  <span class=\"comment\">// 存储1号点到每个点的最短距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];   <span class=\"comment\">// 存储每个点的最短路是否已经确定</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短路，如果不存在则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dijkstra</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n - <span class=\"number\">1</span>; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = <span class=\"number\">-1</span>;     <span class=\"comment\">// 在还未确定最短路的点中，寻找距离最小的点</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!st[j] &amp;&amp; (t == <span class=\"number\">-1</span> || dist[t] &gt; dist[j]))</span><br><span class=\"line\">                t = j;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 用t更新其他点的距离</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            dist[j] = <span class=\"built_in\">min</span>(dist[j], dist[t] + g[t][j]);</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>一般边数m比较多，所以使用邻接矩阵<code>g[a][b]</code>存储</p>\n<h4 id=\"堆优化版dijkstra算法\"><a href=\"#堆优化版dijkstra算法\" class=\"headerlink\" title=\"堆优化版dijkstra算法\"></a>堆优化版dijkstra算法</h4><p>将寻找距离最小的点的时间复杂度降低</p>\n<p>堆可以使用手写堆或<strong>优先队列</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; PII;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 点的数量</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储所有点到1号点的距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点的最短距离是否已确定</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> c)</span></span>&#123;</span><br><span class=\"line\">    e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短距离，如果不存在，则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dijkstra</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap;</span><br><span class=\"line\">    heap.<span class=\"built_in\">push</span>(&#123;<span class=\"number\">0</span>, <span class=\"number\">1</span>&#125;);      <span class=\"comment\">// first存储距离，second存储节点编号</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (heap.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = heap.<span class=\"built_in\">top</span>();</span><br><span class=\"line\">        heap.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> ver = t.second, distance = t.first;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (st[ver]) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        st[ver] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[ver]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; distance + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = distance + w[i];</span><br><span class=\"line\">                heap.<span class=\"built_in\">push</span>(&#123;dist[j], j&#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Bellman-Ford算法\"><a href=\"#Bellman-Ford算法\" class=\"headerlink\" title=\"Bellman-Ford算法\"></a>Bellman-Ford算法</h4><p>迭代n次，每次遍历所有边，对dist[b]进行更新</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n, m;       <span class=\"comment\">// n表示点数，m表示边数</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// dist[x]存储1到x的最短路距离</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Edge</span>     <span class=\"comment\">// 边，a表示出点，b表示入点，w表示边的权重</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a, b, w;</span><br><span class=\"line\">&#125;edges[M];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1到n的最短路距离，如果无法从1走到n，则返回-1。</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bellman_ford</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果第n次迭代仍然会松弛三角不等式，就说明存在一条长度是n+1的最短路径，由抽屉原理，路径中至少存在两个相同的点，说明图中存在负权回路。</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; m; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> a = edges[j].a, b = edges[j].b, w = edges[j].w;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[b] &gt; dist[a] + w)</span><br><span class=\"line\">                dist[b] = dist[a] + w;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] &gt; <span class=\"number\">0x3f3f3f3f</span> / <span class=\"number\">2</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//遍历完后满足三角不等式</span></span><br><span class=\"line\">dist[b] &lt;= dist[a] + w</span><br></pre></td></tr></table></figure>\n\n<p>可以用于找负环，时间复杂度比较高</p>\n<h4 id=\"SPFA算法\"><a href=\"#SPFA算法\" class=\"headerlink\" title=\"SPFA算法\"></a>SPFA算法</h4><p>队列优化的Bellman-Ford算法</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 总点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储每个点到1号点的最短距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否在队列中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短路距离，如果从1号点无法走到n号点则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">spfa</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">    q.<span class=\"built_in\">push</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">    st[<span class=\"number\">1</span>] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">        q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; dist[t] + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = dist[t] + w[i];</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!st[j])     <span class=\"comment\">// 如果队列中已存在j，则不需要将j重复插入</span></span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">                    st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>推荐使用，只要不被卡或者存在负环</p>\n<p><strong>判断负环</strong></p>\n<p>在进行更新dist[j] &#x3D; dist[t] + w时，同时维护cnt[x] (1号点到x号点经过的边数)</p>\n<p>cnt[x] &#x3D; cnt[t]+1;</p>\n<p>若某个cnt[x] 大于等于n，则说明存在负环</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 总点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N], cnt[N];        <span class=\"comment\">// dist[x]存储1号点到x的最短距离，cnt[x]存储1到x的最短路中经过的点数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否在队列中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果存在负环，则返回true，否则返回false。</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">spfa</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 不需要初始化dist数组</span></span><br><span class=\"line\">    <span class=\"comment\">// 原理：如果某条最短路径上有n个点（除了自己），那么加上自己之后一共有n+1个点，由抽屉原理一定有两个点相同，所以存在环。</span></span><br><span class=\"line\"></span><br><span class=\"line\">    queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        q.<span class=\"built_in\">push</span>(i);</span><br><span class=\"line\">        st[i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">        q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; dist[t] + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = dist[t] + w[i];</span><br><span class=\"line\">                cnt[j] = cnt[t] + <span class=\"number\">1</span>;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (cnt[j] &gt;= n) <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;       <span class=\"comment\">// 如果从1号点到x的最短路中包含至少n个点（不包括自己），则说明存在环</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!st[j])</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">                    st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Floyd算法\"><a href=\"#Floyd算法\" class=\"headerlink\" title=\"Floyd算法\"></a>Floyd算法</h4><p>基于动态规划</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">初始化：</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i == j) d[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> d[i][j] = INF;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 算法结束后，d[a][b]表示a到b的最短距离</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">floyd</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">1</span>; k &lt;= n; k ++ )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">                d[i][j] = <span class=\"built_in\">min</span>(d[i][j], d[i][k] + d[k][j]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"最小生成树\"><a href=\"#最小生成树\" class=\"headerlink\" title=\"最小生成树\"></a>最小生成树</h3><h4 id=\"Prim算法\"><a href=\"#Prim算法\" class=\"headerlink\" title=\"Prim算法\"></a>Prim算法</h4><p>处理稠密图</p>\n<p><strong>朴素Prim算法</strong> $O(n^2)$</p>\n<p>类似dijkstra，找出距离集合最短的点，加入集合，更新其他点到集合的距离</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// n表示点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> g[N][N];        <span class=\"comment\">// 邻接矩阵，存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储其他点到当前最小生成树的距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否已经在生成树中</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">prim</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!st[j] &amp;&amp; (t == <span class=\"number\">-1</span> || dist[t] &gt; dist[j]))</span><br><span class=\"line\">                t = j;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &amp;&amp; dist[t] == INF) <span class=\"keyword\">return</span> INF;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i) res += dist[t];</span><br><span class=\"line\">        st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ ) dist[j] = <span class=\"built_in\">min</span>(dist[j], g[t][j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>堆优化Prim</strong> $O(mlogn)$ （不常用）</p>\n<h4 id=\"Kruskal算法\"><a href=\"#Kruskal算法\" class=\"headerlink\" title=\"Kruskal算法\"></a>Kruskal算法</h4><p>$O(mlogm)$ 处理稀疏图</p>\n<ol>\n<li>将所有边按照权重排序</li>\n<li>枚举每条边a-&gt;b,权重c （如果ab不联通，则将边ab加入集合中）</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n, m;       <span class=\"comment\">// n是点数，m是边数</span></span><br><span class=\"line\"><span class=\"type\">int</span> p[N];       <span class=\"comment\">// 并查集的父节点数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Edge</span>     <span class=\"comment\">// 存储边</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a, b, w;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">bool</span> <span class=\"keyword\">operator</span>&lt; (<span class=\"type\">const</span> Edge &amp;W)<span class=\"type\">const</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> w &lt; W.w;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;edges[M];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span>     <span class=\"comment\">// 并查集核心操作</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">kruskal</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(edges, edges + m);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;    <span class=\"comment\">// 初始化并查集</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>, cnt = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; m; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> a = edges[i].a, b = edges[i].b, w = edges[i].w;</span><br><span class=\"line\"></span><br><span class=\"line\">        a = <span class=\"built_in\">find</span>(a), b = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (a != b)     <span class=\"comment\">// 如果两个连通块不连通，则将这两个连通块合并</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            p[a] = b;</span><br><span class=\"line\">            res += w;</span><br><span class=\"line\">            cnt ++ ;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (cnt &lt; n - <span class=\"number\">1</span>) <span class=\"keyword\">return</span> INF;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>","more":"<h3 id=\"最短路\"><a href=\"#最短路\" class=\"headerlink\" title=\"最短路\"></a>最短路</h3><p>$n$为点数，$m$为边数</p>\n<p>若$m$与$n^2$同一级别为稠密图，与$n$同一级别为稀疏图</p>\n<p>稠密图使用邻接矩阵储存，稀疏图用邻接表储存</p>\n<ul>\n<li>单源最短路<ul>\n<li>所有边权都是正数<ul>\n<li>朴素$dijkstra$算法 $O(n^2+m)$</li>\n<li>堆优化版$dijkstra$算法 $O(mlogn)$</li>\n</ul>\n</li>\n<li>存在负权边<ul>\n<li>$Bellman-Ford$算法 $O(nm)$</li>\n<li>$SPFA$算法 一般$O(m)$，最坏$O(nm)$</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>多源汇最短路<ul>\n<li>$floyd$算法 $O(n^3)$</li>\n</ul>\n</li>\n</ul>\n<h4 id=\"朴素dijkstra算法\"><a href=\"#朴素dijkstra算法\" class=\"headerlink\" title=\"朴素dijkstra算法\"></a>朴素dijkstra算法</h4><ol>\n<li>初始化距离，$dist[1]&#x3D;0,dist[i]&#x3D;+\\infty $，st数组：当前已经确定最短路径的点</li>\n<li>循环每一个点，找到不在st中的最短距离点t，t加入到st中，用t更新其他点的距离</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> g[N][N];  <span class=\"comment\">// 存储每条边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];  <span class=\"comment\">// 存储1号点到每个点的最短距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];   <span class=\"comment\">// 存储每个点的最短路是否已经确定</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短路，如果不存在则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dijkstra</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n - <span class=\"number\">1</span>; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = <span class=\"number\">-1</span>;     <span class=\"comment\">// 在还未确定最短路的点中，寻找距离最小的点</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!st[j] &amp;&amp; (t == <span class=\"number\">-1</span> || dist[t] &gt; dist[j]))</span><br><span class=\"line\">                t = j;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"comment\">// 用t更新其他点的距离</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            dist[j] = <span class=\"built_in\">min</span>(dist[j], dist[t] + g[t][j]);</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>一般边数m比较多，所以使用邻接矩阵<code>g[a][b]</code>存储</p>\n<h4 id=\"堆优化版dijkstra算法\"><a href=\"#堆优化版dijkstra算法\" class=\"headerlink\" title=\"堆优化版dijkstra算法\"></a>堆优化版dijkstra算法</h4><p>将寻找距离最小的点的时间复杂度降低</p>\n<p>堆可以使用手写堆或<strong>优先队列</strong></p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">typedef</span> pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; PII;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 点的数量</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储所有点到1号点的距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点的最短距离是否已确定</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> c)</span></span>&#123;</span><br><span class=\"line\">    e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx++;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短距离，如果不存在，则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dijkstra</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap;</span><br><span class=\"line\">    heap.<span class=\"built_in\">push</span>(&#123;<span class=\"number\">0</span>, <span class=\"number\">1</span>&#125;);      <span class=\"comment\">// first存储距离，second存储节点编号</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (heap.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = heap.<span class=\"built_in\">top</span>();</span><br><span class=\"line\">        heap.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"type\">int</span> ver = t.second, distance = t.first;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (st[ver]) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        st[ver] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[ver]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; distance + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = distance + w[i];</span><br><span class=\"line\">                heap.<span class=\"built_in\">push</span>(&#123;dist[j], j&#125;);</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Bellman-Ford算法\"><a href=\"#Bellman-Ford算法\" class=\"headerlink\" title=\"Bellman-Ford算法\"></a>Bellman-Ford算法</h4><p>迭代n次，每次遍历所有边，对dist[b]进行更新</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n, m;       <span class=\"comment\">// n表示点数，m表示边数</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// dist[x]存储1到x的最短路距离</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Edge</span>     <span class=\"comment\">// 边，a表示出点，b表示入点，w表示边的权重</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a, b, w;</span><br><span class=\"line\">&#125;edges[M];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1到n的最短路距离，如果无法从1走到n，则返回-1。</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">bellman_ford</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果第n次迭代仍然会松弛三角不等式，就说明存在一条长度是n+1的最短路径，由抽屉原理，路径中至少存在两个相同的点，说明图中存在负权回路。</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; m; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> a = edges[j].a, b = edges[j].b, w = edges[j].w;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[b] &gt; dist[a] + w)</span><br><span class=\"line\">                dist[b] = dist[a] + w;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] &gt; <span class=\"number\">0x3f3f3f3f</span> / <span class=\"number\">2</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\">//遍历完后满足三角不等式</span></span><br><span class=\"line\">dist[b] &lt;= dist[a] + w</span><br></pre></td></tr></table></figure>\n\n<p>可以用于找负环，时间复杂度比较高</p>\n<h4 id=\"SPFA算法\"><a href=\"#SPFA算法\" class=\"headerlink\" title=\"SPFA算法\"></a>SPFA算法</h4><p>队列优化的Bellman-Ford算法</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 总点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储每个点到1号点的最短距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否在队列中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 求1号点到n号点的最短路距离，如果从1号点无法走到n号点则返回-1</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">spfa</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\">    dist[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">    q.<span class=\"built_in\">push</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\">    st[<span class=\"number\">1</span>] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">        q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; dist[t] + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = dist[t] + w[i];</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!st[j])     <span class=\"comment\">// 如果队列中已存在j，则不需要将j重复插入</span></span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">                    st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (dist[n] == <span class=\"number\">0x3f3f3f3f</span>) <span class=\"keyword\">return</span> <span class=\"number\">-1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> dist[n];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>推荐使用，只要不被卡或者存在负环</p>\n<p><strong>判断负环</strong></p>\n<p>在进行更新dist[j] &#x3D; dist[t] + w时，同时维护cnt[x] (1号点到x号点经过的边数)</p>\n<p>cnt[x] &#x3D; cnt[t]+1;</p>\n<p>若某个cnt[x] 大于等于n，则说明存在负环</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// 总点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], w[N], e[N], ne[N], idx;       <span class=\"comment\">// 邻接表存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N], cnt[N];        <span class=\"comment\">// dist[x]存储1号点到x的最短距离，cnt[x]存储1到x的最短路中经过的点数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否在队列中</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果存在负环，则返回true，否则返回false。</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">spfa</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// 不需要初始化dist数组</span></span><br><span class=\"line\">    <span class=\"comment\">// 原理：如果某条最短路径上有n个点（除了自己），那么加上自己之后一共有n+1个点，由抽屉原理一定有两个点相同，所以存在环。</span></span><br><span class=\"line\"></span><br><span class=\"line\">    queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        q.<span class=\"built_in\">push</span>(i);</span><br><span class=\"line\">        st[i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">auto</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">        q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">        st[t] = <span class=\"literal\">false</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (dist[j] &gt; dist[t] + w[i])</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                dist[j] = dist[t] + w[i];</span><br><span class=\"line\">                cnt[j] = cnt[t] + <span class=\"number\">1</span>;</span><br><span class=\"line\">                <span class=\"keyword\">if</span> (cnt[j] &gt;= n) <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;       <span class=\"comment\">// 如果从1号点到x的最短路中包含至少n个点（不包括自己），则说明存在环</span></span><br><span class=\"line\">                <span class=\"keyword\">if</span> (!st[j])</span><br><span class=\"line\">                &#123;</span><br><span class=\"line\">                    q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">                    st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">                &#125;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Floyd算法\"><a href=\"#Floyd算法\" class=\"headerlink\" title=\"Floyd算法\"></a>Floyd算法</h4><p>基于动态规划</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">初始化：</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i == j) d[i][j] = <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">else</span> d[i][j] = INF;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 算法结束后，d[a][b]表示a到b的最短距离</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">floyd</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> k = <span class=\"number\">1</span>; k &lt;= n; k ++ )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">            <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">                d[i][j] = <span class=\"built_in\">min</span>(d[i][j], d[i][k] + d[k][j]);</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"最小生成树\"><a href=\"#最小生成树\" class=\"headerlink\" title=\"最小生成树\"></a>最小生成树</h3><h4 id=\"Prim算法\"><a href=\"#Prim算法\" class=\"headerlink\" title=\"Prim算法\"></a>Prim算法</h4><p>处理稠密图</p>\n<p><strong>朴素Prim算法</strong> $O(n^2)$</p>\n<p>类似dijkstra，找出距离集合最短的点，加入集合，更新其他点到集合的距离</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n;      <span class=\"comment\">// n表示点数</span></span><br><span class=\"line\"><span class=\"type\">int</span> g[N][N];        <span class=\"comment\">// 邻接矩阵，存储所有边</span></span><br><span class=\"line\"><span class=\"type\">int</span> dist[N];        <span class=\"comment\">// 存储其他点到当前最小生成树的距离</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个点是否已经在生成树中</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 如果图不连通，则返回INF(值是0x3f3f3f3f), 否则返回最小生成树的树边权重之和</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">prim</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">memset</span>(dist, <span class=\"number\">0x3f</span>, <span class=\"keyword\">sizeof</span> dist);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = <span class=\"number\">-1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (!st[j] &amp;&amp; (t == <span class=\"number\">-1</span> || dist[t] &gt; dist[j]))</span><br><span class=\"line\">                t = j;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i &amp;&amp; dist[t] == INF) <span class=\"keyword\">return</span> INF;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (i) res += dist[t];</span><br><span class=\"line\">        st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">1</span>; j &lt;= n; j ++ ) dist[j] = <span class=\"built_in\">min</span>(dist[j], g[t][j]);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>堆优化Prim</strong> $O(mlogn)$ （不常用）</p>\n<h4 id=\"Kruskal算法\"><a href=\"#Kruskal算法\" class=\"headerlink\" title=\"Kruskal算法\"></a>Kruskal算法</h4><p>$O(mlogm)$ 处理稀疏图</p>\n<ol>\n<li>将所有边按照权重排序</li>\n<li>枚举每条边a-&gt;b,权重c （如果ab不联通，则将边ab加入集合中）</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> n, m;       <span class=\"comment\">// n是点数，m是边数</span></span><br><span class=\"line\"><span class=\"type\">int</span> p[N];       <span class=\"comment\">// 并查集的父节点数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">Edge</span>     <span class=\"comment\">// 存储边</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a, b, w;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">bool</span> <span class=\"keyword\">operator</span>&lt; (<span class=\"type\">const</span> Edge &amp;W)<span class=\"type\">const</span></span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> w &lt; W.w;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;edges[M];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span>     <span class=\"comment\">// 并查集核心操作</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">kruskal</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(edges, edges + m);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;    <span class=\"comment\">// 初始化并查集</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>, cnt = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; m; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> a = edges[i].a, b = edges[i].b, w = edges[i].w;</span><br><span class=\"line\"></span><br><span class=\"line\">        a = <span class=\"built_in\">find</span>(a), b = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (a != b)     <span class=\"comment\">// 如果两个连通块不连通，则将这两个连通块合并</span></span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            p[a] = b;</span><br><span class=\"line\">            res += w;</span><br><span class=\"line\">            cnt ++ ;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (cnt &lt; n - <span class=\"number\">1</span>) <span class=\"keyword\">return</span> INF;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>"},{"title":"Algorithm-Math","mathjax":true,"date":"2023-09-07T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛中的数学问题","_content":"\n### 质数\n\n对于**大于一**的整数，如果只包含一和本身这两个约数，它就是质数（也叫素数）\n\n#### 试除法\n\n$O(\\sqrt n)$\n\n```cpp\nbool is_prime(int x)\n{\n    if (x < 2) return false;\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n            return false;\n    return true;\n}\n```\n\n#### 试除法分解质因数\n\n```cpp\nN = p1^c1 * p2^c2 * ... *pk^ck\n```\n\n从小到大枚举每一个数\n\n得到每一对$(p,c)$\n\n```cpp\nvoid divide(int x)\n{\n    // n中只有最多一个大于sqrt(n)的质因子\n    // 枚举到sqrt(t)，最后一个特殊处理 O(sqrt(n))\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            int c = 0;\n            while (x % i == 0) x /= i, c ++ ;\n            cout << i << ' ' << c << endl;\n        }\n    if (x > 1) cout << x << ' ' << 1 << endl;\n    cout << endl;\n}\n```\n\n#### 朴素筛法求素数\n\n枚举每一个数，如果它没有被筛，则加入质数集合，并且把它的所有倍数都筛掉\n\n优化：埃氏筛法，只需要把质数的倍数筛掉\n\n$O(nloglogn)$\n\n质数定理：1-n中有$\\frac{n}{ln_{}{n}}$个质数\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nbool st[N];         // st[x]存储x是否被筛掉\n\nvoid get_primes(int n)\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (st[i]) continue;\n        primes[cnt ++ ] = i;\n        for (int j = i + i; j <= n; j += i)\n            st[j] = true;\n    }\n}\n```\n\n#### 线性筛法求素数\n\n$n$只会被他的最小质因子筛掉\n\n$O(n)$\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nbool st[N];         // st[x]存储x是否被筛掉\n\nvoid get_primes(int n)\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i]) primes[cnt ++ ] = i;\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            st[primes[j] * i] = true;\n            if (i % primes[j] == 0) break; //此时primes[j]是i的最小质因子\n        }\n    }\n}\n```\n\n### 约数\n\n#### 试除法求所有约数\n\n$O(\\sqrt{n})$\n\n```cpp\nvector<int> get_divisors(int x)\n{\n    vector<int> res;\n    for (int i = 1; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            res.push_back(i);\n            if (i != x / i) res.push_back(x / i); //防止相同的数被push进去两倍，例如4*4=16\n        }\n    sort(res.begin(), res.end());\n    return res;\n}\n```\n\n#### 约数个数和约数之和\n\n如果 N = p1^c1 * p2^c2 * … *pk^ck\n约数个数： (c1 + 1) * (c2 + 1) * … * (ck + 1)\n约数之和： (p1^0 + p1^1 + … + p1^c1) * … * (pk^0 + pk^1 + … + pk^ck)\n\n#### 欧几里得算法求最大公约数\n\n```cpp\nint gcd(int a, int b)\n{\n    return b ? gcd(b, a % b) : a;\n}\n```\n\n可以使用库函数`__gcd(int a, int b)`，此外最小公倍数=$\\frac{a  b}{gcd(a,b)}$\n\n### 欧拉函数\n\n$\\phi(n)$：1-n中与n互质的数的个数\n\n$\\phi(n) = n*(1-\\frac{1}{p_1})*(1-\\frac{1}{p_2})*...*(1-\\frac{1}{p_n})$\n\n```cpp\nint phi(int x)\n{\n    int res = x;\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            res = res / i * (i - 1);\n            while (x % i == 0) x /= i;\n        }\n    if (x > 1) res = res / x * (x - 1);\n\n    return res;\n}\n```\n\n#### 筛法求欧拉函数\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nint euler[N];           // 存储每个数的欧拉函数\nbool st[N];         // st[x]存储x是否被筛掉\n\n\nvoid get_eulers(int n)\n{\n    euler[1] = 1;\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i])\n        {\n            primes[cnt ++ ] = i;\n            euler[i] = i - 1;\n        }\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            int t = primes[j] * i;\n            st[t] = true;\n            if (i % primes[j] == 0)\n            {\n                euler[t] = euler[i] * primes[j];\n                break;\n            }\n            euler[t] = euler[i] * (primes[j] - 1);\n        }\n    }\n}\n```\n\n#### 欧拉定理\n\n若$a$与$n$互质，则\n\n$a^{\\phi{(n)}} \\equiv 1 (mod \\ n)$\n\n### 快速幂\n\n在$O(logk)$时间内求出求出$a^k mod p$\n\n```cpp\nint qmi(int m, int k, int p)\n{\n    int res = 1 % p, t = m;\n    while (k)\n    {\n        if (k&1) res = res * t % p;\n        t = t * t % p;\n        k >>= 1;\n    }\n    return res;\n}\n```\n\n### 扩展欧几里得算法\n\n裴蜀定理：对于正整数$a,b$，一定存在整数$x,y$，使得\n\n$ax+by = gcd(a,b)$\n\n```cpp\n// 求x, y，使得ax + by = gcd(a, b)\nint exgcd(int a, int b, int &x, int &y)\n{\n    if (!b)\n    {\n        x = 1; y = 0;\n        return a;\n    }\n    int d = exgcd(b, a % b, y, x);\n    y -= (a/b) * x;\n    return d;\n}\n```\n\n### 中国剩余定理\n\n给定一些两两互质的数$m_1,m_2,m_3,m_k$，求解线性同余方程组\n\n$x \\equiv a_1 (mod \\ m_1)$\n\n$x \\equiv a_2 (mod \\ m_2)$\n\n$...$\n\n$x \\equiv a_k (mod \\ m_k)$\n\n$M = m_1 * m_2*...*m_k$\n\n$M_i = \\frac{M}{m_i}$\n\n$M_i^{-1}$表示$M_i$模$m_i$的逆\n\n$x = a_1M_1M_1^{-1}+a_2M_1M_2^{-1}+...+a_kM_1M_k^{-1}$\n\n### 高斯消元\n\n在$O(n^3)$内求解线性方程组\n\n```cpp\n// a[N][N]是增广矩阵\nint gauss()\n{\n    int c, r;\n    for (c = 0, r = 0; c < n; c ++ )\n    {\n        int t = r;\n        for (int i = r; i < n; i ++ )   // 找到绝对值最大的行\n            if (fabs(a[i][c]) > fabs(a[t][c]))\n                t = i;\n\t\t//eps精度 1e-6\n        if (fabs(a[t][c]) < eps) continue;\n\n        for (int i = c; i <= n; i ++ ) swap(a[t][i], a[r][i]);      // 将绝对值最大的行换到最顶端\n        for (int i = n; i >= c; i -- ) a[r][i] /= a[r][c];      // 将当前行的首位变成1\n        for (int i = r + 1; i < n; i ++ )       // 用当前行将下面所有的列消成0\n            if (fabs(a[i][c]) > eps)\n                for (int j = n; j >= c; j -- )\n                    a[i][j] -= a[r][j] * a[i][c];\n\n        r ++ ;\n    }\n\n    if (r < n)\n    {\n        for (int i = r; i < n; i ++ )\n            if (fabs(a[i][n]) > eps)\n                return 2; // 无解\n        return 1; // 有无穷多组解\n    }\n`\n    for (int i = n - 1; i >= 0; i -- )\n        for (int j = i + 1; j < n; j ++ )\n            //储存答案\n            a[i][n] -= a[i][j] * a[j][n];\n\n    return 0; // 有唯一解\n}\n```\n\n### 组合数\n\n- $1 \\le b \\le a \\le 2000$ 递推 $N^2$\n- $1 \\le b \\le a \\le 10^5$ 预处理 $NlogN$\n- $1 \\le b \\le a \\le 10^{18}, 1 \\le p \\le 10^5$ 卢卡斯定理Lucas\n- \n\n组合数$C_{n}^{m}=\\frac{n!}{m!(n-m)!} $\n\n#### 朴素求法\n\n```cpp\nLL C(int a,int b){\n    LL res = 1;\n    for(int i=a,j=1;j<=b;i--,j++){\n        res = res*i/j;\n    }\n    return res;\n}\n```\n\n#### 递推法求组合数\n\n$C_{a}^{b} = C_{a-1}^{b} + C_{a-1}^{b-1}$\n\n```cpp\n// c[a][b] 表示从a个苹果中选b个的方案数\nfor (int i = 0; i < N; i ++ )\n    for (int j = 0; j <= i; j ++ )\n        if (!j) c[i][j] = 1;\n        else c[i][j] = (c[i - 1][j] + c[i - 1][j - 1]) % mod;\n```\n\n#### 通过预处理逆元的方式求组合数\n\n```cpp\n首先预处理出所有阶乘取模的余数fact[N]，以及所有阶乘取模的逆元infact[N]\n如果取模的数是质数，可以用费马小定理求逆元\nint qmi(int a, int k, int p)    // 快速幂模板\n{\n    int res = 1;\n    while (k)\n    {\n        if (k & 1) res = (LL)res * a % p;\n        a = (LL)a * a % p;\n        k >>= 1;\n    }\n    return res;\n}\n\n// 预处理阶乘的余数和阶乘逆元的余数\nfact[0] = infact[0] = 1;\nfor (int i = 1; i < N; i ++ )\n{\n    fact[i] = (LL)fact[i - 1] * i % mod;\n    infact[i] = (LL)infact[i - 1] * qmi(i, mod - 2, mod) % mod;\n}\n```\n\n#### Lucas定理\n\n```cpp\n若p是质数，则对于任意整数 1 <= m <= n，有：\n    C(n, m) = C(n % p, m % p) * C(n / p, m / p) (mod p)\n\nint qmi(int a, int k, int p)  // 快速幂模板\n{\n    int res = 1 % p;\n    while (k)\n    {\n        if (k & 1) res = (LL)res * a % p;\n        a = (LL)a * a % p;\n        k >>= 1;\n    }\n    return res;\n}\n\nint C(int a, int b, int p)  // 通过定理求组合数C(a, b)\n{\n    if (a < b) return 0;\n\n    LL x = 1, y = 1;  // x是分子，y是分母\n    for (int i = a, j = 1; j <= b; i --, j ++ )\n    {\n        x = (LL)x * i % p;\n        y = (LL) y * j % p;\n    }\n\n    return x * (LL)qmi(y, p - 2, p) % p;\n}\n\nint lucas(LL a, LL b, int p)\n{\n    if (a < p && b < p) return C(a, b, p);\n    return (LL)C(a % p, b % p, p) * lucas(a / p, b / p, p) % p;\n}\n```\n\n#### 分解质因数法求组合数\n\n```cpp\n当我们需要求出组合数的真实值，而非对某个数的余数时，分解质因数的方式比较好用：\n    1. 筛法求出范围内的所有质数\n    2. 通过 C(a, b) = a! / b! / (a - b)! 这个公式求出每个质因子的次数。 n! 中p的次数是 n / p + n / p^2 + n / p^3 + ...\n    3. 用高精度乘法将所有质因子相乘\n\nint primes[N], cnt;     // 存储所有质数\nint sum[N];     // 存储每个质数的次数\nbool st[N];     // 存储每个数是否已被筛掉\n\n\nvoid get_primes(int n)      // 线性筛法求素数\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i]) primes[cnt ++ ] = i;\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            st[primes[j] * i] = true;\n            if (i % primes[j] == 0) break;\n        }\n    }\n}\n\n\nint get(int n, int p)       // 求n！中的次数\n{\n    int res = 0;\n    while (n)\n    {\n        res += n / p;\n        n /= p;\n    }\n    return res;\n}\n\n\nvector<int> mul(vector<int> a, int b)       // 高精度乘低精度模板\n{\n    vector<int> c;\n    int t = 0;\n    for (int i = 0; i < a.size(); i ++ )\n    {\n        t += a[i] * b;\n        c.push_back(t % 10);\n        t /= 10;\n    }\n\n    while (t)\n    {\n        c.push_back(t % 10);\n        t /= 10;\n    }\n\n    return c;\n}\n\nget_primes(a);  // 预处理范围内的所有质数\n\nfor (int i = 0; i < cnt; i ++ )     // 求每个质因数的次数\n{\n    int p = primes[i];\n    sum[i] = get(a, p) - get(b, p) - get(a - b, p);\n}\n\nvector<int> res;\nres.push_back(1);\n\nfor (int i = 0; i < cnt; i ++ )     // 用高精度乘法将所有质因子相乘\n    for (int j = 0; j < sum[i]; j ++ )\n        res = mul(res, primes[i]);\n```\n\n### 卡特兰数\n\n给定n个0和n个1，它们按照某种顺序排成长度为2n的序列，满足任意前缀中0的个数都不少于1的个数的序列的数量为： Cat(n) = C(2n, n) / (n + 1)","source":"_posts/algorithm-math.md","raw":"---\ntitle: Algorithm-Math\nmathjax: true\ndate: 2023/09/07 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛中的数学问题\n---\n\n### 质数\n\n对于**大于一**的整数，如果只包含一和本身这两个约数，它就是质数（也叫素数）\n\n#### 试除法\n\n$O(\\sqrt n)$\n\n```cpp\nbool is_prime(int x)\n{\n    if (x < 2) return false;\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n            return false;\n    return true;\n}\n```\n\n#### 试除法分解质因数\n\n```cpp\nN = p1^c1 * p2^c2 * ... *pk^ck\n```\n\n从小到大枚举每一个数\n\n得到每一对$(p,c)$\n\n```cpp\nvoid divide(int x)\n{\n    // n中只有最多一个大于sqrt(n)的质因子\n    // 枚举到sqrt(t)，最后一个特殊处理 O(sqrt(n))\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            int c = 0;\n            while (x % i == 0) x /= i, c ++ ;\n            cout << i << ' ' << c << endl;\n        }\n    if (x > 1) cout << x << ' ' << 1 << endl;\n    cout << endl;\n}\n```\n\n#### 朴素筛法求素数\n\n枚举每一个数，如果它没有被筛，则加入质数集合，并且把它的所有倍数都筛掉\n\n优化：埃氏筛法，只需要把质数的倍数筛掉\n\n$O(nloglogn)$\n\n质数定理：1-n中有$\\frac{n}{ln_{}{n}}$个质数\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nbool st[N];         // st[x]存储x是否被筛掉\n\nvoid get_primes(int n)\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (st[i]) continue;\n        primes[cnt ++ ] = i;\n        for (int j = i + i; j <= n; j += i)\n            st[j] = true;\n    }\n}\n```\n\n#### 线性筛法求素数\n\n$n$只会被他的最小质因子筛掉\n\n$O(n)$\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nbool st[N];         // st[x]存储x是否被筛掉\n\nvoid get_primes(int n)\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i]) primes[cnt ++ ] = i;\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            st[primes[j] * i] = true;\n            if (i % primes[j] == 0) break; //此时primes[j]是i的最小质因子\n        }\n    }\n}\n```\n\n### 约数\n\n#### 试除法求所有约数\n\n$O(\\sqrt{n})$\n\n```cpp\nvector<int> get_divisors(int x)\n{\n    vector<int> res;\n    for (int i = 1; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            res.push_back(i);\n            if (i != x / i) res.push_back(x / i); //防止相同的数被push进去两倍，例如4*4=16\n        }\n    sort(res.begin(), res.end());\n    return res;\n}\n```\n\n#### 约数个数和约数之和\n\n如果 N = p1^c1 * p2^c2 * … *pk^ck\n约数个数： (c1 + 1) * (c2 + 1) * … * (ck + 1)\n约数之和： (p1^0 + p1^1 + … + p1^c1) * … * (pk^0 + pk^1 + … + pk^ck)\n\n#### 欧几里得算法求最大公约数\n\n```cpp\nint gcd(int a, int b)\n{\n    return b ? gcd(b, a % b) : a;\n}\n```\n\n可以使用库函数`__gcd(int a, int b)`，此外最小公倍数=$\\frac{a  b}{gcd(a,b)}$\n\n### 欧拉函数\n\n$\\phi(n)$：1-n中与n互质的数的个数\n\n$\\phi(n) = n*(1-\\frac{1}{p_1})*(1-\\frac{1}{p_2})*...*(1-\\frac{1}{p_n})$\n\n```cpp\nint phi(int x)\n{\n    int res = x;\n    for (int i = 2; i <= x / i; i ++ )\n        if (x % i == 0)\n        {\n            res = res / i * (i - 1);\n            while (x % i == 0) x /= i;\n        }\n    if (x > 1) res = res / x * (x - 1);\n\n    return res;\n}\n```\n\n#### 筛法求欧拉函数\n\n```cpp\nint primes[N], cnt;     // primes[]存储所有素数\nint euler[N];           // 存储每个数的欧拉函数\nbool st[N];         // st[x]存储x是否被筛掉\n\n\nvoid get_eulers(int n)\n{\n    euler[1] = 1;\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i])\n        {\n            primes[cnt ++ ] = i;\n            euler[i] = i - 1;\n        }\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            int t = primes[j] * i;\n            st[t] = true;\n            if (i % primes[j] == 0)\n            {\n                euler[t] = euler[i] * primes[j];\n                break;\n            }\n            euler[t] = euler[i] * (primes[j] - 1);\n        }\n    }\n}\n```\n\n#### 欧拉定理\n\n若$a$与$n$互质，则\n\n$a^{\\phi{(n)}} \\equiv 1 (mod \\ n)$\n\n### 快速幂\n\n在$O(logk)$时间内求出求出$a^k mod p$\n\n```cpp\nint qmi(int m, int k, int p)\n{\n    int res = 1 % p, t = m;\n    while (k)\n    {\n        if (k&1) res = res * t % p;\n        t = t * t % p;\n        k >>= 1;\n    }\n    return res;\n}\n```\n\n### 扩展欧几里得算法\n\n裴蜀定理：对于正整数$a,b$，一定存在整数$x,y$，使得\n\n$ax+by = gcd(a,b)$\n\n```cpp\n// 求x, y，使得ax + by = gcd(a, b)\nint exgcd(int a, int b, int &x, int &y)\n{\n    if (!b)\n    {\n        x = 1; y = 0;\n        return a;\n    }\n    int d = exgcd(b, a % b, y, x);\n    y -= (a/b) * x;\n    return d;\n}\n```\n\n### 中国剩余定理\n\n给定一些两两互质的数$m_1,m_2,m_3,m_k$，求解线性同余方程组\n\n$x \\equiv a_1 (mod \\ m_1)$\n\n$x \\equiv a_2 (mod \\ m_2)$\n\n$...$\n\n$x \\equiv a_k (mod \\ m_k)$\n\n$M = m_1 * m_2*...*m_k$\n\n$M_i = \\frac{M}{m_i}$\n\n$M_i^{-1}$表示$M_i$模$m_i$的逆\n\n$x = a_1M_1M_1^{-1}+a_2M_1M_2^{-1}+...+a_kM_1M_k^{-1}$\n\n### 高斯消元\n\n在$O(n^3)$内求解线性方程组\n\n```cpp\n// a[N][N]是增广矩阵\nint gauss()\n{\n    int c, r;\n    for (c = 0, r = 0; c < n; c ++ )\n    {\n        int t = r;\n        for (int i = r; i < n; i ++ )   // 找到绝对值最大的行\n            if (fabs(a[i][c]) > fabs(a[t][c]))\n                t = i;\n\t\t//eps精度 1e-6\n        if (fabs(a[t][c]) < eps) continue;\n\n        for (int i = c; i <= n; i ++ ) swap(a[t][i], a[r][i]);      // 将绝对值最大的行换到最顶端\n        for (int i = n; i >= c; i -- ) a[r][i] /= a[r][c];      // 将当前行的首位变成1\n        for (int i = r + 1; i < n; i ++ )       // 用当前行将下面所有的列消成0\n            if (fabs(a[i][c]) > eps)\n                for (int j = n; j >= c; j -- )\n                    a[i][j] -= a[r][j] * a[i][c];\n\n        r ++ ;\n    }\n\n    if (r < n)\n    {\n        for (int i = r; i < n; i ++ )\n            if (fabs(a[i][n]) > eps)\n                return 2; // 无解\n        return 1; // 有无穷多组解\n    }\n`\n    for (int i = n - 1; i >= 0; i -- )\n        for (int j = i + 1; j < n; j ++ )\n            //储存答案\n            a[i][n] -= a[i][j] * a[j][n];\n\n    return 0; // 有唯一解\n}\n```\n\n### 组合数\n\n- $1 \\le b \\le a \\le 2000$ 递推 $N^2$\n- $1 \\le b \\le a \\le 10^5$ 预处理 $NlogN$\n- $1 \\le b \\le a \\le 10^{18}, 1 \\le p \\le 10^5$ 卢卡斯定理Lucas\n- \n\n组合数$C_{n}^{m}=\\frac{n!}{m!(n-m)!} $\n\n#### 朴素求法\n\n```cpp\nLL C(int a,int b){\n    LL res = 1;\n    for(int i=a,j=1;j<=b;i--,j++){\n        res = res*i/j;\n    }\n    return res;\n}\n```\n\n#### 递推法求组合数\n\n$C_{a}^{b} = C_{a-1}^{b} + C_{a-1}^{b-1}$\n\n```cpp\n// c[a][b] 表示从a个苹果中选b个的方案数\nfor (int i = 0; i < N; i ++ )\n    for (int j = 0; j <= i; j ++ )\n        if (!j) c[i][j] = 1;\n        else c[i][j] = (c[i - 1][j] + c[i - 1][j - 1]) % mod;\n```\n\n#### 通过预处理逆元的方式求组合数\n\n```cpp\n首先预处理出所有阶乘取模的余数fact[N]，以及所有阶乘取模的逆元infact[N]\n如果取模的数是质数，可以用费马小定理求逆元\nint qmi(int a, int k, int p)    // 快速幂模板\n{\n    int res = 1;\n    while (k)\n    {\n        if (k & 1) res = (LL)res * a % p;\n        a = (LL)a * a % p;\n        k >>= 1;\n    }\n    return res;\n}\n\n// 预处理阶乘的余数和阶乘逆元的余数\nfact[0] = infact[0] = 1;\nfor (int i = 1; i < N; i ++ )\n{\n    fact[i] = (LL)fact[i - 1] * i % mod;\n    infact[i] = (LL)infact[i - 1] * qmi(i, mod - 2, mod) % mod;\n}\n```\n\n#### Lucas定理\n\n```cpp\n若p是质数，则对于任意整数 1 <= m <= n，有：\n    C(n, m) = C(n % p, m % p) * C(n / p, m / p) (mod p)\n\nint qmi(int a, int k, int p)  // 快速幂模板\n{\n    int res = 1 % p;\n    while (k)\n    {\n        if (k & 1) res = (LL)res * a % p;\n        a = (LL)a * a % p;\n        k >>= 1;\n    }\n    return res;\n}\n\nint C(int a, int b, int p)  // 通过定理求组合数C(a, b)\n{\n    if (a < b) return 0;\n\n    LL x = 1, y = 1;  // x是分子，y是分母\n    for (int i = a, j = 1; j <= b; i --, j ++ )\n    {\n        x = (LL)x * i % p;\n        y = (LL) y * j % p;\n    }\n\n    return x * (LL)qmi(y, p - 2, p) % p;\n}\n\nint lucas(LL a, LL b, int p)\n{\n    if (a < p && b < p) return C(a, b, p);\n    return (LL)C(a % p, b % p, p) * lucas(a / p, b / p, p) % p;\n}\n```\n\n#### 分解质因数法求组合数\n\n```cpp\n当我们需要求出组合数的真实值，而非对某个数的余数时，分解质因数的方式比较好用：\n    1. 筛法求出范围内的所有质数\n    2. 通过 C(a, b) = a! / b! / (a - b)! 这个公式求出每个质因子的次数。 n! 中p的次数是 n / p + n / p^2 + n / p^3 + ...\n    3. 用高精度乘法将所有质因子相乘\n\nint primes[N], cnt;     // 存储所有质数\nint sum[N];     // 存储每个质数的次数\nbool st[N];     // 存储每个数是否已被筛掉\n\n\nvoid get_primes(int n)      // 线性筛法求素数\n{\n    for (int i = 2; i <= n; i ++ )\n    {\n        if (!st[i]) primes[cnt ++ ] = i;\n        for (int j = 0; primes[j] <= n / i; j ++ )\n        {\n            st[primes[j] * i] = true;\n            if (i % primes[j] == 0) break;\n        }\n    }\n}\n\n\nint get(int n, int p)       // 求n！中的次数\n{\n    int res = 0;\n    while (n)\n    {\n        res += n / p;\n        n /= p;\n    }\n    return res;\n}\n\n\nvector<int> mul(vector<int> a, int b)       // 高精度乘低精度模板\n{\n    vector<int> c;\n    int t = 0;\n    for (int i = 0; i < a.size(); i ++ )\n    {\n        t += a[i] * b;\n        c.push_back(t % 10);\n        t /= 10;\n    }\n\n    while (t)\n    {\n        c.push_back(t % 10);\n        t /= 10;\n    }\n\n    return c;\n}\n\nget_primes(a);  // 预处理范围内的所有质数\n\nfor (int i = 0; i < cnt; i ++ )     // 求每个质因数的次数\n{\n    int p = primes[i];\n    sum[i] = get(a, p) - get(b, p) - get(a - b, p);\n}\n\nvector<int> res;\nres.push_back(1);\n\nfor (int i = 0; i < cnt; i ++ )     // 用高精度乘法将所有质因子相乘\n    for (int j = 0; j < sum[i]; j ++ )\n        res = mul(res, primes[i]);\n```\n\n### 卡特兰数\n\n给定n个0和n个1，它们按照某种顺序排成长度为2n的序列，满足任意前缀中0的个数都不少于1的个数的序列的数量为： Cat(n) = C(2n, n) / (n + 1)","slug":"algorithm-math","published":1,"updated":"2025-03-05T03:12:58.052Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9d0007bs998jfo1byw","content":"<h3 id=\"质数\"><a href=\"#质数\" class=\"headerlink\" title=\"质数\"></a>质数</h3><p>对于<strong>大于一</strong>的整数，如果只包含一和本身这两个约数，它就是质数（也叫素数）</p>\n<h4 id=\"试除法\"><a href=\"#试除法\" class=\"headerlink\" title=\"试除法\"></a>试除法</h4><p>$O(\\sqrt n)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">is_prime</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &lt; <span class=\"number\">2</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"试除法分解质因数\"><a href=\"#试除法分解质因数\" class=\"headerlink\" title=\"试除法分解质因数\"></a>试除法分解质因数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = p1^c1 * p2^c2 * ... *pk^ck</span><br></pre></td></tr></table></figure>\n\n<p>从小到大枚举每一个数</p>\n<p>得到每一对$(p,c)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">divide</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// n中只有最多一个大于sqrt(n)的质因子</span></span><br><span class=\"line\">    <span class=\"comment\">// 枚举到sqrt(t)，最后一个特殊处理 O(sqrt(n))</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> c = <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (x % i == <span class=\"number\">0</span>) x /= i, c ++ ;</span><br><span class=\"line\">            cout &lt;&lt; i &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; c &lt;&lt; endl;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &gt; <span class=\"number\">1</span>) cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; <span class=\"number\">1</span> &lt;&lt; endl;</span><br><span class=\"line\">    cout &lt;&lt; endl;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"朴素筛法求素数\"><a href=\"#朴素筛法求素数\" class=\"headerlink\" title=\"朴素筛法求素数\"></a>朴素筛法求素数</h4><p>枚举每一个数，如果它没有被筛，则加入质数集合，并且把它的所有倍数都筛掉</p>\n<p>优化：埃氏筛法，只需要把质数的倍数筛掉</p>\n<p>$O(nloglogn)$</p>\n<p>质数定理：1-n中有$\\frac{n}{ln_{}{n}}$个质数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (st[i]) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = i + i; j &lt;= n; j += i)</span><br><span class=\"line\">            st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"线性筛法求素数\"><a href=\"#线性筛法求素数\" class=\"headerlink\" title=\"线性筛法求素数\"></a>线性筛法求素数</h4><p>$n$只会被他的最小质因子筛掉</p>\n<p>$O(n)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i]) primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[primes[j] * i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>) <span class=\"keyword\">break</span>; <span class=\"comment\">//此时primes[j]是i的最小质因子</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"约数\"><a href=\"#约数\" class=\"headerlink\" title=\"约数\"></a>约数</h3><h4 id=\"试除法求所有约数\"><a href=\"#试除法求所有约数\" class=\"headerlink\" title=\"试除法求所有约数\"></a>试除法求所有约数</h4><p>$O(\\sqrt{n})$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">get_divisors</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; res;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            res.<span class=\"built_in\">push_back</span>(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i != x / i) res.<span class=\"built_in\">push_back</span>(x / i); <span class=\"comment\">//防止相同的数被push进去两倍，例如4*4=16</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(res.<span class=\"built_in\">begin</span>(), res.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"约数个数和约数之和\"><a href=\"#约数个数和约数之和\" class=\"headerlink\" title=\"约数个数和约数之和\"></a>约数个数和约数之和</h4><p>如果 N &#x3D; p1^c1 * p2^c2 * … *pk^ck<br>约数个数： (c1 + 1) * (c2 + 1) * … * (ck + 1)<br>约数之和： (p1^0 + p1^1 + … + p1^c1) * … * (pk^0 + pk^1 + … + pk^ck)</p>\n<h4 id=\"欧几里得算法求最大公约数\"><a href=\"#欧几里得算法求最大公约数\" class=\"headerlink\" title=\"欧几里得算法求最大公约数\"></a>欧几里得算法求最大公约数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">gcd</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> b ? <span class=\"built_in\">gcd</span>(b, a % b) : a;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可以使用库函数<code>__gcd(int a, int b)</code>，此外最小公倍数&#x3D;$\\frac{a  b}{gcd(a,b)}$</p>\n<h3 id=\"欧拉函数\"><a href=\"#欧拉函数\" class=\"headerlink\" title=\"欧拉函数\"></a>欧拉函数</h3><p>$\\phi(n)$：1-n中与n互质的数的个数</p>\n<p>$\\phi(n) &#x3D; n*(1-\\frac{1}{p_1})<em>(1-\\frac{1}{p_2})</em>…*(1-\\frac{1}{p_n})$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">phi</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = x;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            res = res / i * (i - <span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (x % i == <span class=\"number\">0</span>) x /= i;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &gt; <span class=\"number\">1</span>) res = res / x * (x - <span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"筛法求欧拉函数\"><a href=\"#筛法求欧拉函数\" class=\"headerlink\" title=\"筛法求欧拉函数\"></a>筛法求欧拉函数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">int</span> euler[N];           <span class=\"comment\">// 存储每个数的欧拉函数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_eulers</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    euler[<span class=\"number\">1</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            primes[cnt ++ ] = i;</span><br><span class=\"line\">            euler[i] = i - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> t = primes[j] * i;</span><br><span class=\"line\">            st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                euler[t] = euler[i] * primes[j];</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            euler[t] = euler[i] * (primes[j] - <span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"欧拉定理\"><a href=\"#欧拉定理\" class=\"headerlink\" title=\"欧拉定理\"></a>欧拉定理</h4><p>若$a$与$n$互质，则</p>\n<p>$a^{\\phi{(n)}} \\equiv 1 (mod \\ n)$</p>\n<h3 id=\"快速幂\"><a href=\"#快速幂\" class=\"headerlink\" title=\"快速幂\"></a>快速幂</h3><p>在$O(logk)$时间内求出求出$a^k mod p$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">qmi</span><span class=\"params\">(<span class=\"type\">int</span> m, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span> % p, t = m;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k&amp;<span class=\"number\">1</span>) res = res * t % p;</span><br><span class=\"line\">        t = t * t % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"扩展欧几里得算法\"><a href=\"#扩展欧几里得算法\" class=\"headerlink\" title=\"扩展欧几里得算法\"></a>扩展欧几里得算法</h3><p>裴蜀定理：对于正整数$a,b$，一定存在整数$x,y$，使得</p>\n<p>$ax+by &#x3D; gcd(a,b)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 求x, y，使得ax + by = gcd(a, b)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">exgcd</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> &amp;x, <span class=\"type\">int</span> &amp;y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!b)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        x = <span class=\"number\">1</span>; y = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">int</span> d = <span class=\"built_in\">exgcd</span>(b, a % b, y, x);</span><br><span class=\"line\">    y -= (a/b) * x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"中国剩余定理\"><a href=\"#中国剩余定理\" class=\"headerlink\" title=\"中国剩余定理\"></a>中国剩余定理</h3><p>给定一些两两互质的数$m_1,m_2,m_3,m_k$，求解线性同余方程组</p>\n<p>$x \\equiv a_1 (mod \\ m_1)$</p>\n<p>$x \\equiv a_2 (mod \\ m_2)$</p>\n<p>$…$</p>\n<p>$x \\equiv a_k (mod \\ m_k)$</p>\n<p>$M &#x3D; m_1 * m_2*…*m_k$</p>\n<p>$M_i &#x3D; \\frac{M}{m_i}$</p>\n<p>$M_i^{-1}$表示$M_i$模$m_i$的逆</p>\n<p>$x &#x3D; a_1M_1M_1^{-1}+a_2M_1M_2^{-1}+…+a_kM_1M_k^{-1}$</p>\n<h3 id=\"高斯消元\"><a href=\"#高斯消元\" class=\"headerlink\" title=\"高斯消元\"></a>高斯消元</h3><p>在$O(n^3)$内求解线性方程组</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// a[N][N]是增广矩阵</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">gauss</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> c, r;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>, r = <span class=\"number\">0</span>; c &lt; n; c ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = r;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r; i &lt; n; i ++ )   <span class=\"comment\">// 找到绝对值最大的行</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][c]) &gt; <span class=\"built_in\">fabs</span>(a[t][c]))</span><br><span class=\"line\">                t = i;</span><br><span class=\"line\">\t\t<span class=\"comment\">//eps精度 1e-6</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[t][c]) &lt; eps) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = c; i &lt;= n; i ++ ) <span class=\"built_in\">swap</span>(a[t][i], a[r][i]);      <span class=\"comment\">// 将绝对值最大的行换到最顶端</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = n; i &gt;= c; i -- ) a[r][i] /= a[r][c];      <span class=\"comment\">// 将当前行的首位变成1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r + <span class=\"number\">1</span>; i &lt; n; i ++ )       <span class=\"comment\">// 用当前行将下面所有的列消成0</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][c]) &gt; eps)</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = n; j &gt;= c; j -- )</span><br><span class=\"line\">                    a[i][j] -= a[r][j] * a[i][c];</span><br><span class=\"line\"></span><br><span class=\"line\">        r ++ ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (r &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r; i &lt; n; i ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][n]) &gt; eps)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">2</span>; <span class=\"comment\">// 无解</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>; <span class=\"comment\">// 有无穷多组解</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">`</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = n - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; i -- )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = i + <span class=\"number\">1</span>; j &lt; n; j ++ )</span><br><span class=\"line\">            <span class=\"comment\">//储存答案</span></span><br><span class=\"line\">            a[i][n] -= a[i][j] * a[j][n];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>; <span class=\"comment\">// 有唯一解</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"组合数\"><a href=\"#组合数\" class=\"headerlink\" title=\"组合数\"></a>组合数</h3><ul>\n<li>$1 \\le b \\le a \\le 2000$ 递推 $N^2$</li>\n<li>$1 \\le b \\le a \\le 10^5$ 预处理 $NlogN$</li>\n<li>$1 \\le b \\le a \\le 10^{18}, 1 \\le p \\le 10^5$ 卢卡斯定理Lucas</li>\n<li></li>\n</ul>\n<p>组合数$C_{n}^{m}&#x3D;\\frac{n!}{m!(n-m)!} $</p>\n<h4 id=\"朴素求法\"><a href=\"#朴素求法\" class=\"headerlink\" title=\"朴素求法\"></a>朴素求法</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">LL <span class=\"title\">C</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> b)</span></span>&#123;</span><br><span class=\"line\">    LL res = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=a,j=<span class=\"number\">1</span>;j&lt;=b;i--,j++)&#123;</span><br><span class=\"line\">        res = res*i/j;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"递推法求组合数\"><a href=\"#递推法求组合数\" class=\"headerlink\" title=\"递推法求组合数\"></a>递推法求组合数</h4><p>$C_{a}^{b} &#x3D; C_{a-1}^{b} + C_{a-1}^{b-1}$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// c[a][b] 表示从a个苹果中选b个的方案数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i ++ )</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt;= i; j ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!j) c[i][j] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> c[i][j] = (c[i - <span class=\"number\">1</span>][j] + c[i - <span class=\"number\">1</span>][j - <span class=\"number\">1</span>]) % mod;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"通过预处理逆元的方式求组合数\"><a href=\"#通过预处理逆元的方式求组合数\" class=\"headerlink\" title=\"通过预处理逆元的方式求组合数\"></a>通过预处理逆元的方式求组合数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">首先预处理出所有阶乘取模的余数fact[N]，以及所有阶乘取模的逆元infact[N]</span><br><span class=\"line\">如果取模的数是质数，可以用费马小定理求逆元</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">qmi</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)</span>    <span class=\"comment\">// 快速幂模板</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k &amp; <span class=\"number\">1</span>) res = (LL)res * a % p;</span><br><span class=\"line\">        a = (LL)a * a % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 预处理阶乘的余数和阶乘逆元的余数</span></span><br><span class=\"line\">fact[<span class=\"number\">0</span>] = infact[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt; N; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    fact[i] = (LL)fact[i - <span class=\"number\">1</span>] * i % mod;</span><br><span class=\"line\">    infact[i] = (LL)infact[i - <span class=\"number\">1</span>] * <span class=\"built_in\">qmi</span>(i, mod - <span class=\"number\">2</span>, mod) % mod;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Lucas定理\"><a href=\"#Lucas定理\" class=\"headerlink\" title=\"Lucas定理\"></a>Lucas定理</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">若p是质数，则对于任意整数 <span class=\"number\">1</span> &lt;= m &lt;= n，有：</span><br><span class=\"line\">    <span class=\"built_in\">C</span>(n, m) = <span class=\"built_in\">C</span>(n % p, m % p) * <span class=\"built_in\">C</span>(n / p, m / p) (mod p)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"built_in\">qmi</span>(<span class=\"type\">int</span> a, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)  <span class=\"comment\">// 快速幂模板</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span> % p;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k &amp; <span class=\"number\">1</span>) res = (LL)res * a % p;</span><br><span class=\"line\">        a = (LL)a * a % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">C</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> p)</span>  <span class=\"comment\">// 通过定理求组合数C(a, b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a &lt; b) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    LL x = <span class=\"number\">1</span>, y = <span class=\"number\">1</span>;  <span class=\"comment\">// x是分子，y是分母</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = a, j = <span class=\"number\">1</span>; j &lt;= b; i --, j ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        x = (LL)x * i % p;</span><br><span class=\"line\">        y = (LL) y * j % p;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x * (LL)<span class=\"built_in\">qmi</span>(y, p - <span class=\"number\">2</span>, p) % p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">lucas</span><span class=\"params\">(LL a, LL b, <span class=\"type\">int</span> p)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a &lt; p &amp;&amp; b &lt; p) <span class=\"keyword\">return</span> <span class=\"built_in\">C</span>(a, b, p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (LL)<span class=\"built_in\">C</span>(a % p, b % p, p) * <span class=\"built_in\">lucas</span>(a / p, b / p, p) % p;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"分解质因数法求组合数\"><a href=\"#分解质因数法求组合数\" class=\"headerlink\" title=\"分解质因数法求组合数\"></a>分解质因数法求组合数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">当我们需要求出组合数的真实值，而非对某个数的余数时，分解质因数的方式比较好用：</span><br><span class=\"line\">    <span class=\"number\">1.</span> 筛法求出范围内的所有质数</span><br><span class=\"line\">    <span class=\"number\">2.</span> 通过 <span class=\"built_in\">C</span>(a, b) = a! / b! / (a - b)! 这个公式求出每个质因子的次数。 n! 中p的次数是 n / p + n / p^<span class=\"number\">2</span> + n / p^<span class=\"number\">3</span> + ...</span><br><span class=\"line\">    <span class=\"number\">3.</span> 用高精度乘法将所有质因子相乘</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// 存储所有质数</span></span><br><span class=\"line\"><span class=\"type\">int</span> sum[N];     <span class=\"comment\">// 存储每个质数的次数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个数是否已被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span>      <span class=\"comment\">// 线性筛法求素数</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i]) primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[primes[j] * i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">get</span><span class=\"params\">(<span class=\"type\">int</span> n, <span class=\"type\">int</span> p)</span>       <span class=\"comment\">// 求n！中的次数</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        res += n / p;</span><br><span class=\"line\">        n /= p;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">mul</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; a, <span class=\"type\">int</span> b)</span>       <span class=\"comment\">// 高精度乘低精度模板</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; c;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; a.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t += a[i] * b;</span><br><span class=\"line\">        c.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (t)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> c;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">get_primes</span>(a);  <span class=\"comment\">// 预处理范围内的所有质数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; cnt; i ++ )     <span class=\"comment\">// 求每个质因数的次数</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = primes[i];</span><br><span class=\"line\">    sum[i] = <span class=\"built_in\">get</span>(a, p) - <span class=\"built_in\">get</span>(b, p) - <span class=\"built_in\">get</span>(a - b, p);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt; res;</span><br><span class=\"line\">res.<span class=\"built_in\">push_back</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; cnt; i ++ )     <span class=\"comment\">// 用高精度乘法将所有质因子相乘</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; sum[i]; j ++ )</span><br><span class=\"line\">        res = <span class=\"built_in\">mul</span>(res, primes[i]);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"卡特兰数\"><a href=\"#卡特兰数\" class=\"headerlink\" title=\"卡特兰数\"></a>卡特兰数</h3><p>给定n个0和n个1，它们按照某种顺序排成长度为2n的序列，满足任意前缀中0的个数都不少于1的个数的序列的数量为： Cat(n) &#x3D; C(2n, n) &#x2F; (n + 1)</p>\n","more":"<h3 id=\"质数\"><a href=\"#质数\" class=\"headerlink\" title=\"质数\"></a>质数</h3><p>对于<strong>大于一</strong>的整数，如果只包含一和本身这两个约数，它就是质数（也叫素数）</p>\n<h4 id=\"试除法\"><a href=\"#试除法\" class=\"headerlink\" title=\"试除法\"></a>试除法</h4><p>$O(\\sqrt n)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">is_prime</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &lt; <span class=\"number\">2</span>) <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"literal\">false</span>;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"literal\">true</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"试除法分解质因数\"><a href=\"#试除法分解质因数\" class=\"headerlink\" title=\"试除法分解质因数\"></a>试除法分解质因数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">N = p1^c1 * p2^c2 * ... *pk^ck</span><br></pre></td></tr></table></figure>\n\n<p>从小到大枚举每一个数</p>\n<p>得到每一对$(p,c)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">divide</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">// n中只有最多一个大于sqrt(n)的质因子</span></span><br><span class=\"line\">    <span class=\"comment\">// 枚举到sqrt(t)，最后一个特殊处理 O(sqrt(n))</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> c = <span class=\"number\">0</span>;</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (x % i == <span class=\"number\">0</span>) x /= i, c ++ ;</span><br><span class=\"line\">            cout &lt;&lt; i &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; c &lt;&lt; endl;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &gt; <span class=\"number\">1</span>) cout &lt;&lt; x &lt;&lt; <span class=\"string\">&#x27; &#x27;</span> &lt;&lt; <span class=\"number\">1</span> &lt;&lt; endl;</span><br><span class=\"line\">    cout &lt;&lt; endl;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"朴素筛法求素数\"><a href=\"#朴素筛法求素数\" class=\"headerlink\" title=\"朴素筛法求素数\"></a>朴素筛法求素数</h4><p>枚举每一个数，如果它没有被筛，则加入质数集合，并且把它的所有倍数都筛掉</p>\n<p>优化：埃氏筛法，只需要把质数的倍数筛掉</p>\n<p>$O(nloglogn)$</p>\n<p>质数定理：1-n中有$\\frac{n}{ln_{}{n}}$个质数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (st[i]) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\">        primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = i + i; j &lt;= n; j += i)</span><br><span class=\"line\">            st[j] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"线性筛法求素数\"><a href=\"#线性筛法求素数\" class=\"headerlink\" title=\"线性筛法求素数\"></a>线性筛法求素数</h4><p>$n$只会被他的最小质因子筛掉</p>\n<p>$O(n)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i]) primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[primes[j] * i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>) <span class=\"keyword\">break</span>; <span class=\"comment\">//此时primes[j]是i的最小质因子</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"约数\"><a href=\"#约数\" class=\"headerlink\" title=\"约数\"></a>约数</h3><h4 id=\"试除法求所有约数\"><a href=\"#试除法求所有约数\" class=\"headerlink\" title=\"试除法求所有约数\"></a>试除法求所有约数</h4><p>$O(\\sqrt{n})$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">get_divisors</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; res;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            res.<span class=\"built_in\">push_back</span>(i);</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i != x / i) res.<span class=\"built_in\">push_back</span>(x / i); <span class=\"comment\">//防止相同的数被push进去两倍，例如4*4=16</span></span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"built_in\">sort</span>(res.<span class=\"built_in\">begin</span>(), res.<span class=\"built_in\">end</span>());</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"约数个数和约数之和\"><a href=\"#约数个数和约数之和\" class=\"headerlink\" title=\"约数个数和约数之和\"></a>约数个数和约数之和</h4><p>如果 N &#x3D; p1^c1 * p2^c2 * … *pk^ck<br>约数个数： (c1 + 1) * (c2 + 1) * … * (ck + 1)<br>约数之和： (p1^0 + p1^1 + … + p1^c1) * … * (pk^0 + pk^1 + … + pk^ck)</p>\n<h4 id=\"欧几里得算法求最大公约数\"><a href=\"#欧几里得算法求最大公约数\" class=\"headerlink\" title=\"欧几里得算法求最大公约数\"></a>欧几里得算法求最大公约数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">gcd</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> b ? <span class=\"built_in\">gcd</span>(b, a % b) : a;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>可以使用库函数<code>__gcd(int a, int b)</code>，此外最小公倍数&#x3D;$\\frac{a  b}{gcd(a,b)}$</p>\n<h3 id=\"欧拉函数\"><a href=\"#欧拉函数\" class=\"headerlink\" title=\"欧拉函数\"></a>欧拉函数</h3><p>$\\phi(n)$：1-n中与n互质的数的个数</p>\n<p>$\\phi(n) &#x3D; n*(1-\\frac{1}{p_1})<em>(1-\\frac{1}{p_2})</em>…*(1-\\frac{1}{p_n})$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">phi</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = x;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= x / i; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (x % i == <span class=\"number\">0</span>)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            res = res / i * (i - <span class=\"number\">1</span>);</span><br><span class=\"line\">            <span class=\"keyword\">while</span> (x % i == <span class=\"number\">0</span>) x /= i;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (x &gt; <span class=\"number\">1</span>) res = res / x * (x - <span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"筛法求欧拉函数\"><a href=\"#筛法求欧拉函数\" class=\"headerlink\" title=\"筛法求欧拉函数\"></a>筛法求欧拉函数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// primes[]存储所有素数</span></span><br><span class=\"line\"><span class=\"type\">int</span> euler[N];           <span class=\"comment\">// 存储每个数的欧拉函数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];         <span class=\"comment\">// st[x]存储x是否被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_eulers</span><span class=\"params\">(<span class=\"type\">int</span> n)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    euler[<span class=\"number\">1</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            primes[cnt ++ ] = i;</span><br><span class=\"line\">            euler[i] = i - <span class=\"number\">1</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> t = primes[j] * i;</span><br><span class=\"line\">            st[t] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>)</span><br><span class=\"line\">            &#123;</span><br><span class=\"line\">                euler[t] = euler[i] * primes[j];</span><br><span class=\"line\">                <span class=\"keyword\">break</span>;</span><br><span class=\"line\">            &#125;</span><br><span class=\"line\">            euler[t] = euler[i] * (primes[j] - <span class=\"number\">1</span>);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"欧拉定理\"><a href=\"#欧拉定理\" class=\"headerlink\" title=\"欧拉定理\"></a>欧拉定理</h4><p>若$a$与$n$互质，则</p>\n<p>$a^{\\phi{(n)}} \\equiv 1 (mod \\ n)$</p>\n<h3 id=\"快速幂\"><a href=\"#快速幂\" class=\"headerlink\" title=\"快速幂\"></a>快速幂</h3><p>在$O(logk)$时间内求出求出$a^k mod p$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">qmi</span><span class=\"params\">(<span class=\"type\">int</span> m, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span> % p, t = m;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k&amp;<span class=\"number\">1</span>) res = res * t % p;</span><br><span class=\"line\">        t = t * t % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"扩展欧几里得算法\"><a href=\"#扩展欧几里得算法\" class=\"headerlink\" title=\"扩展欧几里得算法\"></a>扩展欧几里得算法</h3><p>裴蜀定理：对于正整数$a,b$，一定存在整数$x,y$，使得</p>\n<p>$ax+by &#x3D; gcd(a,b)$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 求x, y，使得ax + by = gcd(a, b)</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">exgcd</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> &amp;x, <span class=\"type\">int</span> &amp;y)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (!b)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        x = <span class=\"number\">1</span>; y = <span class=\"number\">0</span>;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"type\">int</span> d = <span class=\"built_in\">exgcd</span>(b, a % b, y, x);</span><br><span class=\"line\">    y -= (a/b) * x;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> d;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"中国剩余定理\"><a href=\"#中国剩余定理\" class=\"headerlink\" title=\"中国剩余定理\"></a>中国剩余定理</h3><p>给定一些两两互质的数$m_1,m_2,m_3,m_k$，求解线性同余方程组</p>\n<p>$x \\equiv a_1 (mod \\ m_1)$</p>\n<p>$x \\equiv a_2 (mod \\ m_2)$</p>\n<p>$…$</p>\n<p>$x \\equiv a_k (mod \\ m_k)$</p>\n<p>$M &#x3D; m_1 * m_2*…*m_k$</p>\n<p>$M_i &#x3D; \\frac{M}{m_i}$</p>\n<p>$M_i^{-1}$表示$M_i$模$m_i$的逆</p>\n<p>$x &#x3D; a_1M_1M_1^{-1}+a_2M_1M_2^{-1}+…+a_kM_1M_k^{-1}$</p>\n<h3 id=\"高斯消元\"><a href=\"#高斯消元\" class=\"headerlink\" title=\"高斯消元\"></a>高斯消元</h3><p>在$O(n^3)$内求解线性方程组</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// a[N][N]是增广矩阵</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">gauss</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> c, r;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (c = <span class=\"number\">0</span>, r = <span class=\"number\">0</span>; c &lt; n; c ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = r;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r; i &lt; n; i ++ )   <span class=\"comment\">// 找到绝对值最大的行</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][c]) &gt; <span class=\"built_in\">fabs</span>(a[t][c]))</span><br><span class=\"line\">                t = i;</span><br><span class=\"line\">\t\t<span class=\"comment\">//eps精度 1e-6</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[t][c]) &lt; eps) <span class=\"keyword\">continue</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = c; i &lt;= n; i ++ ) <span class=\"built_in\">swap</span>(a[t][i], a[r][i]);      <span class=\"comment\">// 将绝对值最大的行换到最顶端</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = n; i &gt;= c; i -- ) a[r][i] /= a[r][c];      <span class=\"comment\">// 将当前行的首位变成1</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r + <span class=\"number\">1</span>; i &lt; n; i ++ )       <span class=\"comment\">// 用当前行将下面所有的列消成0</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][c]) &gt; eps)</span><br><span class=\"line\">                <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = n; j &gt;= c; j -- )</span><br><span class=\"line\">                    a[i][j] -= a[r][j] * a[i][c];</span><br><span class=\"line\"></span><br><span class=\"line\">        r ++ ;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">if</span> (r &lt; n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = r; i &lt; n; i ++ )</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (<span class=\"built_in\">fabs</span>(a[i][n]) &gt; eps)</span><br><span class=\"line\">                <span class=\"keyword\">return</span> <span class=\"number\">2</span>; <span class=\"comment\">// 无解</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span> <span class=\"number\">1</span>; <span class=\"comment\">// 有无穷多组解</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">`</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = n - <span class=\"number\">1</span>; i &gt;= <span class=\"number\">0</span>; i -- )</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = i + <span class=\"number\">1</span>; j &lt; n; j ++ )</span><br><span class=\"line\">            <span class=\"comment\">//储存答案</span></span><br><span class=\"line\">            a[i][n] -= a[i][j] * a[j][n];</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>; <span class=\"comment\">// 有唯一解</span></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"组合数\"><a href=\"#组合数\" class=\"headerlink\" title=\"组合数\"></a>组合数</h3><ul>\n<li>$1 \\le b \\le a \\le 2000$ 递推 $N^2$</li>\n<li>$1 \\le b \\le a \\le 10^5$ 预处理 $NlogN$</li>\n<li>$1 \\le b \\le a \\le 10^{18}, 1 \\le p \\le 10^5$ 卢卡斯定理Lucas</li>\n<li></li>\n</ul>\n<p>组合数$C_{n}^{m}&#x3D;\\frac{n!}{m!(n-m)!} $</p>\n<h4 id=\"朴素求法\"><a href=\"#朴素求法\" class=\"headerlink\" title=\"朴素求法\"></a>朴素求法</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\">LL <span class=\"title\">C</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> b)</span></span>&#123;</span><br><span class=\"line\">    LL res = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span>(<span class=\"type\">int</span> i=a,j=<span class=\"number\">1</span>;j&lt;=b;i--,j++)&#123;</span><br><span class=\"line\">        res = res*i/j;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"递推法求组合数\"><a href=\"#递推法求组合数\" class=\"headerlink\" title=\"递推法求组合数\"></a>递推法求组合数</h4><p>$C_{a}^{b} &#x3D; C_{a-1}^{b} + C_{a-1}^{b-1}$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// c[a][b] 表示从a个苹果中选b个的方案数</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; N; i ++ )</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt;= i; j ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!j) c[i][j] = <span class=\"number\">1</span>;</span><br><span class=\"line\">        <span class=\"keyword\">else</span> c[i][j] = (c[i - <span class=\"number\">1</span>][j] + c[i - <span class=\"number\">1</span>][j - <span class=\"number\">1</span>]) % mod;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"通过预处理逆元的方式求组合数\"><a href=\"#通过预处理逆元的方式求组合数\" class=\"headerlink\" title=\"通过预处理逆元的方式求组合数\"></a>通过预处理逆元的方式求组合数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">首先预处理出所有阶乘取模的余数fact[N]，以及所有阶乘取模的逆元infact[N]</span><br><span class=\"line\">如果取模的数是质数，可以用费马小定理求逆元</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">qmi</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)</span>    <span class=\"comment\">// 快速幂模板</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k &amp; <span class=\"number\">1</span>) res = (LL)res * a % p;</span><br><span class=\"line\">        a = (LL)a * a % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 预处理阶乘的余数和阶乘逆元的余数</span></span><br><span class=\"line\">fact[<span class=\"number\">0</span>] = infact[<span class=\"number\">0</span>] = <span class=\"number\">1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt; N; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    fact[i] = (LL)fact[i - <span class=\"number\">1</span>] * i % mod;</span><br><span class=\"line\">    infact[i] = (LL)infact[i - <span class=\"number\">1</span>] * <span class=\"built_in\">qmi</span>(i, mod - <span class=\"number\">2</span>, mod) % mod;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"Lucas定理\"><a href=\"#Lucas定理\" class=\"headerlink\" title=\"Lucas定理\"></a>Lucas定理</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">若p是质数，则对于任意整数 <span class=\"number\">1</span> &lt;= m &lt;= n，有：</span><br><span class=\"line\">    <span class=\"built_in\">C</span>(n, m) = <span class=\"built_in\">C</span>(n % p, m % p) * <span class=\"built_in\">C</span>(n / p, m / p) (mod p)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> <span class=\"built_in\">qmi</span>(<span class=\"type\">int</span> a, <span class=\"type\">int</span> k, <span class=\"type\">int</span> p)  <span class=\"comment\">// 快速幂模板</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">1</span> % p;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (k)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (k &amp; <span class=\"number\">1</span>) res = (LL)res * a % p;</span><br><span class=\"line\">        a = (LL)a * a % p;</span><br><span class=\"line\">        k &gt;&gt;= <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">C</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b, <span class=\"type\">int</span> p)</span>  <span class=\"comment\">// 通过定理求组合数C(a, b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a &lt; b) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    LL x = <span class=\"number\">1</span>, y = <span class=\"number\">1</span>;  <span class=\"comment\">// x是分子，y是分母</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = a, j = <span class=\"number\">1</span>; j &lt;= b; i --, j ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        x = (LL)x * i % p;</span><br><span class=\"line\">        y = (LL) y * j % p;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x * (LL)<span class=\"built_in\">qmi</span>(y, p - <span class=\"number\">2</span>, p) % p;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">lucas</span><span class=\"params\">(LL a, LL b, <span class=\"type\">int</span> p)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (a &lt; p &amp;&amp; b &lt; p) <span class=\"keyword\">return</span> <span class=\"built_in\">C</span>(a, b, p);</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (LL)<span class=\"built_in\">C</span>(a % p, b % p, p) * <span class=\"built_in\">lucas</span>(a / p, b / p, p) % p;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"分解质因数法求组合数\"><a href=\"#分解质因数法求组合数\" class=\"headerlink\" title=\"分解质因数法求组合数\"></a>分解质因数法求组合数</h4><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">当我们需要求出组合数的真实值，而非对某个数的余数时，分解质因数的方式比较好用：</span><br><span class=\"line\">    <span class=\"number\">1.</span> 筛法求出范围内的所有质数</span><br><span class=\"line\">    <span class=\"number\">2.</span> 通过 <span class=\"built_in\">C</span>(a, b) = a! / b! / (a - b)! 这个公式求出每个质因子的次数。 n! 中p的次数是 n / p + n / p^<span class=\"number\">2</span> + n / p^<span class=\"number\">3</span> + ...</span><br><span class=\"line\">    <span class=\"number\">3.</span> 用高精度乘法将所有质因子相乘</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"type\">int</span> primes[N], cnt;     <span class=\"comment\">// 存储所有质数</span></span><br><span class=\"line\"><span class=\"type\">int</span> sum[N];     <span class=\"comment\">// 存储每个质数的次数</span></span><br><span class=\"line\"><span class=\"type\">bool</span> st[N];     <span class=\"comment\">// 存储每个数是否已被筛掉</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">get_primes</span><span class=\"params\">(<span class=\"type\">int</span> n)</span>      <span class=\"comment\">// 线性筛法求素数</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[i]) primes[cnt ++ ] = i;</span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; primes[j] &lt;= n / i; j ++ )</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[primes[j] * i] = <span class=\"literal\">true</span>;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (i % primes[j] == <span class=\"number\">0</span>) <span class=\"keyword\">break</span>;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">get</span><span class=\"params\">(<span class=\"type\">int</span> n, <span class=\"type\">int</span> p)</span>       <span class=\"comment\">// 求n！中的次数</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> res = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (n)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        res += n / p;</span><br><span class=\"line\">        n /= p;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> res;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"function\">vector&lt;<span class=\"type\">int</span>&gt; <span class=\"title\">mul</span><span class=\"params\">(vector&lt;<span class=\"type\">int</span>&gt; a, <span class=\"type\">int</span> b)</span>       <span class=\"comment\">// 高精度乘低精度模板</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    vector&lt;<span class=\"type\">int</span>&gt; c;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; a.<span class=\"built_in\">size</span>(); i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        t += a[i] * b;</span><br><span class=\"line\">        c.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (t)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        c.<span class=\"built_in\">push_back</span>(t % <span class=\"number\">10</span>);</span><br><span class=\"line\">        t /= <span class=\"number\">10</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">return</span> c;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">get_primes</span>(a);  <span class=\"comment\">// 预处理范围内的所有质数</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; cnt; i ++ )     <span class=\"comment\">// 求每个质因数的次数</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = primes[i];</span><br><span class=\"line\">    sum[i] = <span class=\"built_in\">get</span>(a, p) - <span class=\"built_in\">get</span>(b, p) - <span class=\"built_in\">get</span>(a - b, p);</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\">vector&lt;<span class=\"type\">int</span>&gt; res;</span><br><span class=\"line\">res.<span class=\"built_in\">push_back</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; cnt; i ++ )     <span class=\"comment\">// 用高精度乘法将所有质因子相乘</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> j = <span class=\"number\">0</span>; j &lt; sum[i]; j ++ )</span><br><span class=\"line\">        res = <span class=\"built_in\">mul</span>(res, primes[i]);</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"卡特兰数\"><a href=\"#卡特兰数\" class=\"headerlink\" title=\"卡特兰数\"></a>卡特兰数</h3><p>给定n个0和n个1，它们按照某种顺序排成长度为2n的序列，满足任意前缀中0的个数都不少于1的个数的序列的数量为： Cat(n) &#x3D; C(2n, n) &#x2F; (n + 1)</p>\n"},{"title":"Algorithm-Search","mathjax":true,"date":"2023-08-03T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛中的搜索问题","_content":"\n### DFS与BFS\n\n- 深度优先搜索（DFS）\n\n用$Stack$递归，空间$O(h)$，不具有最短性\n\n- 宽度优先搜索（BFS）\n\n用$Queue$，空间$O(2^h)$，“最短路”\n\n**回溯、剪枝**\n\n在矩阵中4个方向遍历\n\n```cpp\nint dx[] = {1,0,-1,0},y = {0,1,0,-1};\n```\n\n防止走相反的方向导致搜索回溯\n\n```cpp\nif(i ^ 2 == d) continue;\n```\n\n8个方向遍历\n\n```cpp\nint dx[8] = {-1, -1, -1, 0, 1, 1, 1, 0};\nint dy[8] = {-1, 0, 1, 1, 1, 0, -1, -1};\n```\n\n防止走相反的方向导致搜索回溯\n\n```cpp\nif(i ^ 4 == d) continue;\n```\n\n### 树和图的存储\n\n树是特殊的无环连通图\n\n**有向图$a \\to b$**\n\n- 邻接矩阵 $g[a][b]$\n- 邻接表，用链表储存点$i$可以到达的点\n\n```cpp\n// 对于每个点k，开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点\nint h[N], e[N], ne[N], idx;\n\n// 添加一条边a->b\nvoid add(int a, int b)\n{\n    e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;\n}\n\n// 初始化\nidx = 0;\nmemset(h, -1, sizeof h);\n```\n\n#### 树和图的遍历\n\n时间复杂度$O(n+m)$，n表示点数，m表示边数\n\n- 深度优先遍历\n\n```cpp\nint dfs(int u)\n{\n    st[u] = true; // st[u] 表示点u已经被遍历过\n\n    for (int i = h[u]; i != -1; i = ne[i])\n    {\n        int j = e[i];\n        if (!st[j]) dfs(j);\n    }\n}\n```\n\n- 宽度优先遍历\n\n```cpp\nqueue<int> q;\nst[1] = true; // 表示1号点已经被遍历过\nq.push(1);\n\nwhile (q.size())\n{\n    int t = q.front();\n    q.pop();\n\n    for (int i = h[t]; i != -1; i = ne[i])\n    {\n        int j = e[i];\n        if (!st[j])\n        {\n            st[j] = true; // 表示点j已经被遍历过\n            q.push(j);\n        }\n    }\n}\n```\n\n#### 拓扑排序\n\n时间复杂度$O(n+m)$，n表示点数，m表示边数\n\n有向无环图一定可以拓扑排序，序列可能不唯一\n\n**入度、出度**：有多少条边指向自己/从自己这里指出去\n\n1. 将入度为0的点入队\n2. 宽搜，枚举队头的所有出边$t \\to j$，删掉$t \\to j$，$t$的出度减一\n\n```cpp\nbool topsort()\n{\n    int hh = 0, tt = -1;\n\n    // d[i] 存储点i的入度\n    for (int i = 1; i <= n; i ++ )\n        if (!d[i])\n            q[ ++ tt] = i;\n\n    while (hh <= tt)\n    {\n        int t = q[hh ++ ];\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            d[j]--;\n            if (d[j] == 0)\n                q[ ++ tt] = j;\n        }\n    }\n\n    // 如果所有点都入队了，说明存在拓扑序列；否则不存在拓扑序列。\n    return tt == n - 1;\n}\n```\n\n**一个有向无环图至少有一个入度为0的点**","source":"_posts/algorithm-search.md","raw":"---\ntitle: Algorithm-Search\nmathjax: true\ndate: 2023/08/03 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛中的搜索问题\n---\n\n### DFS与BFS\n\n- 深度优先搜索（DFS）\n\n用$Stack$递归，空间$O(h)$，不具有最短性\n\n- 宽度优先搜索（BFS）\n\n用$Queue$，空间$O(2^h)$，“最短路”\n\n**回溯、剪枝**\n\n在矩阵中4个方向遍历\n\n```cpp\nint dx[] = {1,0,-1,0},y = {0,1,0,-1};\n```\n\n防止走相反的方向导致搜索回溯\n\n```cpp\nif(i ^ 2 == d) continue;\n```\n\n8个方向遍历\n\n```cpp\nint dx[8] = {-1, -1, -1, 0, 1, 1, 1, 0};\nint dy[8] = {-1, 0, 1, 1, 1, 0, -1, -1};\n```\n\n防止走相反的方向导致搜索回溯\n\n```cpp\nif(i ^ 4 == d) continue;\n```\n\n### 树和图的存储\n\n树是特殊的无环连通图\n\n**有向图$a \\to b$**\n\n- 邻接矩阵 $g[a][b]$\n- 邻接表，用链表储存点$i$可以到达的点\n\n```cpp\n// 对于每个点k，开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点\nint h[N], e[N], ne[N], idx;\n\n// 添加一条边a->b\nvoid add(int a, int b)\n{\n    e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;\n}\n\n// 初始化\nidx = 0;\nmemset(h, -1, sizeof h);\n```\n\n#### 树和图的遍历\n\n时间复杂度$O(n+m)$，n表示点数，m表示边数\n\n- 深度优先遍历\n\n```cpp\nint dfs(int u)\n{\n    st[u] = true; // st[u] 表示点u已经被遍历过\n\n    for (int i = h[u]; i != -1; i = ne[i])\n    {\n        int j = e[i];\n        if (!st[j]) dfs(j);\n    }\n}\n```\n\n- 宽度优先遍历\n\n```cpp\nqueue<int> q;\nst[1] = true; // 表示1号点已经被遍历过\nq.push(1);\n\nwhile (q.size())\n{\n    int t = q.front();\n    q.pop();\n\n    for (int i = h[t]; i != -1; i = ne[i])\n    {\n        int j = e[i];\n        if (!st[j])\n        {\n            st[j] = true; // 表示点j已经被遍历过\n            q.push(j);\n        }\n    }\n}\n```\n\n#### 拓扑排序\n\n时间复杂度$O(n+m)$，n表示点数，m表示边数\n\n有向无环图一定可以拓扑排序，序列可能不唯一\n\n**入度、出度**：有多少条边指向自己/从自己这里指出去\n\n1. 将入度为0的点入队\n2. 宽搜，枚举队头的所有出边$t \\to j$，删掉$t \\to j$，$t$的出度减一\n\n```cpp\nbool topsort()\n{\n    int hh = 0, tt = -1;\n\n    // d[i] 存储点i的入度\n    for (int i = 1; i <= n; i ++ )\n        if (!d[i])\n            q[ ++ tt] = i;\n\n    while (hh <= tt)\n    {\n        int t = q[hh ++ ];\n\n        for (int i = h[t]; i != -1; i = ne[i])\n        {\n            int j = e[i];\n            d[j]--;\n            if (d[j] == 0)\n                q[ ++ tt] = j;\n        }\n    }\n\n    // 如果所有点都入队了，说明存在拓扑序列；否则不存在拓扑序列。\n    return tt == n - 1;\n}\n```\n\n**一个有向无环图至少有一个入度为0的点**","slug":"algorithm-search","published":1,"updated":"2025-02-28T03:12:34.855Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9e0008bs991s689q4d","content":"<h3 id=\"DFS与BFS\"><a href=\"#DFS与BFS\" class=\"headerlink\" title=\"DFS与BFS\"></a>DFS与BFS</h3><ul>\n<li>深度优先搜索（DFS）</li>\n</ul>\n<p>用$Stack$递归，空间$O(h)$，不具有最短性</p>\n<ul>\n<li>宽度优先搜索（BFS）</li>\n</ul>\n<p>用$Queue$，空间$O(2^h)$，“最短路”</p>\n<p><strong>回溯、剪枝</strong></p>\n<p>在矩阵中4个方向遍历</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> dx[] = &#123;<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>,<span class=\"number\">0</span>&#125;,y = &#123;<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>防止走相反的方向导致搜索回溯</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(i ^ <span class=\"number\">2</span> == d) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n\n<p>8个方向遍历</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> dx[<span class=\"number\">8</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">-1</span>, <span class=\"number\">-1</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"type\">int</span> dy[<span class=\"number\">8</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>, <span class=\"number\">-1</span>&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>防止走相反的方向导致搜索回溯</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(i ^ <span class=\"number\">4</span> == d) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"树和图的存储\"><a href=\"#树和图的存储\" class=\"headerlink\" title=\"树和图的存储\"></a>树和图的存储</h3><p>树是特殊的无环连通图</p>\n<p><strong>有向图$a \\to b$</strong></p>\n<ul>\n<li>邻接矩阵 $g[a][b]$</li>\n<li>邻接表，用链表储存点$i$可以到达的点</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 对于每个点k，开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 添加一条边a-&gt;b</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\">idx = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"built_in\">memset</span>(h, <span class=\"number\">-1</span>, <span class=\"keyword\">sizeof</span> h);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"树和图的遍历\"><a href=\"#树和图的遍历\" class=\"headerlink\" title=\"树和图的遍历\"></a>树和图的遍历</h4><p>时间复杂度$O(n+m)$，n表示点数，m表示边数</p>\n<ul>\n<li>深度优先遍历</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"type\">int</span> u)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    st[u] = <span class=\"literal\">true</span>; <span class=\"comment\">// st[u] 表示点u已经被遍历过</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[u]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[j]) <span class=\"built_in\">dfs</span>(j);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>宽度优先遍历</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">st[<span class=\"number\">1</span>] = <span class=\"literal\">true</span>; <span class=\"comment\">// 表示1号点已经被遍历过</span></span><br><span class=\"line\">q.<span class=\"built_in\">push</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">    q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[j])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[j] = <span class=\"literal\">true</span>; <span class=\"comment\">// 表示点j已经被遍历过</span></span><br><span class=\"line\">            q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"拓扑排序\"><a href=\"#拓扑排序\" class=\"headerlink\" title=\"拓扑排序\"></a>拓扑排序</h4><p>时间复杂度$O(n+m)$，n表示点数，m表示边数</p>\n<p>有向无环图一定可以拓扑排序，序列可能不唯一</p>\n<p><strong>入度、出度</strong>：有多少条边指向自己&#x2F;从自己这里指出去</p>\n<ol>\n<li>将入度为0的点入队</li>\n<li>宽搜，枚举队头的所有出边$t \\to j$，删掉$t \\to j$，$t$的出度减一</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">topsort</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// d[i] 存储点i的入度</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!d[i])</span><br><span class=\"line\">            q[ ++ tt] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = q[hh ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            d[j]--;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (d[j] == <span class=\"number\">0</span>)</span><br><span class=\"line\">                q[ ++ tt] = j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果所有点都入队了，说明存在拓扑序列；否则不存在拓扑序列。</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tt == n - <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>一个有向无环图至少有一个入度为0的点</strong></p>\n","more":"<h3 id=\"DFS与BFS\"><a href=\"#DFS与BFS\" class=\"headerlink\" title=\"DFS与BFS\"></a>DFS与BFS</h3><ul>\n<li>深度优先搜索（DFS）</li>\n</ul>\n<p>用$Stack$递归，空间$O(h)$，不具有最短性</p>\n<ul>\n<li>宽度优先搜索（BFS）</li>\n</ul>\n<p>用$Queue$，空间$O(2^h)$，“最短路”</p>\n<p><strong>回溯、剪枝</strong></p>\n<p>在矩阵中4个方向遍历</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> dx[] = &#123;<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>,<span class=\"number\">0</span>&#125;,y = &#123;<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">-1</span>&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>防止走相反的方向导致搜索回溯</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(i ^ <span class=\"number\">2</span> == d) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n\n<p>8个方向遍历</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> dx[<span class=\"number\">8</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">-1</span>, <span class=\"number\">-1</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>&#125;;</span><br><span class=\"line\"><span class=\"type\">int</span> dy[<span class=\"number\">8</span>] = &#123;<span class=\"number\">-1</span>, <span class=\"number\">0</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">1</span>, <span class=\"number\">0</span>, <span class=\"number\">-1</span>, <span class=\"number\">-1</span>&#125;;</span><br></pre></td></tr></table></figure>\n\n<p>防止走相反的方向导致搜索回溯</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">if</span>(i ^ <span class=\"number\">4</span> == d) <span class=\"keyword\">continue</span>;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"树和图的存储\"><a href=\"#树和图的存储\" class=\"headerlink\" title=\"树和图的存储\"></a>树和图的存储</h3><p>树是特殊的无环连通图</p>\n<p><strong>有向图$a \\to b$</strong></p>\n<ul>\n<li>邻接矩阵 $g[a][b]$</li>\n<li>邻接表，用链表储存点$i$可以到达的点</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// 对于每个点k，开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点</span></span><br><span class=\"line\"><span class=\"type\">int</span> h[N], e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 添加一条边a-&gt;b</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">add</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> b)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\">idx = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"built_in\">memset</span>(h, <span class=\"number\">-1</span>, <span class=\"keyword\">sizeof</span> h);</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"树和图的遍历\"><a href=\"#树和图的遍历\" class=\"headerlink\" title=\"树和图的遍历\"></a>树和图的遍历</h4><p>时间复杂度$O(n+m)$，n表示点数，m表示边数</p>\n<ul>\n<li>深度优先遍历</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">dfs</span><span class=\"params\">(<span class=\"type\">int</span> u)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    st[u] = <span class=\"literal\">true</span>; <span class=\"comment\">// st[u] 表示点u已经被遍历过</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[u]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[j]) <span class=\"built_in\">dfs</span>(j);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>宽度优先遍历</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">queue&lt;<span class=\"type\">int</span>&gt; q;</span><br><span class=\"line\">st[<span class=\"number\">1</span>] = <span class=\"literal\">true</span>; <span class=\"comment\">// 表示1号点已经被遍历过</span></span><br><span class=\"line\">q.<span class=\"built_in\">push</span>(<span class=\"number\">1</span>);</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">while</span> (q.<span class=\"built_in\">size</span>())</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> t = q.<span class=\"built_in\">front</span>();</span><br><span class=\"line\">    q.<span class=\"built_in\">pop</span>();</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!st[j])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            st[j] = <span class=\"literal\">true</span>; <span class=\"comment\">// 表示点j已经被遍历过</span></span><br><span class=\"line\">            q.<span class=\"built_in\">push</span>(j);</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"拓扑排序\"><a href=\"#拓扑排序\" class=\"headerlink\" title=\"拓扑排序\"></a>拓扑排序</h4><p>时间复杂度$O(n+m)$，n表示点数，m表示边数</p>\n<p>有向无环图一定可以拓扑排序，序列可能不唯一</p>\n<p><strong>入度、出度</strong>：有多少条边指向自己&#x2F;从自己这里指出去</p>\n<ol>\n<li>将入度为0的点入队</li>\n<li>宽搜，枚举队头的所有出边$t \\to j$，删掉$t \\to j$，$t$的出度减一</li>\n</ol>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"function\"><span class=\"type\">bool</span> <span class=\"title\">topsort</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// d[i] 存储点i的入度</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!d[i])</span><br><span class=\"line\">            q[ ++ tt] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> t = q[hh ++ ];</span><br><span class=\"line\"></span><br><span class=\"line\">        <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = h[t]; i != <span class=\"number\">-1</span>; i = ne[i])</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> j = e[i];</span><br><span class=\"line\">            d[j]--;</span><br><span class=\"line\">            <span class=\"keyword\">if</span> (d[j] == <span class=\"number\">0</span>)</span><br><span class=\"line\">                q[ ++ tt] = j;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 如果所有点都入队了，说明存在拓扑序列；否则不存在拓扑序列。</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> tt == n - <span class=\"number\">1</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><strong>一个有向无环图至少有一个入度为0的点</strong></p>\n"},{"title":"Algorithm-Skills","mathjax":true,"date":"2023-10-22T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛中的一些技巧","_content":"\n一般竞赛时间限制在1s或2s，所以时间复杂度尽量控制在$10^7 - 10^8$\n\n下面给出在不同数据范围下，代码的时间复杂度和算法如何选择：\n\n1. $n \\le 30$，dfs+剪枝、状态压缩dp\n2. $n \\le 100$，$O(n^3)$，Floyd、dp、高斯消元\n3. $n \\le 1000$，$O(n^2)$，dp、二分、朴素Dijsktra、朴素Prim、Bellman-Ford\n4. $n \\le 10000$，$O(n \\sqrt n)$，块状链表、分块、莫队\n5. $n \\le 10^5$，$O(nlogn)$，sort、线段树、树状数组、set/map、heap、拓扑排序、堆优化Dijkstra、堆优化Prim、Kruskal、spfa、二分、CDQ分治、整体二分、后缀数组\n6. $n \\le 10^6$，$O(n)$，单调队列、hash、双指针、BFS、并查集、kmp、AC自动机；常数比较小的$O(nlogn)$，sort、树状数组、heap、dijkstra、prim\n7. $n \\le 10^7$，$O(n)$，双指针扫描、Kmp、AC自动机、线性筛素数\n8. $n \\le 10^9$，$O(\\sqrt n)$，判断质数\n9. $n \\le 10^{18}$，$O(nlogn)$，最大公约数、快速幂、数位dp\n10. $n \\le 10^{1000}$，$O((logn)^2)$，高精度加减乘除\n\n一些常见数据类型的大小\n\n1. long long 内的最大阶乘 20!\n2. int 内的最大阶乘 12!\n3. int => $2^{31}$，$2*10^9$\n4. long long => $2^{63}$，$9*10^{18}$\n5. float => 38位\n6. double => 308位\n\nmemset常赋值：-1，0，0x3f，-0x3f\n\n无穷大：0x3f3f3f3f\n\n`cout`相关：\n\n- 设置场宽: `left(right)<<setw()`\n- 设置精度:`fixed<<setprecision()`\n- 此时要导入头文件`#include <iomanip>`\n\n`cin`相关：\n\n- 读入整行:`cin.getline(c,N,'\\n')` c表示目标char数组，N表示长度，'\\n’表示结束符\n\n结构体小于号重载\n\n```cpp\nstruct s{\n    int a;\n    string b;\n    bool operator< (const s &ss) const{\n        return a < ss.a\n\t}\n}\n```\n\n`string`相关：\n\n```cpp\nstring s = \"I love China\";\ns.substr(start,len); //取子串\nchar c = s.c_str(); //转成char数组\nstrstr(s.c_str(),\"love\") //kmp，返回出现以后的子串，这里返回\"love China\"\ns.find(\"China\") //kmp,返回第一次出现的下标，不存在则返回s.npos\n//与int\nstring s = to_string(i);\nint i = stoi(s);\n```","source":"_posts/algorithm-skills.md","raw":"---\ntitle: Algorithm-Skills\nmathjax: true\ndate: 2023/10/22 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛中的一些技巧\n---\n\n一般竞赛时间限制在1s或2s，所以时间复杂度尽量控制在$10^7 - 10^8$\n\n下面给出在不同数据范围下，代码的时间复杂度和算法如何选择：\n\n1. $n \\le 30$，dfs+剪枝、状态压缩dp\n2. $n \\le 100$，$O(n^3)$，Floyd、dp、高斯消元\n3. $n \\le 1000$，$O(n^2)$，dp、二分、朴素Dijsktra、朴素Prim、Bellman-Ford\n4. $n \\le 10000$，$O(n \\sqrt n)$，块状链表、分块、莫队\n5. $n \\le 10^5$，$O(nlogn)$，sort、线段树、树状数组、set/map、heap、拓扑排序、堆优化Dijkstra、堆优化Prim、Kruskal、spfa、二分、CDQ分治、整体二分、后缀数组\n6. $n \\le 10^6$，$O(n)$，单调队列、hash、双指针、BFS、并查集、kmp、AC自动机；常数比较小的$O(nlogn)$，sort、树状数组、heap、dijkstra、prim\n7. $n \\le 10^7$，$O(n)$，双指针扫描、Kmp、AC自动机、线性筛素数\n8. $n \\le 10^9$，$O(\\sqrt n)$，判断质数\n9. $n \\le 10^{18}$，$O(nlogn)$，最大公约数、快速幂、数位dp\n10. $n \\le 10^{1000}$，$O((logn)^2)$，高精度加减乘除\n\n一些常见数据类型的大小\n\n1. long long 内的最大阶乘 20!\n2. int 内的最大阶乘 12!\n3. int => $2^{31}$，$2*10^9$\n4. long long => $2^{63}$，$9*10^{18}$\n5. float => 38位\n6. double => 308位\n\nmemset常赋值：-1，0，0x3f，-0x3f\n\n无穷大：0x3f3f3f3f\n\n`cout`相关：\n\n- 设置场宽: `left(right)<<setw()`\n- 设置精度:`fixed<<setprecision()`\n- 此时要导入头文件`#include <iomanip>`\n\n`cin`相关：\n\n- 读入整行:`cin.getline(c,N,'\\n')` c表示目标char数组，N表示长度，'\\n’表示结束符\n\n结构体小于号重载\n\n```cpp\nstruct s{\n    int a;\n    string b;\n    bool operator< (const s &ss) const{\n        return a < ss.a\n\t}\n}\n```\n\n`string`相关：\n\n```cpp\nstring s = \"I love China\";\ns.substr(start,len); //取子串\nchar c = s.c_str(); //转成char数组\nstrstr(s.c_str(),\"love\") //kmp，返回出现以后的子串，这里返回\"love China\"\ns.find(\"China\") //kmp,返回第一次出现的下标，不存在则返回s.npos\n//与int\nstring s = to_string(i);\nint i = stoi(s);\n```","slug":"algorithm-skills","published":1,"updated":"2025-02-28T03:00:25.621Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9e0009bs9961rfeuoj","content":"<p>一般竞赛时间限制在1s或2s，所以时间复杂度尽量控制在$10^7 - 10^8$</p>\n<p>下面给出在不同数据范围下，代码的时间复杂度和算法如何选择：</p>\n<ol>\n<li>$n \\le 30$，dfs+剪枝、状态压缩dp</li>\n<li>$n \\le 100$，$O(n^3)$，Floyd、dp、高斯消元</li>\n<li>$n \\le 1000$，$O(n^2)$，dp、二分、朴素Dijsktra、朴素Prim、Bellman-Ford</li>\n<li>$n \\le 10000$，$O(n \\sqrt n)$，块状链表、分块、莫队</li>\n<li>$n \\le 10^5$，$O(nlogn)$，sort、线段树、树状数组、set&#x2F;map、heap、拓扑排序、堆优化Dijkstra、堆优化Prim、Kruskal、spfa、二分、CDQ分治、整体二分、后缀数组</li>\n<li>$n \\le 10^6$，$O(n)$，单调队列、hash、双指针、BFS、并查集、kmp、AC自动机；常数比较小的$O(nlogn)$，sort、树状数组、heap、dijkstra、prim</li>\n<li>$n \\le 10^7$，$O(n)$，双指针扫描、Kmp、AC自动机、线性筛素数</li>\n<li>$n \\le 10^9$，$O(\\sqrt n)$，判断质数</li>\n<li>$n \\le 10^{18}$，$O(nlogn)$，最大公约数、快速幂、数位dp</li>\n<li>$n \\le 10^{1000}$，$O((logn)^2)$，高精度加减乘除</li>\n</ol>\n<p>一些常见数据类型的大小</p>\n<ol>\n<li>long long 内的最大阶乘 20!</li>\n<li>int 内的最大阶乘 12!</li>\n<li>int &#x3D;&gt; $2^{31}$，$2*10^9$</li>\n<li>long long &#x3D;&gt; $2^{63}$，$9*10^{18}$</li>\n<li>float &#x3D;&gt; 38位</li>\n<li>double &#x3D;&gt; 308位</li>\n</ol>\n<p>memset常赋值：-1，0，0x3f，-0x3f</p>\n<p>无穷大：0x3f3f3f3f</p>\n<p><code>cout</code>相关：</p>\n<ul>\n<li>设置场宽: <code>left(right)&lt;&lt;setw()</code></li>\n<li>设置精度:<code>fixed&lt;&lt;setprecision()</code></li>\n<li>此时要导入头文件<code>#include &lt;iomanip&gt;</code></li>\n</ul>\n<p><code>cin</code>相关：</p>\n<ul>\n<li>读入整行:<code>cin.getline(c,N,&#39;\\n&#39;)</code> c表示目标char数组，N表示长度，’\\n’表示结束符</li>\n</ul>\n<p>结构体小于号重载</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">s</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a;</span><br><span class=\"line\">    string b;</span><br><span class=\"line\">    <span class=\"type\">bool</span> <span class=\"keyword\">operator</span>&lt; (<span class=\"type\">const</span> s &amp;ss) <span class=\"type\">const</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a &lt; ss.a</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><code>string</code>相关：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">string s = <span class=\"string\">&quot;I love China&quot;</span>;</span><br><span class=\"line\">s.<span class=\"built_in\">substr</span>(start,len); <span class=\"comment\">//取子串</span></span><br><span class=\"line\"><span class=\"type\">char</span> c = s.<span class=\"built_in\">c_str</span>(); <span class=\"comment\">//转成char数组</span></span><br><span class=\"line\"><span class=\"built_in\">strstr</span>(s.<span class=\"built_in\">c_str</span>(),<span class=\"string\">&quot;love&quot;</span>) <span class=\"comment\">//kmp，返回出现以后的子串，这里返回&quot;love China&quot;</span></span><br><span class=\"line\">s.<span class=\"built_in\">find</span>(<span class=\"string\">&quot;China&quot;</span>) <span class=\"comment\">//kmp,返回第一次出现的下标，不存在则返回s.npos</span></span><br><span class=\"line\"><span class=\"comment\">//与int</span></span><br><span class=\"line\">string s = <span class=\"built_in\">to_string</span>(i);</span><br><span class=\"line\"><span class=\"type\">int</span> i = <span class=\"built_in\">stoi</span>(s);</span><br></pre></td></tr></table></figure>","more":"<p>一般竞赛时间限制在1s或2s，所以时间复杂度尽量控制在$10^7 - 10^8$</p>\n<p>下面给出在不同数据范围下，代码的时间复杂度和算法如何选择：</p>\n<ol>\n<li>$n \\le 30$，dfs+剪枝、状态压缩dp</li>\n<li>$n \\le 100$，$O(n^3)$，Floyd、dp、高斯消元</li>\n<li>$n \\le 1000$，$O(n^2)$，dp、二分、朴素Dijsktra、朴素Prim、Bellman-Ford</li>\n<li>$n \\le 10000$，$O(n \\sqrt n)$，块状链表、分块、莫队</li>\n<li>$n \\le 10^5$，$O(nlogn)$，sort、线段树、树状数组、set&#x2F;map、heap、拓扑排序、堆优化Dijkstra、堆优化Prim、Kruskal、spfa、二分、CDQ分治、整体二分、后缀数组</li>\n<li>$n \\le 10^6$，$O(n)$，单调队列、hash、双指针、BFS、并查集、kmp、AC自动机；常数比较小的$O(nlogn)$，sort、树状数组、heap、dijkstra、prim</li>\n<li>$n \\le 10^7$，$O(n)$，双指针扫描、Kmp、AC自动机、线性筛素数</li>\n<li>$n \\le 10^9$，$O(\\sqrt n)$，判断质数</li>\n<li>$n \\le 10^{18}$，$O(nlogn)$，最大公约数、快速幂、数位dp</li>\n<li>$n \\le 10^{1000}$，$O((logn)^2)$，高精度加减乘除</li>\n</ol>\n<p>一些常见数据类型的大小</p>\n<ol>\n<li>long long 内的最大阶乘 20!</li>\n<li>int 内的最大阶乘 12!</li>\n<li>int &#x3D;&gt; $2^{31}$，$2*10^9$</li>\n<li>long long &#x3D;&gt; $2^{63}$，$9*10^{18}$</li>\n<li>float &#x3D;&gt; 38位</li>\n<li>double &#x3D;&gt; 308位</li>\n</ol>\n<p>memset常赋值：-1，0，0x3f，-0x3f</p>\n<p>无穷大：0x3f3f3f3f</p>\n<p><code>cout</code>相关：</p>\n<ul>\n<li>设置场宽: <code>left(right)&lt;&lt;setw()</code></li>\n<li>设置精度:<code>fixed&lt;&lt;setprecision()</code></li>\n<li>此时要导入头文件<code>#include &lt;iomanip&gt;</code></li>\n</ul>\n<p><code>cin</code>相关：</p>\n<ul>\n<li>读入整行:<code>cin.getline(c,N,&#39;\\n&#39;)</code> c表示目标char数组，N表示长度，’\\n’表示结束符</li>\n</ul>\n<p>结构体小于号重载</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">struct</span> <span class=\"title class_\">s</span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> a;</span><br><span class=\"line\">    string b;</span><br><span class=\"line\">    <span class=\"type\">bool</span> <span class=\"keyword\">operator</span>&lt; (<span class=\"type\">const</span> s &amp;ss) <span class=\"type\">const</span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> a &lt; ss.a</span><br><span class=\"line\">\t&#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><code>string</code>相关：</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">string s = <span class=\"string\">&quot;I love China&quot;</span>;</span><br><span class=\"line\">s.<span class=\"built_in\">substr</span>(start,len); <span class=\"comment\">//取子串</span></span><br><span class=\"line\"><span class=\"type\">char</span> c = s.<span class=\"built_in\">c_str</span>(); <span class=\"comment\">//转成char数组</span></span><br><span class=\"line\"><span class=\"built_in\">strstr</span>(s.<span class=\"built_in\">c_str</span>(),<span class=\"string\">&quot;love&quot;</span>) <span class=\"comment\">//kmp，返回出现以后的子串，这里返回&quot;love China&quot;</span></span><br><span class=\"line\">s.<span class=\"built_in\">find</span>(<span class=\"string\">&quot;China&quot;</span>) <span class=\"comment\">//kmp,返回第一次出现的下标，不存在则返回s.npos</span></span><br><span class=\"line\"><span class=\"comment\">//与int</span></span><br><span class=\"line\">string s = <span class=\"built_in\">to_string</span>(i);</span><br><span class=\"line\"><span class=\"type\">int</span> i = <span class=\"built_in\">stoi</span>(s);</span><br></pre></td></tr></table></figure>"},{"title":"dive-into-deep-learning-notes","mathjax":true,"date":"2024-01-22T12:46:25.000Z","img":"https://zh.d2l.ai/_static/logo-with-text.png","excerpt":"动手深度学习笔记","_content":"# 1. 预备知识\n\n## 1.1 数据处理\n\n`Tensor`数据类型和numpy中的`ndarray`类型相似，但是差异点在于\n\n首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；\n\n其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习\n\n对于任意具有相同形状的张量， 常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。\n\n我们也可以把多个张量*连结*（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。\n\n```python\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n```\n\n对张量中的所有元素进行求和，会产生一个单元素张量。\n\n```python\ntorch.sum(x)\n```\n\n在某些情况下，即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n\n1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n2. 对生成的数组执行按元素操作。\n\n将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n\n```python\nA = X.numpy()\nB = torch.tensor(A)\n```\n\n要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。\n\n```python\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n```\n\n## 1.2 线性代数\n\n- *Hadamard*积\n\n两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号⊙）\n$$\nA⊙B = \\begin{bmatrix} a_{11}b_{11} & a_{12}b_{12} &a_{13}b_{13} \\\\ a_{21}b_{21} & a_{22}b_{22} &a_{23}b_{23} \\\\ a_{31}b_{31}&a_{32}b_{32} &a_{3,3}b_{33} \\end{bmatrix}\n$$\n\n\n将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n\n- 降维求和\n\n 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。\n\nsum,mean都是同理的\n\n- 非降维求和\n\n如果我们想沿某个轴计算`A`元素的累积总和， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。\n\n```python\nA.cumsum(axis=0)\n```\n\n- 点积\n\n给定两个向量$x,y$的点积$x^Ty$(或$<x,y>$)是相同位置的按元素乘积的和\n$$\nx^Ty = \\sum_{i=1}^{d}x_iy_i\n$$\n\n\n将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。\n\n```python\ntorch.dot(x, y)\n```\n\n- 矩阵-向量积\n\n$$\nAx = \\begin{bmatrix} a_1^T \\\\ a_2^T \\\\ a_3^T \\end{bmatrix}x = \\begin{bmatrix} a_1^Tx \\\\ a_2^Tx \\\\ a_3^Tx \\end{bmatrix}\n$$\n\n\n\n在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。\n\n```python\ntorch.mv(A, x)\n```\n\n- 矩阵-矩阵乘法\n\n用行向量$A_i^T$表示矩阵$A$的第$i$行，列向量$b_j$作为矩阵$B$的第$j$列\n\n看作简单地执行m次矩阵-向量积，并将结果拼接在一起，使用`mm`函数\n$$\nC=AB= \\begin{bmatrix} a_1^T \\\\ a_2^T \\\\ a_3^T \\end{bmatrix}\\begin{bmatrix} b_1 & b_2 & b_3\\\\ \\end{bmatrix} = \\begin{bmatrix} a_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3\\\\ a_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\ a_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3 \\end{bmatrix}\n$$\n\n\n```python\ntorch.mm(A, B)\n```\n\n- 范数\n\n欧几里得距离是一个$L_2$范数： 假设$n$维向量$x$中的元素是$x_1,x_2...x_n$，其$L_2$*范数*是向量元素平方和的平方根：\n\n$||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2}$\n\n```python\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\ntensor(5.)\n```\n\n深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：\n\n$||x||_1 = \\sum_{i=1}^{n}|x_i|$\n\n```python\ntorch.abs(u).sum()\n```\n\n矩阵$X$的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根\n\n## 1.3 微积分\n\n我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。 具体而言，设函数f:Rn→R的输入是 一个n维向量x=[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量:.\n$$\n\\nabla_\\mathbf{x}f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2},\\ldots,\\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\right]^\\top\n$$\n\n\n## 1.4 自动微分\n\n- 计算图\n- 反向传播\n\n```python\nx = torch.arange(4.0, requires_grad=True) #需要保存梯度\ny = 2 * torch.dot(x, x)\nprint(x.grad) # None\ny.backward()\nprint(x.grad == 4 * x) #tensor([True, True, True, True])\n# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\nx.grad.zero_()\n```\n\n当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n\n然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。\n\n```python\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()\ny = x * x\n# 等价于y.backward(torch.ones(len(x)))\ny.sum().backward() #对y的sum反向传播，把张量变成标量\nx.grad # tensor([0., 2., 4., 6.])\n```\n\n有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。\n\n这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。\n\n```python\nx.grad.zero_()\ny = x * x\nu = y.detach() #分离y\nz = u * x\n\nz.sum().backward()\nx.grad == u # tensor([True, True, True, True])\n```\n\n- 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n\n## 1.5 概率\n\n[TODO]\n\n# 2. 线性神经网络\n\n## 2.1 线性回归\n\n定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n$\\hat{y} = Xw+b$\n\n损失函数$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2$\n\n随机梯度下降`SGD(stochastic gradient descent)`\n\n## 2.2 softmax\n\n- *独热编码*（one-hot encoding）\n\n  独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。\n\n![../_images/softmaxreg.svg](https://zh.d2l.ai/_images/softmaxreg.svg)\n\n要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做*校准*（calibration）。\n\nsoftmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：\n\n$\\hat{y} = softmax(o)$\n\n其中，\n$$\n\\hat{y}_j = \\frac{exp(o_j)}{\\sum_{k}{exp(o_k)}}\n$$\n\n\n在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n\n$argmax\\ \\hat{y}_j = argmax\\ o_j$\n\n尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型*（linear model）。\n\n其中，对于任何标签$y$和模型预测$\\hat{y}$，损失函数为：\n\n$l(y,\\hat{y})=-\\sum_{j=q}^{q}y_jlog\\hat{y}_j$\n\n通常被称为*交叉熵损失*（cross-entropy loss）\n\n# 3. 多层感知机\n\n## 3.1 激活函数\n\n我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。\n\n多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。\n\n![../_images/mlp.svg](https://zh.d2l.ai/_images/mlp.svg)\n\n*激活函数*（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。\n\n- ReLU\n\n$ReLU(x) = max(x,0)$\n\n![../_images/output_mlp_76f463_21_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg)\n\n- sigmoid\n\n$sigmoid(x)=\\frac{1}{1+exp(-x)}$\n\n![../_images/output_mlp_76f463_51_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg)\n\n- tanh\n\n$tanh(x) = \\frac{1-exp(-2x)}{1+exp(-2x)}$\n\n![../_images/output_mlp_76f463_81_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg)\n\n## 3.2 误差\n\n*训练误差*（training error）是指， 模型在训练数据集上计算得到的误差。 *泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n问题是，我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。\n\n![../_images/capacity-vs-error.svg](https://zh.d2l.ai/_images/capacity-vs-error.svg)\n\n## 3.3 权重衰退\n\n在训练参数化机器学习模型时， *权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。\n\n一种简单的方法是通过线性函数$f(x)=w^Tx$ 中的权重向量的某个范数来度量其复杂性， 例如$||w||^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标*最小化训练标签上的预测损失*， 调整为*最小化预测损失和惩罚项之和*。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$||w||^2$。 这正是我们想要的。\n\n我们通过*正则化常数*$\\lambda$来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：\n\n$L(w,b)+\\frac{\\lambda}{2}||w||_2$\n\n此外，为什么我们首先使用$L_2$范数，而不是$L_1$范数。 事实上，这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的*岭回归*（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为*套索回归*（lasso regression）。 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为*特征选择*（feature selection），这可能是其他场景下需要的。\n\n$L_2$正则化回归的小批量随机梯度下降更新如下式：\n\n$w = (1-\\alpha\\lambda) w - \\frac{\\alpha}{n}\\sum_{i=1}^{n}(w^Tx^{(i)}+b-y^{(i)})$\n\n我们仅考虑惩罚项，优化算法在训练的每一步*衰减*权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\\lambda$值对应较少约束的$w$， 而较大的$\\lambda$值对$w$的约束更大。\n\n在`pytorch`中，我们在实例化优化器时直接通过`weight_decay`指定weight decay超参数。 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了`weight_decay`，所以偏置参数$b$不会衰减。\n\n```python\ntrainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay': wd},\n        {\"params\":net[0].bias}], lr=lr)\n# wd为lambda\n```\n\n- 正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。\n- 保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。\n- 权重衰减功能在深度学习框架的优化器中提供。\n- 在同一训练代码实现中，不同的参数集可以有不同的更新行为。\n\n## 3.4 丢弃法\n\n当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n\n那么关键的挑战就是如何注入这种噪声。 一种想法是以一种*无偏向*（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。\n\n在每次训练迭代中，他将从均值为零的分布$\\epsilon~N(0,\\delta^2)$采样噪声添加到输入$x$， 从而产生扰动点$x'=x+\\epsilon$， 预期是$E(x')=x$。\n\n在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率*$p$由随机变量ℎ′替换，如下所示：\n\n$h'= 0 \\ when\\ p=0$\n\n$h'=\\frac{h}{1-p} \\ otherwise$\n\n![../_images/dropout2.svg](https://zh.d2l.ai/_images/dropout2.svg)\n\n对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据。\n\n```python\nnet = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # 在第一个全连接层之后添加一个dropout层\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # 在第二个全连接层之后添加一个dropout层\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n```\n\n- 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。\n- 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。\n- 暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。\n- 暂退法仅在训练期间使用。\n\n## 3.5 正向传播、反向传播、计算图\n\n- 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。\n- 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。\n- 在训练深度学习模型时，前向传播和反向传播是相互依赖的。\n- 训练比预测需要更多的内存。\n\n\n\n","source":"_posts/dive-into-deep-learning.md","raw":"---\ntitle: dive-into-deep-learning-notes\nmathjax: true\ndate: 2024/1/22 20:46:25\nimg: https://zh.d2l.ai/_static/logo-with-text.png\nexcerpt: 动手深度学习笔记\n---\n# 1. 预备知识\n\n## 1.1 数据处理\n\n`Tensor`数据类型和numpy中的`ndarray`类型相似，但是差异点在于\n\n首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；\n\n其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习\n\n对于任意具有相同形状的张量， 常见的标准算术运算符（`+`、`-`、`*`、`/`和`**`）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。\n\n我们也可以把多个张量*连结*（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。\n\n```python\ntorch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)\n```\n\n对张量中的所有元素进行求和，会产生一个单元素张量。\n\n```python\ntorch.sum(x)\n```\n\n在某些情况下，即使形状不同，我们仍然可以通过调用 *广播机制*（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：\n\n1. 通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；\n2. 对生成的数组执行按元素操作。\n\n将深度学习框架定义的张量转换为NumPy张量（`ndarray`）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。\n\n```python\nA = X.numpy()\nB = torch.tensor(A)\n```\n\n要将大小为1的张量转换为Python标量，我们可以调用`item`函数或Python的内置函数。\n\n```python\na = torch.tensor([3.5])\na, a.item(), float(a), int(a)\n```\n\n## 1.2 线性代数\n\n- *Hadamard*积\n\n两个矩阵的按元素乘法称为*Hadamard积*（Hadamard product）（数学符号⊙）\n$$\nA⊙B = \\begin{bmatrix} a_{11}b_{11} & a_{12}b_{12} &a_{13}b_{13} \\\\ a_{21}b_{21} & a_{22}b_{22} &a_{23}b_{23} \\\\ a_{31}b_{31}&a_{32}b_{32} &a_{3,3}b_{33} \\end{bmatrix}\n$$\n\n\n将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。\n\n- 降维求和\n\n 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定`axis=0`。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。\n\nsum,mean都是同理的\n\n- 非降维求和\n\n如果我们想沿某个轴计算`A`元素的累积总和， 比如`axis=0`（按行计算），可以调用`cumsum`函数。 此函数不会沿任何轴降低输入张量的维度。\n\n```python\nA.cumsum(axis=0)\n```\n\n- 点积\n\n给定两个向量$x,y$的点积$x^Ty$(或$<x,y>$)是相同位置的按元素乘积的和\n$$\nx^Ty = \\sum_{i=1}^{d}x_iy_i\n$$\n\n\n将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。\n\n```python\ntorch.dot(x, y)\n```\n\n- 矩阵-向量积\n\n$$\nAx = \\begin{bmatrix} a_1^T \\\\ a_2^T \\\\ a_3^T \\end{bmatrix}x = \\begin{bmatrix} a_1^Tx \\\\ a_2^Tx \\\\ a_3^Tx \\end{bmatrix}\n$$\n\n\n\n在代码中使用张量表示矩阵-向量积，我们使用`mv`函数。\n\n```python\ntorch.mv(A, x)\n```\n\n- 矩阵-矩阵乘法\n\n用行向量$A_i^T$表示矩阵$A$的第$i$行，列向量$b_j$作为矩阵$B$的第$j$列\n\n看作简单地执行m次矩阵-向量积，并将结果拼接在一起，使用`mm`函数\n$$\nC=AB= \\begin{bmatrix} a_1^T \\\\ a_2^T \\\\ a_3^T \\end{bmatrix}\\begin{bmatrix} b_1 & b_2 & b_3\\\\ \\end{bmatrix} = \\begin{bmatrix} a_1^Tb_1 & a_1^Tb_2 & a_1^Tb_3\\\\ a_2^Tb_1 & a_2^Tb_2 & a_2^Tb_3 \\\\ a_3^Tb_1 & a_3^Tb_2 & a_3^Tb_3 \\end{bmatrix}\n$$\n\n\n```python\ntorch.mm(A, B)\n```\n\n- 范数\n\n欧几里得距离是一个$L_2$范数： 假设$n$维向量$x$中的元素是$x_1,x_2...x_n$，其$L_2$*范数*是向量元素平方和的平方根：\n\n$||x||_2 = \\sqrt{\\sum_{i=1}^{n}x_i^2}$\n\n```python\nu = torch.tensor([3.0, -4.0])\ntorch.norm(u)\ntensor(5.)\n```\n\n深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：\n\n$||x||_1 = \\sum_{i=1}^{n}|x_i|$\n\n```python\ntorch.abs(u).sum()\n```\n\n矩阵$X$的*Frobenius范数*（Frobenius norm）是矩阵元素平方和的平方根\n\n## 1.3 微积分\n\n我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的*梯度*（gradient）向量。 具体而言，设函数f:Rn→R的输入是 一个n维向量x=[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量:.\n$$\n\\nabla_\\mathbf{x}f(\\mathbf{x})=\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2},\\ldots,\\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\right]^\\top\n$$\n\n\n## 1.4 自动微分\n\n- 计算图\n- 反向传播\n\n```python\nx = torch.arange(4.0, requires_grad=True) #需要保存梯度\ny = 2 * torch.dot(x, x)\nprint(x.grad) # None\ny.backward()\nprint(x.grad == 4 * x) #tensor([True, True, True, True])\n# 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值\nx.grad.zero_()\n```\n\n当`y`不是标量时，向量`y`关于向量`x`的导数的最自然解释是一个矩阵。 对于高阶和高维的`y`和`x`，求导的结果可以是一个高阶张量。\n\n然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。\n\n```python\n# 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。\n# 本例只想求偏导数的和，所以传递一个1的梯度是合适的\nx.grad.zero_()\ny = x * x\n# 等价于y.backward(torch.ones(len(x)))\ny.sum().backward() #对y的sum反向传播，把张量变成标量\nx.grad # tensor([0., 2., 4., 6.])\n```\n\n有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设`y`是作为`x`的函数计算的，而`z`则是作为`y`和`x`的函数计算的。 想象一下，我们想计算`z`关于`x`的梯度，但由于某种原因，希望将`y`视为一个常数， 并且只考虑到`x`在`y`被计算后发挥的作用。\n\n这里可以分离`y`来返回一个新变量`u`，该变量与`y`具有相同的值， 但丢弃计算图中如何计算`y`的任何信息。 换句话说，梯度不会向后流经`u`到`x`。 因此，下面的反向传播函数计算`z=u*x`关于`x`的偏导数，同时将`u`作为常数处理， 而不是`z=x*x*x`关于`x`的偏导数。\n\n```python\nx.grad.zero_()\ny = x * x\nu = y.detach() #分离y\nz = u * x\n\nz.sum().backward()\nx.grad == u # tensor([True, True, True, True])\n```\n\n- 深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。\n\n## 1.5 概率\n\n[TODO]\n\n# 2. 线性神经网络\n\n## 2.1 线性回归\n\n定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n$\\hat{y} = Xw+b$\n\n损失函数$J(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2$\n\n随机梯度下降`SGD(stochastic gradient descent)`\n\n## 2.2 softmax\n\n- *独热编码*（one-hot encoding）\n\n  独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。\n\n![../_images/softmaxreg.svg](https://zh.d2l.ai/_images/softmaxreg.svg)\n\n要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做*校准*（calibration）。\n\nsoftmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：\n\n$\\hat{y} = softmax(o)$\n\n其中，\n$$\n\\hat{y}_j = \\frac{exp(o_j)}{\\sum_{k}{exp(o_k)}}\n$$\n\n\n在预测过程中，我们仍然可以用下式来选择最有可能的类别。\n\n$argmax\\ \\hat{y}_j = argmax\\ o_j$\n\n尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型*（linear model）。\n\n其中，对于任何标签$y$和模型预测$\\hat{y}$，损失函数为：\n\n$l(y,\\hat{y})=-\\sum_{j=q}^{q}y_jlog\\hat{y}_j$\n\n通常被称为*交叉熵损失*（cross-entropy loss）\n\n# 3. 多层感知机\n\n## 3.1 激活函数\n\n我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为*多层感知机*（multilayer perceptron），通常缩写为*MLP*。\n\n多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。\n\n![../_images/mlp.svg](https://zh.d2l.ai/_images/mlp.svg)\n\n*激活函数*（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。\n\n- ReLU\n\n$ReLU(x) = max(x,0)$\n\n![../_images/output_mlp_76f463_21_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg)\n\n- sigmoid\n\n$sigmoid(x)=\\frac{1}{1+exp(-x)}$\n\n![../_images/output_mlp_76f463_51_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg)\n\n- tanh\n\n$tanh(x) = \\frac{1-exp(-2x)}{1+exp(-2x)}$\n\n![../_images/output_mlp_76f463_81_0.svg](https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg)\n\n## 3.2 误差\n\n*训练误差*（training error）是指， 模型在训练数据集上计算得到的误差。 *泛化误差*（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。\n\n问题是，我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。\n\n![../_images/capacity-vs-error.svg](https://zh.d2l.ai/_images/capacity-vs-error.svg)\n\n## 3.3 权重衰退\n\n在训练参数化机器学习模型时， *权重衰减*（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。\n\n一种简单的方法是通过线性函数$f(x)=w^Tx$ 中的权重向量的某个范数来度量其复杂性， 例如$||w||^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标*最小化训练标签上的预测损失*， 调整为*最小化预测损失和惩罚项之和*。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$||w||^2$。 这正是我们想要的。\n\n我们通过*正则化常数*$\\lambda$来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：\n\n$L(w,b)+\\frac{\\lambda}{2}||w||_2$\n\n此外，为什么我们首先使用$L_2$范数，而不是$L_1$范数。 事实上，这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的*岭回归*（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为*套索回归*（lasso regression）。 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为*特征选择*（feature selection），这可能是其他场景下需要的。\n\n$L_2$正则化回归的小批量随机梯度下降更新如下式：\n\n$w = (1-\\alpha\\lambda) w - \\frac{\\alpha}{n}\\sum_{i=1}^{n}(w^Tx^{(i)}+b-y^{(i)})$\n\n我们仅考虑惩罚项，优化算法在训练的每一步*衰减*权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\\lambda$值对应较少约束的$w$， 而较大的$\\lambda$值对$w$的约束更大。\n\n在`pytorch`中，我们在实例化优化器时直接通过`weight_decay`指定weight decay超参数。 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了`weight_decay`，所以偏置参数$b$不会衰减。\n\n```python\ntrainer = torch.optim.SGD([\n        {\"params\":net[0].weight,'weight_decay': wd},\n        {\"params\":net[0].bias}], lr=lr)\n# wd为lambda\n```\n\n- 正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。\n- 保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。\n- 权重衰减功能在深度学习框架的优化器中提供。\n- 在同一训练代码实现中，不同的参数集可以有不同的更新行为。\n\n## 3.4 丢弃法\n\n当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。\n\n那么关键的挑战就是如何注入这种噪声。 一种想法是以一种*无偏向*（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。\n\n在每次训练迭代中，他将从均值为零的分布$\\epsilon~N(0,\\delta^2)$采样噪声添加到输入$x$， 从而产生扰动点$x'=x+\\epsilon$， 预期是$E(x')=x$。\n\n在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以*暂退概率*$p$由随机变量ℎ′替换，如下所示：\n\n$h'= 0 \\ when\\ p=0$\n\n$h'=\\frac{h}{1-p} \\ otherwise$\n\n![../_images/dropout2.svg](https://zh.d2l.ai/_images/dropout2.svg)\n\n对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个`Dropout`层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，`Dropout`层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，`Dropout`层仅传递数据。\n\n```python\nnet = nn.Sequential(nn.Flatten(),\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        # 在第一个全连接层之后添加一个dropout层\n        nn.Dropout(dropout1),\n        nn.Linear(256, 256),\n        nn.ReLU(),\n        # 在第二个全连接层之后添加一个dropout层\n        nn.Dropout(dropout2),\n        nn.Linear(256, 10))\n```\n\n- 暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。\n- 暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。\n- 暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。\n- 暂退法仅在训练期间使用。\n\n## 3.5 正向传播、反向传播、计算图\n\n- 前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。\n- 反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。\n- 在训练深度学习模型时，前向传播和反向传播是相互依赖的。\n- 训练比预测需要更多的内存。\n\n\n\n","slug":"dive-into-deep-learning","published":1,"updated":"2025-03-03T10:42:18.757Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9f000abs998exyekvv","content":"<h1 id=\"1-预备知识\"><a href=\"#1-预备知识\" class=\"headerlink\" title=\"1. 预备知识\"></a>1. 预备知识</h1><h2 id=\"1-1-数据处理\"><a href=\"#1-1-数据处理\" class=\"headerlink\" title=\"1.1 数据处理\"></a>1.1 数据处理</h2><p><code>Tensor</code>数据类型和numpy中的<code>ndarray</code>类型相似，但是差异点在于</p>\n<p>首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；</p>\n<p>其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习</p>\n<p>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。</p>\n<p>我们也可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.cat((X, Y), dim=<span class=\"number\">0</span>), torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对张量中的所有元素进行求和，会产生一个单元素张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">sum</span>(x)</span><br></pre></td></tr></table></figure>\n\n<p>在某些情况下，即使形状不同，我们仍然可以通过调用 <em>广播机制</em>（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：</p>\n<ol>\n<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>\n<li>对生成的数组执行按元素操作。</li>\n</ol>\n<p>将深度学习框架定义的张量转换为NumPy张量（<code>ndarray</code>）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = X.numpy()</span><br><span class=\"line\">B = torch.tensor(A)</span><br></pre></td></tr></table></figure>\n\n<p>要将大小为1的张量转换为Python标量，我们可以调用<code>item</code>函数或Python的内置函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.tensor([<span class=\"number\">3.5</span>])</span><br><span class=\"line\">a, a.item(), <span class=\"built_in\">float</span>(a), <span class=\"built_in\">int</span>(a)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-2-线性代数\"><a href=\"#1-2-线性代数\" class=\"headerlink\" title=\"1.2 线性代数\"></a>1.2 线性代数</h2><ul>\n<li><em>Hadamard</em>积</li>\n</ul>\n<p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）<br>$$<br>A⊙B &#x3D; \\begin{bmatrix} a_{11}b_{11} &amp; a_{12}b_{12} &amp;a_{13}b_{13} \\ a_{21}b_{21} &amp; a_{22}b_{22} &amp;a_{23}b_{23} \\ a_{31}b_{31}&amp;a_{32}b_{32} &amp;a_{3,3}b_{33} \\end{bmatrix}<br>$$</p>\n<p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p>\n<ul>\n<li>降维求和</li>\n</ul>\n<p> 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p>\n<p>sum,mean都是同理的</p>\n<ul>\n<li>非降维求和</li>\n</ul>\n<p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.cumsum(axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>点积</li>\n</ul>\n<p>给定两个向量$x,y$的点积$x^Ty$(或$&lt;x,y&gt;$)是相同位置的按元素乘积的和<br>$$<br>x^Ty &#x3D; \\sum_{i&#x3D;1}^{d}x_iy_i<br>$$</p>\n<p>将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.dot(x, y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>矩阵-向量积</li>\n</ul>\n<p>$$<br>Ax &#x3D; \\begin{bmatrix} a_1^T \\ a_2^T \\ a_3^T \\end{bmatrix}x &#x3D; \\begin{bmatrix} a_1^Tx \\ a_2^Tx \\ a_3^Tx \\end{bmatrix}<br>$$</p>\n<p>在代码中使用张量表示矩阵-向量积，我们使用<code>mv</code>函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.mv(A, x)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>矩阵-矩阵乘法</li>\n</ul>\n<p>用行向量$A_i^T$表示矩阵$A$的第$i$行，列向量$b_j$作为矩阵$B$的第$j$列</p>\n<p>看作简单地执行m次矩阵-向量积，并将结果拼接在一起，使用<code>mm</code>函数<br>$$<br>C&#x3D;AB&#x3D; \\begin{bmatrix} a_1^T \\ a_2^T \\ a_3^T \\end{bmatrix}\\begin{bmatrix} b_1 &amp; b_2 &amp; b_3\\ \\end{bmatrix} &#x3D; \\begin{bmatrix} a_1^Tb_1 &amp; a_1^Tb_2 &amp; a_1^Tb_3\\ a_2^Tb_1 &amp; a_2^Tb_2 &amp; a_2^Tb_3 \\ a_3^Tb_1 &amp; a_3^Tb_2 &amp; a_3^Tb_3 \\end{bmatrix}<br>$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.mm(A, B)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>范数</li>\n</ul>\n<p>欧几里得距离是一个$L_2$范数： 假设$n$维向量$x$中的元素是$x_1,x_2…x_n$，其$L_2$<em>范数</em>是向量元素平方和的平方根：</p>\n<p>$||x||<em>2 &#x3D; \\sqrt{\\sum</em>{i&#x3D;1}^{n}x_i^2}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u = torch.tensor([<span class=\"number\">3.0</span>, -<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u)</span><br><span class=\"line\">tensor(<span class=\"number\">5.</span>)</span><br></pre></td></tr></table></figure>\n\n<p>深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：</p>\n<p>$||x||<em>1 &#x3D; \\sum</em>{i&#x3D;1}^{n}|x_i|$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n<p>矩阵$X$的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根</p>\n<h2 id=\"1-3-微积分\"><a href=\"#1-3-微积分\" class=\"headerlink\" title=\"1.3 微积分\"></a>1.3 微积分</h2><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的<em>梯度</em>（gradient）向量。 具体而言，设函数f:Rn→R的输入是 一个n维向量x&#x3D;[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量:.<br>$$<br>\\nabla_\\mathbf{x}f(\\mathbf{x})&#x3D;\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2},\\ldots,\\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\right]^\\top<br>$$</p>\n<h2 id=\"1-4-自动微分\"><a href=\"#1-4-自动微分\" class=\"headerlink\" title=\"1.4 自动微分\"></a>1.4 自动微分</h2><ul>\n<li>计算图</li>\n<li>反向传播</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4.0</span>, requires_grad=<span class=\"literal\">True</span>) <span class=\"comment\">#需要保存梯度</span></span><br><span class=\"line\">y = <span class=\"number\">2</span> * torch.dot(x, x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\"># None</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad == <span class=\"number\">4</span> * x) <span class=\"comment\">#tensor([True, True, True, True])</span></span><br><span class=\"line\"><span class=\"comment\"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class=\"line\">x.grad.zero_()</span><br></pre></td></tr></table></figure>\n\n<p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。 对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</p>\n<p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class=\"line\"><span class=\"comment\"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\"><span class=\"comment\"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#对y的sum反向传播，把张量变成标量</span></span><br><span class=\"line\">x.grad <span class=\"comment\"># tensor([0., 2., 4., 6.])</span></span><br></pre></td></tr></table></figure>\n\n<p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。 想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数， 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p>\n<p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">u = y.detach() <span class=\"comment\">#分离y</span></span><br><span class=\"line\">z = u * x</span><br><span class=\"line\"></span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == u <span class=\"comment\"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li>\n</ul>\n<h2 id=\"1-5-概率\"><a href=\"#1-5-概率\" class=\"headerlink\" title=\"1.5 概率\"></a>1.5 概率</h2><p>[TODO]</p>\n<h1 id=\"2-线性神经网络\"><a href=\"#2-线性神经网络\" class=\"headerlink\" title=\"2. 线性神经网络\"></a>2. 线性神经网络</h1><h2 id=\"2-1-线性回归\"><a href=\"#2-1-线性回归\" class=\"headerlink\" title=\"2.1 线性回归\"></a>2.1 线性回归</h2><p>定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>$\\hat{y} &#x3D; Xw+b$</p>\n<p>损失函数$J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2$</p>\n<p>随机梯度下降<code>SGD(stochastic gradient descent)</code></p>\n<h2 id=\"2-2-softmax\"><a href=\"#2-2-softmax\" class=\"headerlink\" title=\"2.2 softmax\"></a>2.2 softmax</h2><ul>\n<li><p><em>独热编码</em>（one-hot encoding）</p>\n<p>独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。</p>\n</li>\n</ul>\n<p><img src=\"https://zh.d2l.ai/_images/softmaxreg.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/softmaxreg.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/softmaxreg.svg\"></p>\n<p>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做<em>校准</em>（calibration）。</p>\n<p>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：</p>\n<p>$\\hat{y} &#x3D; softmax(o)$</p>\n<p>其中，<br>$$<br>\\hat{y}<em>j &#x3D; \\frac{exp(o_j)}{\\sum</em>{k}{exp(o_k)}}<br>$$</p>\n<p>在预测过程中，我们仍然可以用下式来选择最有可能的类别。</p>\n<p>$argmax\\ \\hat{y}_j &#x3D; argmax\\ o_j$</p>\n<p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个<em>线性模型</em>（linear model）。</p>\n<p>其中，对于任何标签$y$和模型预测$\\hat{y}$，损失函数为：</p>\n<p>$l(y,\\hat{y})&#x3D;-\\sum_{j&#x3D;q}^{q}y_jlog\\hat{y}_j$</p>\n<p>通常被称为<em>交叉熵损失</em>（cross-entropy loss）</p>\n<h1 id=\"3-多层感知机\"><a href=\"#3-多层感知机\" class=\"headerlink\" title=\"3. 多层感知机\"></a>3. 多层感知机</h1><h2 id=\"3-1-激活函数\"><a href=\"#3-1-激活函数\" class=\"headerlink\" title=\"3.1 激活函数\"></a>3.1 激活函数</h2><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为<em>多层感知机</em>（multilayer perceptron），通常缩写为<em>MLP</em>。</p>\n<p>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</p>\n<p><img src=\"https://zh.d2l.ai/_images/mlp.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/mlp.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/mlp.svg\"></p>\n<p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p>\n<ul>\n<li>ReLU</li>\n</ul>\n<p>$ReLU(x) &#x3D; max(x,0)$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/output_mlp_76f463_21_0.svg\"></p>\n<ul>\n<li>sigmoid</li>\n</ul>\n<p>$sigmoid(x)&#x3D;\\frac{1}{1+exp(-x)}$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/output_mlp_76f463_51_0.svg\"></p>\n<ul>\n<li>tanh</li>\n</ul>\n<p>$tanh(x) &#x3D; \\frac{1-exp(-2x)}{1+exp(-2x)}$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/output_mlp_76f463_81_0.svg\"></p>\n<h2 id=\"3-2-误差\"><a href=\"#3-2-误差\" class=\"headerlink\" title=\"3.2 误差\"></a>3.2 误差</h2><p><em>训练误差</em>（training error）是指， 模型在训练数据集上计算得到的误差。 <em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>问题是，我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p>\n<p><img src=\"https://zh.d2l.ai/_images/capacity-vs-error.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/capacity-vs-error.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/capacity-vs-error.svg\"></p>\n<h2 id=\"3-3-权重衰退\"><a href=\"#3-3-权重衰退\" class=\"headerlink\" title=\"3.3 权重衰退\"></a>3.3 权重衰退</h2><p>在训练参数化机器学习模型时， <em>权重衰减</em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。</p>\n<p>一种简单的方法是通过线性函数$f(x)&#x3D;w^Tx$ 中的权重向量的某个范数来度量其复杂性， 例如$||w||^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标<em>最小化训练标签上的预测损失</em>， 调整为<em>最小化预测损失和惩罚项之和</em>。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$||w||^2$。 这正是我们想要的。</p>\n<p>我们通过<em>正则化常数</em>$\\lambda$来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：</p>\n<p>$L(w,b)+\\frac{\\lambda}{2}||w||_2$</p>\n<p>此外，为什么我们首先使用$L_2$范数，而不是$L_1$范数。 事实上，这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的<em>岭回归</em>（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为<em>套索回归</em>（lasso regression）。 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p>\n<p>$L_2$正则化回归的小批量随机梯度下降更新如下式：</p>\n<p>$w &#x3D; (1-\\alpha\\lambda) w - \\frac{\\alpha}{n}\\sum_{i&#x3D;1}^{n}(w^Tx^{(i)}+b-y^{(i)})$</p>\n<p>我们仅考虑惩罚项，优化算法在训练的每一步<em>衰减</em>权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\\lambda$值对应较少约束的$w$， 而较大的$\\lambda$值对$w$的约束更大。</p>\n<p>在<code>pytorch</code>中，我们在实例化优化器时直接通过<code>weight_decay</code>指定weight decay超参数。 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了<code>weight_decay</code>，所以偏置参数$b$不会衰减。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].weight,<span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\"><span class=\"comment\"># wd为lambda</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li>\n<li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。</li>\n<li>权重衰减功能在深度学习框架的优化器中提供。</li>\n<li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li>\n</ul>\n<h2 id=\"3-4-丢弃法\"><a href=\"#3-4-丢弃法\" class=\"headerlink\" title=\"3.4 丢弃法\"></a>3.4 丢弃法</h2><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p>\n<p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种<em>无偏向</em>（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>\n<p>在每次训练迭代中，他将从均值为零的分布$\\epsilon~N(0,\\delta^2)$采样噪声添加到输入$x$， 从而产生扰动点$x’&#x3D;x+\\epsilon$， 预期是$E(x’)&#x3D;x$。</p>\n<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以<em>暂退概率</em>$p$由随机变量ℎ′替换，如下所示：</p>\n<p>$h’&#x3D; 0 \\ when\\ p&#x3D;0$</p>\n<p>$h’&#x3D;\\frac{h}{1-p} \\ otherwise$</p>\n<p><img src=\"https://zh.d2l.ai/_images/dropout2.svg\" class=\"lazyload placeholder\" data-srcset=\"https://zh.d2l.ai/_images/dropout2.svg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"../_images/dropout2.svg\"></p>\n<p>对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个<code>Dropout</code>层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，<code>Dropout</code>层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，<code>Dropout</code>层仅传递数据。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout1),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout2),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li>\n<li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li>\n<li>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。</li>\n<li>暂退法仅在训练期间使用。</li>\n</ul>\n<h2 id=\"3-5-正向传播、反向传播、计算图\"><a href=\"#3-5-正向传播、反向传播、计算图\" class=\"headerlink\" title=\"3.5 正向传播、反向传播、计算图\"></a>3.5 正向传播、反向传播、计算图</h2><ul>\n<li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li>\n<li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li>\n<li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li>\n<li>训练比预测需要更多的内存。</li>\n</ul>\n","more":"<h1 id=\"1-预备知识\"><a href=\"#1-预备知识\" class=\"headerlink\" title=\"1. 预备知识\"></a>1. 预备知识</h1><h2 id=\"1-1-数据处理\"><a href=\"#1-1-数据处理\" class=\"headerlink\" title=\"1.1 数据处理\"></a>1.1 数据处理</h2><p><code>Tensor</code>数据类型和numpy中的<code>ndarray</code>类型相似，但是差异点在于</p>\n<p>首先，GPU很好地支持加速计算，而NumPy仅支持CPU计算；</p>\n<p>其次，张量类支持自动微分。 这些功能使得张量类更适合深度学习</p>\n<p>对于任意具有相同形状的张量， 常见的标准算术运算符（<code>+</code>、<code>-</code>、<code>*</code>、<code>/</code>和<code>**</code>）都可以被升级为按元素运算。 我们可以在同一形状的任意两个张量上调用按元素操作。</p>\n<p>我们也可以把多个张量<em>连结</em>（concatenate）在一起， 把它们端对端地叠起来形成一个更大的张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.cat((X, Y), dim=<span class=\"number\">0</span>), torch.cat((X, Y), dim=<span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<p>对张量中的所有元素进行求和，会产生一个单元素张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">sum</span>(x)</span><br></pre></td></tr></table></figure>\n\n<p>在某些情况下，即使形状不同，我们仍然可以通过调用 <em>广播机制</em>（broadcasting mechanism）来执行按元素操作。 这种机制的工作方式如下：</p>\n<ol>\n<li>通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状；</li>\n<li>对生成的数组执行按元素操作。</li>\n</ol>\n<p>将深度学习框架定义的张量转换为NumPy张量（<code>ndarray</code>）很容易，反之也同样容易。 torch张量和numpy数组将共享它们的底层内存，就地操作更改一个张量也会同时更改另一个张量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A = X.numpy()</span><br><span class=\"line\">B = torch.tensor(A)</span><br></pre></td></tr></table></figure>\n\n<p>要将大小为1的张量转换为Python标量，我们可以调用<code>item</code>函数或Python的内置函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = torch.tensor([<span class=\"number\">3.5</span>])</span><br><span class=\"line\">a, a.item(), <span class=\"built_in\">float</span>(a), <span class=\"built_in\">int</span>(a)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"1-2-线性代数\"><a href=\"#1-2-线性代数\" class=\"headerlink\" title=\"1.2 线性代数\"></a>1.2 线性代数</h2><ul>\n<li><em>Hadamard</em>积</li>\n</ul>\n<p>两个矩阵的按元素乘法称为<em>Hadamard积</em>（Hadamard product）（数学符号⊙）<br>$$<br>A⊙B &#x3D; \\begin{bmatrix} a_{11}b_{11} &amp; a_{12}b_{12} &amp;a_{13}b_{13} \\ a_{21}b_{21} &amp; a_{22}b_{22} &amp;a_{23}b_{23} \\ a_{31}b_{31}&amp;a_{32}b_{32} &amp;a_{3,3}b_{33} \\end{bmatrix}<br>$$</p>\n<p>将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。</p>\n<ul>\n<li>降维求和</li>\n</ul>\n<p> 我们还可以指定张量沿哪一个轴来通过求和降低维度。 以矩阵为例，为了通过求和所有行的元素来降维（轴0），可以在调用函数时指定<code>axis=0</code>。 由于输入矩阵沿0轴降维以生成输出向量，因此输入轴0的维数在输出形状中消失。</p>\n<p>sum,mean都是同理的</p>\n<ul>\n<li>非降维求和</li>\n</ul>\n<p>如果我们想沿某个轴计算<code>A</code>元素的累积总和， 比如<code>axis=0</code>（按行计算），可以调用<code>cumsum</code>函数。 此函数不会沿任何轴降低输入张量的维度。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">A.cumsum(axis=<span class=\"number\">0</span>)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>点积</li>\n</ul>\n<p>给定两个向量$x,y$的点积$x^Ty$(或$&lt;x,y&gt;$)是相同位置的按元素乘积的和<br>$$<br>x^Ty &#x3D; \\sum_{i&#x3D;1}^{d}x_iy_i<br>$$</p>\n<p>将两个向量规范化得到单位长度后，点积表示它们夹角的余弦。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.dot(x, y)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>矩阵-向量积</li>\n</ul>\n<p>$$<br>Ax &#x3D; \\begin{bmatrix} a_1^T \\ a_2^T \\ a_3^T \\end{bmatrix}x &#x3D; \\begin{bmatrix} a_1^Tx \\ a_2^Tx \\ a_3^Tx \\end{bmatrix}<br>$$</p>\n<p>在代码中使用张量表示矩阵-向量积，我们使用<code>mv</code>函数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.mv(A, x)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>矩阵-矩阵乘法</li>\n</ul>\n<p>用行向量$A_i^T$表示矩阵$A$的第$i$行，列向量$b_j$作为矩阵$B$的第$j$列</p>\n<p>看作简单地执行m次矩阵-向量积，并将结果拼接在一起，使用<code>mm</code>函数<br>$$<br>C&#x3D;AB&#x3D; \\begin{bmatrix} a_1^T \\ a_2^T \\ a_3^T \\end{bmatrix}\\begin{bmatrix} b_1 &amp; b_2 &amp; b_3\\ \\end{bmatrix} &#x3D; \\begin{bmatrix} a_1^Tb_1 &amp; a_1^Tb_2 &amp; a_1^Tb_3\\ a_2^Tb_1 &amp; a_2^Tb_2 &amp; a_2^Tb_3 \\ a_3^Tb_1 &amp; a_3^Tb_2 &amp; a_3^Tb_3 \\end{bmatrix}<br>$$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.mm(A, B)</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>范数</li>\n</ul>\n<p>欧几里得距离是一个$L_2$范数： 假设$n$维向量$x$中的元素是$x_1,x_2…x_n$，其$L_2$<em>范数</em>是向量元素平方和的平方根：</p>\n<p>$||x||<em>2 &#x3D; \\sqrt{\\sum</em>{i&#x3D;1}^{n}x_i^2}$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">u = torch.tensor([<span class=\"number\">3.0</span>, -<span class=\"number\">4.0</span>])</span><br><span class=\"line\">torch.norm(u)</span><br><span class=\"line\">tensor(<span class=\"number\">5.</span>)</span><br></pre></td></tr></table></figure>\n\n<p>深度学习中更经常地使用$L_2$范数的平方，也会经常遇到$L_1$范数，它表示为向量元素的绝对值之和：</p>\n<p>$||x||<em>1 &#x3D; \\sum</em>{i&#x3D;1}^{n}|x_i|$</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">torch.<span class=\"built_in\">abs</span>(u).<span class=\"built_in\">sum</span>()</span><br></pre></td></tr></table></figure>\n\n<p>矩阵$X$的<em>Frobenius范数</em>（Frobenius norm）是矩阵元素平方和的平方根</p>\n<h2 id=\"1-3-微积分\"><a href=\"#1-3-微积分\" class=\"headerlink\" title=\"1.3 微积分\"></a>1.3 微积分</h2><p>我们可以连结一个多元函数对其所有变量的偏导数，以得到该函数的<em>梯度</em>（gradient）向量。 具体而言，设函数f:Rn→R的输入是 一个n维向量x&#x3D;[x1,x2,…,xn]⊤，并且输出是一个标量。 函数f(x)相对于x的梯度是一个包含n个偏导数的向量:.<br>$$<br>\\nabla_\\mathbf{x}f(\\mathbf{x})&#x3D;\\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2},\\ldots,\\frac{\\partial f(\\mathbf{x})}{\\partial x_n}\\right]^\\top<br>$$</p>\n<h2 id=\"1-4-自动微分\"><a href=\"#1-4-自动微分\" class=\"headerlink\" title=\"1.4 自动微分\"></a>1.4 自动微分</h2><ul>\n<li>计算图</li>\n<li>反向传播</li>\n</ul>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.arange(<span class=\"number\">4.0</span>, requires_grad=<span class=\"literal\">True</span>) <span class=\"comment\">#需要保存梯度</span></span><br><span class=\"line\">y = <span class=\"number\">2</span> * torch.dot(x, x)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad) <span class=\"comment\"># None</span></span><br><span class=\"line\">y.backward()</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad == <span class=\"number\">4</span> * x) <span class=\"comment\">#tensor([True, True, True, True])</span></span><br><span class=\"line\"><span class=\"comment\"># 在默认情况下，PyTorch会累积梯度，我们需要清除之前的值</span></span><br><span class=\"line\">x.grad.zero_()</span><br></pre></td></tr></table></figure>\n\n<p>当<code>y</code>不是标量时，向量<code>y</code>关于向量<code>x</code>的导数的最自然解释是一个矩阵。 对于高阶和高维的<code>y</code>和<code>x</code>，求导的结果可以是一个高阶张量。</p>\n<p>然而，虽然这些更奇特的对象确实出现在高级机器学习中（包括深度学习中）， 但当调用向量的反向计算时，我们通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里，我们的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 对非标量调用backward需要传入一个gradient参数，该参数指定微分函数关于self的梯度。</span></span><br><span class=\"line\"><span class=\"comment\"># 本例只想求偏导数的和，所以传递一个1的梯度是合适的</span></span><br><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\"><span class=\"comment\"># 等价于y.backward(torch.ones(len(x)))</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward() <span class=\"comment\">#对y的sum反向传播，把张量变成标量</span></span><br><span class=\"line\">x.grad <span class=\"comment\"># tensor([0., 2., 4., 6.])</span></span><br></pre></td></tr></table></figure>\n\n<p>有时，我们希望将某些计算移动到记录的计算图之外。 例如，假设<code>y</code>是作为<code>x</code>的函数计算的，而<code>z</code>则是作为<code>y</code>和<code>x</code>的函数计算的。 想象一下，我们想计算<code>z</code>关于<code>x</code>的梯度，但由于某种原因，希望将<code>y</code>视为一个常数， 并且只考虑到<code>x</code>在<code>y</code>被计算后发挥的作用。</p>\n<p>这里可以分离<code>y</code>来返回一个新变量<code>u</code>，该变量与<code>y</code>具有相同的值， 但丢弃计算图中如何计算<code>y</code>的任何信息。 换句话说，梯度不会向后流经<code>u</code>到<code>x</code>。 因此，下面的反向传播函数计算<code>z=u*x</code>关于<code>x</code>的偏导数，同时将<code>u</code>作为常数处理， 而不是<code>z=x*x*x</code>关于<code>x</code>的偏导数。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x.grad.zero_()</span><br><span class=\"line\">y = x * x</span><br><span class=\"line\">u = y.detach() <span class=\"comment\">#分离y</span></span><br><span class=\"line\">z = u * x</span><br><span class=\"line\"></span><br><span class=\"line\">z.<span class=\"built_in\">sum</span>().backward()</span><br><span class=\"line\">x.grad == u <span class=\"comment\"># tensor([True, True, True, True])</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>深度学习框架可以自动计算导数：我们首先将梯度附加到想要对其计算偏导数的变量上，然后记录目标值的计算，执行它的反向传播函数，并访问得到的梯度。</li>\n</ul>\n<h2 id=\"1-5-概率\"><a href=\"#1-5-概率\" class=\"headerlink\" title=\"1.5 概率\"></a>1.5 概率</h2><p>[TODO]</p>\n<h1 id=\"2-线性神经网络\"><a href=\"#2-线性神经网络\" class=\"headerlink\" title=\"2. 线性神经网络\"></a>2. 线性神经网络</h1><h2 id=\"2-1-线性回归\"><a href=\"#2-1-线性回归\" class=\"headerlink\" title=\"2.1 线性回归\"></a>2.1 线性回归</h2><p>定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>$\\hat{y} &#x3D; Xw+b$</p>\n<p>损失函数$J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2$</p>\n<p>随机梯度下降<code>SGD(stochastic gradient descent)</code></p>\n<h2 id=\"2-2-softmax\"><a href=\"#2-2-softmax\" class=\"headerlink\" title=\"2.2 softmax\"></a>2.2 softmax</h2><ul>\n<li><p><em>独热编码</em>（one-hot encoding）</p>\n<p>独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。</p>\n</li>\n</ul>\n<p><img src=\"https://zh.d2l.ai/_images/softmaxreg.svg\" alt=\"../_images/softmaxreg.svg\"></p>\n<p>要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练的目标函数，来激励模型精准地估计概率。 例如， 在分类器输出0.5的所有样本中，我们希望这些样本是刚好有一半实际上属于预测的类别。 这个属性叫做<em>校准</em>（calibration）。</p>\n<p>softmax函数能够将未规范化的预测变换为非负数并且总和为1，同时让模型保持 可导的性质。 为了完成这一目标，我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的概率值总和为1，我们再让每个求幂后的结果除以它们的总和。如下式：</p>\n<p>$\\hat{y} &#x3D; softmax(o)$</p>\n<p>其中，<br>$$<br>\\hat{y}<em>j &#x3D; \\frac{exp(o_j)}{\\sum</em>{k}{exp(o_k)}}<br>$$</p>\n<p>在预测过程中，我们仍然可以用下式来选择最有可能的类别。</p>\n<p>$argmax\\ \\hat{y}_j &#x3D; argmax\\ o_j$</p>\n<p>尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个<em>线性模型</em>（linear model）。</p>\n<p>其中，对于任何标签$y$和模型预测$\\hat{y}$，损失函数为：</p>\n<p>$l(y,\\hat{y})&#x3D;-\\sum_{j&#x3D;q}^{q}y_jlog\\hat{y}_j$</p>\n<p>通常被称为<em>交叉熵损失</em>（cross-entropy loss）</p>\n<h1 id=\"3-多层感知机\"><a href=\"#3-多层感知机\" class=\"headerlink\" title=\"3. 多层感知机\"></a>3. 多层感知机</h1><h2 id=\"3-1-激活函数\"><a href=\"#3-1-激活函数\" class=\"headerlink\" title=\"3.1 激活函数\"></a>3.1 激活函数</h2><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前$L-1$层看作表示，把最后一层看作线性预测器。 这种架构通常称为<em>多层感知机</em>（multilayer perceptron），通常缩写为<em>MLP</em>。</p>\n<p>多层感知机在输出层和输入层之间增加一个或多个全连接隐藏层，并通过激活函数转换隐藏层的输出。</p>\n<p><img src=\"https://zh.d2l.ai/_images/mlp.svg\" alt=\"../_images/mlp.svg\"></p>\n<p><em>激活函数</em>（activation function）通过计算加权和并加上偏置来确定神经元是否应该被激活， 它们将输入信号转换为输出的可微运算。 大多数激活函数都是非线性的。 由于激活函数是深度学习的基础，下面简要介绍一些常见的激活函数。</p>\n<ul>\n<li>ReLU</li>\n</ul>\n<p>$ReLU(x) &#x3D; max(x,0)$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_21_0.svg\" alt=\"../_images/output_mlp_76f463_21_0.svg\"></p>\n<ul>\n<li>sigmoid</li>\n</ul>\n<p>$sigmoid(x)&#x3D;\\frac{1}{1+exp(-x)}$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_51_0.svg\" alt=\"../_images/output_mlp_76f463_51_0.svg\"></p>\n<ul>\n<li>tanh</li>\n</ul>\n<p>$tanh(x) &#x3D; \\frac{1-exp(-2x)}{1+exp(-2x)}$</p>\n<p><img src=\"https://zh.d2l.ai/_images/output_mlp_76f463_81_0.svg\" alt=\"../_images/output_mlp_76f463_81_0.svg\"></p>\n<h2 id=\"3-2-误差\"><a href=\"#3-2-误差\" class=\"headerlink\" title=\"3.2 误差\"></a>3.2 误差</h2><p><em>训练误差</em>（training error）是指， 模型在训练数据集上计算得到的误差。 <em>泛化误差</em>（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。</p>\n<p>问题是，我们永远不能准确地计算出泛化误差。 这是因为无限多的数据样本是一个虚构的对象。 在实际中，我们只能通过将模型应用于一个独立的测试集来估计泛化误差， 该测试集由随机选取的、未曾在训练集中出现的数据样本构成。</p>\n<p><img src=\"https://zh.d2l.ai/_images/capacity-vs-error.svg\" alt=\"../_images/capacity-vs-error.svg\"></p>\n<h2 id=\"3-3-权重衰退\"><a href=\"#3-3-权重衰退\" class=\"headerlink\" title=\"3.3 权重衰退\"></a>3.3 权重衰退</h2><p>在训练参数化机器学习模型时， <em>权重衰减</em>（weight decay）是最广泛使用的正则化的技术之一， 它通常也被称为$L_2$正则化。</p>\n<p>一种简单的方法是通过线性函数$f(x)&#x3D;w^Tx$ 中的权重向量的某个范数来度量其复杂性， 例如$||w||^2$。 要保证权重向量比较小， 最常用方法是将其范数作为惩罚项加到最小化损失的问题中。 将原来的训练目标<em>最小化训练标签上的预测损失</em>， 调整为<em>最小化预测损失和惩罚项之和</em>。 现在，如果我们的权重向量增长的太大， 我们的学习算法可能会更集中于最小化权重范数$||w||^2$。 这正是我们想要的。</p>\n<p>我们通过<em>正则化常数</em>$\\lambda$来描述这种权衡， 这是一个非负超参数，我们使用验证数据拟合：</p>\n<p>$L(w,b)+\\frac{\\lambda}{2}||w||_2$</p>\n<p>此外，为什么我们首先使用$L_2$范数，而不是$L_1$范数。 事实上，这个选择在整个统计领域中都是有效的和受欢迎的。 $L_2$正则化线性模型构成经典的<em>岭回归</em>（ridge regression）算法， $L_1$正则化线性回归是统计学中类似的基本模型， 通常被称为<em>套索回归</em>（lasso regression）。 使用$L_2$范数的一个原因是它对权重向量的大分量施加了巨大的惩罚。 这使得我们的学习算法偏向于在大量特征上均匀分布权重的模型。 在实践中，这可能使它们对单个变量中的观测误差更为稳定。 相比之下，$L_1$惩罚会导致模型将权重集中在一小部分特征上， 而将其他权重清除为零。 这称为<em>特征选择</em>（feature selection），这可能是其他场景下需要的。</p>\n<p>$L_2$正则化回归的小批量随机梯度下降更新如下式：</p>\n<p>$w &#x3D; (1-\\alpha\\lambda) w - \\frac{\\alpha}{n}\\sum_{i&#x3D;1}^{n}(w^Tx^{(i)}+b-y^{(i)})$</p>\n<p>我们仅考虑惩罚项，优化算法在训练的每一步<em>衰减</em>权重。 与特征选择相比，权重衰减为我们提供了一种连续的机制来调整函数的复杂度。 较小的$\\lambda$值对应较少约束的$w$， 而较大的$\\lambda$值对$w$的约束更大。</p>\n<p>在<code>pytorch</code>中，我们在实例化优化器时直接通过<code>weight_decay</code>指定weight decay超参数。 默认情况下，PyTorch同时衰减权重和偏移。 这里我们只为权重设置了<code>weight_decay</code>，所以偏置参数$b$不会衰减。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">trainer = torch.optim.SGD([</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].weight,<span class=\"string\">&#x27;weight_decay&#x27;</span>: wd&#125;,</span><br><span class=\"line\">        &#123;<span class=\"string\">&quot;params&quot;</span>:net[<span class=\"number\">0</span>].bias&#125;], lr=lr)</span><br><span class=\"line\"><span class=\"comment\"># wd为lambda</span></span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>正则化是处理过拟合的常用方法：在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度。</li>\n<li>保持模型简单的一个特别的选择是使用$L_2$惩罚的权重衰减。这会导致学习算法更新步骤中的权重衰减。</li>\n<li>权重衰减功能在深度学习框架的优化器中提供。</li>\n<li>在同一训练代码实现中，不同的参数集可以有不同的更新行为。</li>\n</ul>\n<h2 id=\"3-4-丢弃法\"><a href=\"#3-4-丢弃法\" class=\"headerlink\" title=\"3.4 丢弃法\"></a>3.4 丢弃法</h2><p>当面对更多的特征而样本不足时，线性模型往往会过拟合。 相反，当给出更多样本而不是特征，通常线性模型不会过拟合。 不幸的是，线性模型泛化的可靠性是有代价的。 简单地说，线性模型没有考虑到特征之间的交互作用。 对于每个特征，线性模型必须指定正的或负的权重，而忽略其他特征。</p>\n<p>那么关键的挑战就是如何注入这种噪声。 一种想法是以一种<em>无偏向</em>（unbiased）的方式注入噪声。 这样在固定住其他层时，每一层的期望值等于没有噪音时的值。</p>\n<p>在每次训练迭代中，他将从均值为零的分布$\\epsilon~N(0,\\delta^2)$采样噪声添加到输入$x$， 从而产生扰动点$x’&#x3D;x+\\epsilon$， 预期是$E(x’)&#x3D;x$。</p>\n<p>在标准暂退法正则化中，通过按保留（未丢弃）的节点的分数进行规范化来消除每一层的偏差。 换言之，每个中间活性值ℎ以<em>暂退概率</em>$p$由随机变量ℎ′替换，如下所示：</p>\n<p>$h’&#x3D; 0 \\ when\\ p&#x3D;0$</p>\n<p>$h’&#x3D;\\frac{h}{1-p} \\ otherwise$</p>\n<p><img src=\"https://zh.d2l.ai/_images/dropout2.svg\" alt=\"../_images/dropout2.svg\"></p>\n<p>对于深度学习框架的高级API，我们只需在每个全连接层之后添加一个<code>Dropout</code>层， 将暂退概率作为唯一的参数传递给它的构造函数。 在训练时，<code>Dropout</code>层将根据指定的暂退概率随机丢弃上一层的输出（相当于下一层的输入）。 在测试时，<code>Dropout</code>层仅传递数据。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">net = nn.Sequential(nn.Flatten(),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">784</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第一个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout1),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">256</span>),</span><br><span class=\"line\">        nn.ReLU(),</span><br><span class=\"line\">        <span class=\"comment\"># 在第二个全连接层之后添加一个dropout层</span></span><br><span class=\"line\">        nn.Dropout(dropout2),</span><br><span class=\"line\">        nn.Linear(<span class=\"number\">256</span>, <span class=\"number\">10</span>))</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>暂退法在前向传播过程中，计算每一内部层的同时丢弃一些神经元。</li>\n<li>暂退法可以避免过拟合，它通常与控制权重向量的维数和大小结合使用的。</li>\n<li>暂退法将活性值ℎ替换为具有期望值ℎ的随机变量。</li>\n<li>暂退法仅在训练期间使用。</li>\n</ul>\n<h2 id=\"3-5-正向传播、反向传播、计算图\"><a href=\"#3-5-正向传播、反向传播、计算图\" class=\"headerlink\" title=\"3.5 正向传播、反向传播、计算图\"></a>3.5 正向传播、反向传播、计算图</h2><ul>\n<li>前向传播在神经网络定义的计算图中按顺序计算和存储中间变量，它的顺序是从输入层到输出层。</li>\n<li>反向传播按相反的顺序（从输出层到输入层）计算和存储神经网络的中间变量和参数的梯度。</li>\n<li>在训练深度学习模型时，前向传播和反向传播是相互依赖的。</li>\n<li>训练比预测需要更多的内存。</li>\n</ul>\n"},{"title":"从0开始的Cpp学习笔记","mathjax":true,"date":"+020235-04-26T12:46:25.000Z","img":"https://i2.hdslb.com/bfs/archive/6487769f0e49718a24c293df29bd840be0ca2e9c.png","excerpt":"RT","_content":"windows: vs开发桌面c++，安装略\n\nCS106L：\n\n编译语言&解释语言\n\nC++是静态类型语言，变量类型不可变。\n\nusing关键词解决很长的类型表达，auto自动选择类型，仍然是静态类型\n\n# C++是如何工作的？\n\n源代码（.cpp和.h文件）-> 预处理（处理#开头指令，包括头文件展开，宏替换，条件编译等，生成.i或.ii文件） -> 编译（将预处理文件转化成机器可读的目标文件，先变成汇编文件.s，再变成目标文件.o或.obj） -> 链接（合并目标文件和库，生成可执行文件）\n\n## 编译器如何工作？\n\n一个cpp文件通常是一个翻译单元（有时是多个），每个翻译单元单独编译，生成一个obj目标文件\n\n首先进行预处理，处理预处理语句（include，if endif ，pragma）\n\n- include：把头文件的内容直接复制到当前文件位置\n- if-endif：把符合条件的代码选择上\n\n然后将预处理后的代码（.i文件）编译成机器语言，即.obj文件里都是十六进制指令（中间先汇编成汇编语言）\n\n## 链接如何工作？\n\n将每个obj文件合并起来，注意合并后必须要有入口函数（main），不然会在链接阶段报错（LNK，编译阶段是C），\n\n如果要使用（在文件中定义了调用）其他文件的函数，需要在main函数的文件里面定义函数，不然在链接阶段会报错\n\n如果有两个一样的函数（返回值，参数，名称），链接器就不知道你要调用的是哪一个函数，会报错\n\n（例如两个文件include同一个库文件，其中的库函数就一样）\n\n","source":"_posts/learning-cpp-notes.md","raw":"---\ntitle: 从0开始的Cpp学习笔记\nmathjax: true\ndate: 20235/4/26 20:46:25\nimg: https://i2.hdslb.com/bfs/archive/6487769f0e49718a24c293df29bd840be0ca2e9c.png\nexcerpt: RT\n---\nwindows: vs开发桌面c++，安装略\n\nCS106L：\n\n编译语言&解释语言\n\nC++是静态类型语言，变量类型不可变。\n\nusing关键词解决很长的类型表达，auto自动选择类型，仍然是静态类型\n\n# C++是如何工作的？\n\n源代码（.cpp和.h文件）-> 预处理（处理#开头指令，包括头文件展开，宏替换，条件编译等，生成.i或.ii文件） -> 编译（将预处理文件转化成机器可读的目标文件，先变成汇编文件.s，再变成目标文件.o或.obj） -> 链接（合并目标文件和库，生成可执行文件）\n\n## 编译器如何工作？\n\n一个cpp文件通常是一个翻译单元（有时是多个），每个翻译单元单独编译，生成一个obj目标文件\n\n首先进行预处理，处理预处理语句（include，if endif ，pragma）\n\n- include：把头文件的内容直接复制到当前文件位置\n- if-endif：把符合条件的代码选择上\n\n然后将预处理后的代码（.i文件）编译成机器语言，即.obj文件里都是十六进制指令（中间先汇编成汇编语言）\n\n## 链接如何工作？\n\n将每个obj文件合并起来，注意合并后必须要有入口函数（main），不然会在链接阶段报错（LNK，编译阶段是C），\n\n如果要使用（在文件中定义了调用）其他文件的函数，需要在main函数的文件里面定义函数，不然在链接阶段会报错\n\n如果有两个一样的函数（返回值，参数，名称），链接器就不知道你要调用的是哪一个函数，会报错\n\n（例如两个文件include同一个库文件，其中的库函数就一样）\n\n","slug":"learning-cpp-notes","published":1,"updated":"2025-04-28T15:49:43.357Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9g000bbs998193051l","content":"<p>windows: vs开发桌面c++，安装略</p>\n<p>CS106L：</p>\n<p>编译语言&amp;解释语言</p>\n<p>C++是静态类型语言，变量类型不可变。</p>\n<p>using关键词解决很长的类型表达，auto自动选择类型，仍然是静态类型</p>\n<h1 id=\"C-是如何工作的？\"><a href=\"#C-是如何工作的？\" class=\"headerlink\" title=\"C++是如何工作的？\"></a>C++是如何工作的？</h1><p>源代码（.cpp和.h文件）-&gt; 预处理（处理#开头指令，包括头文件展开，宏替换，条件编译等，生成.i或.ii文件） -&gt; 编译（将预处理文件转化成机器可读的目标文件，先变成汇编文件.s，再变成目标文件.o或.obj） -&gt; 链接（合并目标文件和库，生成可执行文件）</p>\n<h2 id=\"编译器如何工作？\"><a href=\"#编译器如何工作？\" class=\"headerlink\" title=\"编译器如何工作？\"></a>编译器如何工作？</h2><p>一个cpp文件通常是一个翻译单元（有时是多个），每个翻译单元单独编译，生成一个obj目标文件</p>\n<p>首先进行预处理，处理预处理语句（include，if endif ，pragma）</p>\n<ul>\n<li>include：把头文件的内容直接复制到当前文件位置</li>\n<li>if-endif：把符合条件的代码选择上</li>\n</ul>\n<p>然后将预处理后的代码（.i文件）编译成机器语言，即.obj文件里都是十六进制指令（中间先汇编成汇编语言）</p>\n<h2 id=\"链接如何工作？\"><a href=\"#链接如何工作？\" class=\"headerlink\" title=\"链接如何工作？\"></a>链接如何工作？</h2><p>将每个obj文件合并起来，注意合并后必须要有入口函数（main），不然会在链接阶段报错（LNK，编译阶段是C），</p>\n<p>如果要使用（在文件中定义了调用）其他文件的函数，需要在main函数的文件里面定义函数，不然在链接阶段会报错</p>\n<p>如果有两个一样的函数（返回值，参数，名称），链接器就不知道你要调用的是哪一个函数，会报错</p>\n<p>（例如两个文件include同一个库文件，其中的库函数就一样）</p>\n","more":"<p>windows: vs开发桌面c++，安装略</p>\n<p>CS106L：</p>\n<p>编译语言&amp;解释语言</p>\n<p>C++是静态类型语言，变量类型不可变。</p>\n<p>using关键词解决很长的类型表达，auto自动选择类型，仍然是静态类型</p>\n<h1 id=\"C-是如何工作的？\"><a href=\"#C-是如何工作的？\" class=\"headerlink\" title=\"C++是如何工作的？\"></a>C++是如何工作的？</h1><p>源代码（.cpp和.h文件）-&gt; 预处理（处理#开头指令，包括头文件展开，宏替换，条件编译等，生成.i或.ii文件） -&gt; 编译（将预处理文件转化成机器可读的目标文件，先变成汇编文件.s，再变成目标文件.o或.obj） -&gt; 链接（合并目标文件和库，生成可执行文件）</p>\n<h2 id=\"编译器如何工作？\"><a href=\"#编译器如何工作？\" class=\"headerlink\" title=\"编译器如何工作？\"></a>编译器如何工作？</h2><p>一个cpp文件通常是一个翻译单元（有时是多个），每个翻译单元单独编译，生成一个obj目标文件</p>\n<p>首先进行预处理，处理预处理语句（include，if endif ，pragma）</p>\n<ul>\n<li>include：把头文件的内容直接复制到当前文件位置</li>\n<li>if-endif：把符合条件的代码选择上</li>\n</ul>\n<p>然后将预处理后的代码（.i文件）编译成机器语言，即.obj文件里都是十六进制指令（中间先汇编成汇编语言）</p>\n<h2 id=\"链接如何工作？\"><a href=\"#链接如何工作？\" class=\"headerlink\" title=\"链接如何工作？\"></a>链接如何工作？</h2><p>将每个obj文件合并起来，注意合并后必须要有入口函数（main），不然会在链接阶段报错（LNK，编译阶段是C），</p>\n<p>如果要使用（在文件中定义了调用）其他文件的函数，需要在main函数的文件里面定义函数，不然在链接阶段会报错</p>\n<p>如果有两个一样的函数（返回值，参数，名称），链接器就不知道你要调用的是哪一个函数，会报错</p>\n<p>（例如两个文件include同一个库文件，其中的库函数就一样）</p>\n"},{"title":"Hello World","mathjax":true,"date":"2023-01-22T12:46:25.000Z","img":"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png","excerpt":"我的halo word","_content":"勇神牛逼！！！！\n\n测试latex\n\n$sin(\\alpha + \\beta)$\n\n测试图片\n\n![img](/img/hello-world/Kaz.jpg)\n\n![Transformer](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png)\n\n测试代码块\n\n```python\nimport torch\n#attention is all u need\nprint(\"Yong shen NB\")\n```\n\n```c++\n#include <iostream>\nusing namespace std;\nint main(){\n    cout<<\"勇神牛逼\";\n    pair<int, int> p;\n    int get = [&](int l, int r){\n    \treturn l + r >> 1;  \n    };\n    return 0;\n}\n```\n\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\nmathjax: true\ndate: 2023/1/22 20:46:25\nimg: https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\nexcerpt: 我的halo word\n---\n勇神牛逼！！！！\n\n测试latex\n\n$sin(\\alpha + \\beta)$\n\n测试图片\n\n![img](/img/hello-world/Kaz.jpg)\n\n![Transformer](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png)\n\n测试代码块\n\n```python\nimport torch\n#attention is all u need\nprint(\"Yong shen NB\")\n```\n\n```c++\n#include <iostream>\nusing namespace std;\nint main(){\n    cout<<\"勇神牛逼\";\n    pair<int, int> p;\n    int get = [&](int l, int r){\n    \treturn l + r >> 1;  \n    };\n    return 0;\n}\n```\n\n","slug":"hello-world","published":1,"updated":"2025-02-17T12:39:42.971Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9g000cbs993yxz3i48","content":"<p>勇神牛逼！！！！</p>\n<p>测试latex</p>\n<p>$sin(\\alpha + \\beta)$</p>\n<p>测试图片</p>\n<p><img src=\"/img/hello-world/Kaz.jpg\" class=\"lazyload placeholder\" data-srcset=\"/img/hello-world/Kaz.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" class=\"lazyload placeholder\" data-srcset=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"Transformer\"></p>\n<p>测试代码块</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#attention is all u need</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Yong shen NB&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"string\">&quot;勇神牛逼&quot;</span>;</span><br><span class=\"line\">    pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; p;</span><br><span class=\"line\">    <span class=\"type\">int</span> get = [&amp;](<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">return</span> l + r &gt;&gt; <span class=\"number\">1</span>;  </span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","more":"<p>勇神牛逼！！！！</p>\n<p>测试latex</p>\n<p>$sin(\\alpha + \\beta)$</p>\n<p>测试图片</p>\n<p><img src=\"/img/hello-world/Kaz.jpg\" alt=\"img\"></p>\n<p><img src=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" alt=\"Transformer\"></p>\n<p>测试代码块</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#attention is all u need</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Yong shen NB&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"string\">&quot;勇神牛逼&quot;</span>;</span><br><span class=\"line\">    pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; p;</span><br><span class=\"line\">    <span class=\"type\">int</span> get = [&amp;](<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">return</span> l + r &gt;&gt; <span class=\"number\">1</span>;  </span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Attention Is All You Need阅读笔记","mathjax":true,"date":"2025-03-11T12:46:25.000Z","img":"https://img0.baidu.com/it/u=3520508,2967101156&fm=253&fmt=auto&app=138&f=JPEG?w=786&h=500","excerpt":"transformer论文阅读笔记","_content":"\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png)\n\n# 之前的工作\n\nRNN 存在长期依赖问题（隐含信息一直传递到后面会消失），且无法并行\n\nLSTM 通过遗忘门，输入门，输出门解决长期依赖问题，但是还是无法并行\n\nTransformer仅仅依赖注意力机制（Attention），不需要考虑两个词离得多远，且能实现并行\n\n# 模型架构\n\n输入$(x_1,x_2,...,x_n)$，编码器输出$(z_1,z_2,...,z_n)$，输入到解码器输出$(y_1,y_2,...,y_m)$\n\n每一步都是自回归的（编码的时候可以看到一整个句子，但是在解码的时候只能看到已经生成好的句子，叫做自回归，它将时间序列的当前值表示为过去若干个值的线性组合)\n\n![pic-1](/img/transformer-notes/pic-1.png)\n\n## 编码器\n\nN=6，每一层有两个子层：多头注意力层和前馈神经网络层，每个子层后面还有一个残差网络和层归一化(Add & Norm)\n\n## 解码器\n\nN=6，在输入添加一个掩码多头注意力\n\n## 注意力机制\n\n通过Q，K，V，计算两个词的相似度\n\nTransformer采用自注意力机制\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/QKV-%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.jpg)\n\n## Scaled Dot-Product Attention\n\nQ，K的维度都是$d_k$，V的维度是$d_v$\n\nQ，K做内积，再除以$\\sqrt{d_k}$，做一层`softmax`就是V的权重\n\n$Attention(Q,K,V) = softmax( \\frac{QK^T}{\\sqrt{d_k}}  )V$\n\n为了防止softmax的梯度很小减慢训练速度，所以处理$\\sqrt{d_k}$\n\n![pic-2](/img/transformer-notes/pic-2.png)\n\n## 多头自注意力机制\n\nh=8\n\n**多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息**。\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg)\n\n## 其他\n\n在Encoder-Decoder注意力层\n\nKV来自编码器，Q来自解码器\n\n掩码多头注意力时，点积设置为-∞\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg)\n\n## 前馈神经网络\n\n由两个线性回归和一个ReLU的全连接层\n\n$FFN(x) = ReLU(xW1 + b1)W2 + b2$\n\n内层的维度2048\n\n（先由attention计算相似度拿到感兴趣的信息，语义空间在MLP隐藏层里面转化为2048维）\n\n## 嵌入层和Softmax\n\n在两个嵌入层的矩阵参数选择一样的，然后再乘以$\\sqrt{d_{model}}$\n\n（可能由于L2正则化权重值很小，下面还要和位置编码相加，保证两个向量的scale差不多，所以乘）\n\n## 位置编码\n\n**由于 Attention 值的计算最终会被加权求和，也就是说两者最终计算的 Attention 值都是一样的，进而也就表明了 Attention 丢掉了 X1的序列顺序信息。**\n\nAttention自己是没有包含时序的信息的\n所以要有位置编码\n\n$PE(pos,2i) = sin(pos/10000^{2i/d_{model}})$\n\n$PE(pos,2i + 1) = cos(pos/10000^{2i/d_{model}})$\n\nPE都在[-1,1]且$PE_{pos_k}$是$PE_{pos}$的线性组合\n\n**某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。**\n\n$X_{final\\_embedding}=Embedding+PositionalEmbedding$\n","source":"_posts/transformer-notes.md","raw":"---\ntitle: Attention Is All You Need阅读笔记\nmathjax: true\ndate: 2025/3/11 20:46:25\nimg: https://img0.baidu.com/it/u=3520508,2967101156&fm=253&fmt=auto&app=138&f=JPEG?w=786&h=500\nexcerpt: transformer论文阅读笔记\n---\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png)\n\n# 之前的工作\n\nRNN 存在长期依赖问题（隐含信息一直传递到后面会消失），且无法并行\n\nLSTM 通过遗忘门，输入门，输出门解决长期依赖问题，但是还是无法并行\n\nTransformer仅仅依赖注意力机制（Attention），不需要考虑两个词离得多远，且能实现并行\n\n# 模型架构\n\n输入$(x_1,x_2,...,x_n)$，编码器输出$(z_1,z_2,...,z_n)$，输入到解码器输出$(y_1,y_2,...,y_m)$\n\n每一步都是自回归的（编码的时候可以看到一整个句子，但是在解码的时候只能看到已经生成好的句子，叫做自回归，它将时间序列的当前值表示为过去若干个值的线性组合)\n\n![pic-1](/img/transformer-notes/pic-1.png)\n\n## 编码器\n\nN=6，每一层有两个子层：多头注意力层和前馈神经网络层，每个子层后面还有一个残差网络和层归一化(Add & Norm)\n\n## 解码器\n\nN=6，在输入添加一个掩码多头注意力\n\n## 注意力机制\n\n通过Q，K，V，计算两个词的相似度\n\nTransformer采用自注意力机制\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/QKV-%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.jpg)\n\n## Scaled Dot-Product Attention\n\nQ，K的维度都是$d_k$，V的维度是$d_v$\n\nQ，K做内积，再除以$\\sqrt{d_k}$，做一层`softmax`就是V的权重\n\n$Attention(Q,K,V) = softmax( \\frac{QK^T}{\\sqrt{d_k}}  )V$\n\n为了防止softmax的梯度很小减慢训练速度，所以处理$\\sqrt{d_k}$\n\n![pic-2](/img/transformer-notes/pic-2.png)\n\n## 多头自注意力机制\n\nh=8\n\n**多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息**。\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg)\n\n## 其他\n\n在Encoder-Decoder注意力层\n\nKV来自编码器，Q来自解码器\n\n掩码多头注意力时，点积设置为-∞\n\n![img](https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg)\n\n## 前馈神经网络\n\n由两个线性回归和一个ReLU的全连接层\n\n$FFN(x) = ReLU(xW1 + b1)W2 + b2$\n\n内层的维度2048\n\n（先由attention计算相似度拿到感兴趣的信息，语义空间在MLP隐藏层里面转化为2048维）\n\n## 嵌入层和Softmax\n\n在两个嵌入层的矩阵参数选择一样的，然后再乘以$\\sqrt{d_{model}}$\n\n（可能由于L2正则化权重值很小，下面还要和位置编码相加，保证两个向量的scale差不多，所以乘）\n\n## 位置编码\n\n**由于 Attention 值的计算最终会被加权求和，也就是说两者最终计算的 Attention 值都是一样的，进而也就表明了 Attention 丢掉了 X1的序列顺序信息。**\n\nAttention自己是没有包含时序的信息的\n所以要有位置编码\n\n$PE(pos,2i) = sin(pos/10000^{2i/d_{model}})$\n\n$PE(pos,2i + 1) = cos(pos/10000^{2i/d_{model}})$\n\nPE都在[-1,1]且$PE_{pos_k}$是$PE_{pos}$的线性组合\n\n**某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。**\n\n$X_{final\\_embedding}=Embedding+PositionalEmbedding$\n","slug":"transformer-notes","published":1,"updated":"2025-03-15T07:26:33.982Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9g000dbs99g92i6iqr","content":"<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png\" class=\"lazyload placeholder\" data-srcset=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h1 id=\"之前的工作\"><a href=\"#之前的工作\" class=\"headerlink\" title=\"之前的工作\"></a>之前的工作</h1><p>RNN 存在长期依赖问题（隐含信息一直传递到后面会消失），且无法并行</p>\n<p>LSTM 通过遗忘门，输入门，输出门解决长期依赖问题，但是还是无法并行</p>\n<p>Transformer仅仅依赖注意力机制（Attention），不需要考虑两个词离得多远，且能实现并行</p>\n<h1 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h1><p>输入$(x_1,x_2,…,x_n)$，编码器输出$(z_1,z_2,…,z_n)$，输入到解码器输出$(y_1,y_2,…,y_m)$</p>\n<p>每一步都是自回归的（编码的时候可以看到一整个句子，但是在解码的时候只能看到已经生成好的句子，叫做自回归，它将时间序列的当前值表示为过去若干个值的线性组合)</p>\n<p><img src=\"/img/transformer-notes/pic-1.png\" class=\"lazyload placeholder\" data-srcset=\"/img/transformer-notes/pic-1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"pic-1\"></p>\n<h2 id=\"编码器\"><a href=\"#编码器\" class=\"headerlink\" title=\"编码器\"></a>编码器</h2><p>N&#x3D;6，每一层有两个子层：多头注意力层和前馈神经网络层，每个子层后面还有一个残差网络和层归一化(Add &amp; Norm)</p>\n<h2 id=\"解码器\"><a href=\"#解码器\" class=\"headerlink\" title=\"解码器\"></a>解码器</h2><p>N&#x3D;6，在输入添加一个掩码多头注意力</p>\n<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>通过Q，K，V，计算两个词的相似度</p>\n<p>Transformer采用自注意力机制</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/QKV-%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/QKV-%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"Scaled-Dot-Product-Attention\"><a href=\"#Scaled-Dot-Product-Attention\" class=\"headerlink\" title=\"Scaled Dot-Product Attention\"></a>Scaled Dot-Product Attention</h2><p>Q，K的维度都是$d_k$，V的维度是$d_v$</p>\n<p>Q，K做内积，再除以$\\sqrt{d_k}$，做一层<code>softmax</code>就是V的权重</p>\n<p>$Attention(Q,K,V) &#x3D; softmax( \\frac{QK^T}{\\sqrt{d_k}}  )V$</p>\n<p>为了防止softmax的梯度很小减慢训练速度，所以处理$\\sqrt{d_k}$</p>\n<p><img src=\"/img/transformer-notes/pic-2.png\" class=\"lazyload placeholder\" data-srcset=\"/img/transformer-notes/pic-2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"pic-2\"></p>\n<h2 id=\"多头自注意力机制\"><a href=\"#多头自注意力机制\" class=\"headerlink\" title=\"多头自注意力机制\"></a>多头自注意力机制</h2><p>h&#x3D;8</p>\n<p><strong>多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息</strong>。</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>在Encoder-Decoder注意力层</p>\n<p>KV来自编码器，Q来自解码器</p>\n<p>掩码多头注意力时，点积设置为-∞</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg\" class=\"lazyload placeholder\" data-srcset=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h2><p>由两个线性回归和一个ReLU的全连接层</p>\n<p>$FFN(x) &#x3D; ReLU(xW1 + b1)W2 + b2$</p>\n<p>内层的维度2048</p>\n<p>（先由attention计算相似度拿到感兴趣的信息，语义空间在MLP隐藏层里面转化为2048维）</p>\n<h2 id=\"嵌入层和Softmax\"><a href=\"#嵌入层和Softmax\" class=\"headerlink\" title=\"嵌入层和Softmax\"></a>嵌入层和Softmax</h2><p>在两个嵌入层的矩阵参数选择一样的，然后再乘以$\\sqrt{d_{model}}$</p>\n<p>（可能由于L2正则化权重值很小，下面还要和位置编码相加，保证两个向量的scale差不多，所以乘）</p>\n<h2 id=\"位置编码\"><a href=\"#位置编码\" class=\"headerlink\" title=\"位置编码\"></a>位置编码</h2><p><strong>由于 Attention 值的计算最终会被加权求和，也就是说两者最终计算的 Attention 值都是一样的，进而也就表明了 Attention 丢掉了 X1的序列顺序信息。</strong></p>\n<p>Attention自己是没有包含时序的信息的<br>所以要有位置编码</p>\n<p>$PE(pos,2i) &#x3D; sin(pos&#x2F;10000^{2i&#x2F;d_{model}})$</p>\n<p>$PE(pos,2i + 1) &#x3D; cos(pos&#x2F;10000^{2i&#x2F;d_{model}})$</p>\n<p>PE都在[-1,1]且$PE_{pos_k}$是$PE_{pos}$的线性组合</p>\n<p><strong>某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。</strong></p>\n<p>$X_{final_embedding}&#x3D;Embedding+PositionalEmbedding$</p>\n","more":"<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/attention-%E8%AE%A1%E7%AE%97%E5%9B%BE.png\" alt=\"img\"></p>\n<h1 id=\"之前的工作\"><a href=\"#之前的工作\" class=\"headerlink\" title=\"之前的工作\"></a>之前的工作</h1><p>RNN 存在长期依赖问题（隐含信息一直传递到后面会消失），且无法并行</p>\n<p>LSTM 通过遗忘门，输入门，输出门解决长期依赖问题，但是还是无法并行</p>\n<p>Transformer仅仅依赖注意力机制（Attention），不需要考虑两个词离得多远，且能实现并行</p>\n<h1 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h1><p>输入$(x_1,x_2,…,x_n)$，编码器输出$(z_1,z_2,…,z_n)$，输入到解码器输出$(y_1,y_2,…,y_m)$</p>\n<p>每一步都是自回归的（编码的时候可以看到一整个句子，但是在解码的时候只能看到已经生成好的句子，叫做自回归，它将时间序列的当前值表示为过去若干个值的线性组合)</p>\n<p><img src=\"/img/transformer-notes/pic-1.png\" alt=\"pic-1\"></p>\n<h2 id=\"编码器\"><a href=\"#编码器\" class=\"headerlink\" title=\"编码器\"></a>编码器</h2><p>N&#x3D;6，每一层有两个子层：多头注意力层和前馈神经网络层，每个子层后面还有一个残差网络和层归一化(Add &amp; Norm)</p>\n<h2 id=\"解码器\"><a href=\"#解码器\" class=\"headerlink\" title=\"解码器\"></a>解码器</h2><p>N&#x3D;6，在输入添加一个掩码多头注意力</p>\n<h2 id=\"注意力机制\"><a href=\"#注意力机制\" class=\"headerlink\" title=\"注意力机制\"></a>注意力机制</h2><p>通过Q，K，V，计算两个词的相似度</p>\n<p>Transformer采用自注意力机制</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/QKV-%E7%9F%A9%E9%98%B5%E8%A1%A8%E7%A4%BA.jpg\" alt=\"img\"></p>\n<h2 id=\"Scaled-Dot-Product-Attention\"><a href=\"#Scaled-Dot-Product-Attention\" class=\"headerlink\" title=\"Scaled Dot-Product Attention\"></a>Scaled Dot-Product Attention</h2><p>Q，K的维度都是$d_k$，V的维度是$d_v$</p>\n<p>Q，K做内积，再除以$\\sqrt{d_k}$，做一层<code>softmax</code>就是V的权重</p>\n<p>$Attention(Q,K,V) &#x3D; softmax( \\frac{QK^T}{\\sqrt{d_k}}  )V$</p>\n<p>为了防止softmax的梯度很小减慢训练速度，所以处理$\\sqrt{d_k}$</p>\n<p><img src=\"/img/transformer-notes/pic-2.png\" alt=\"pic-2\"></p>\n<h2 id=\"多头自注意力机制\"><a href=\"#多头自注意力机制\" class=\"headerlink\" title=\"多头自注意力机制\"></a>多头自注意力机制</h2><p>h&#x3D;8</p>\n<p><strong>多头相当于把原始信息 Source 放入了多个子空间中，也就是捕捉了多个信息，对于使用 multi-head（多头） attention 的简单回答就是，多头保证了 attention 可以注意到不同子空间的信息，捕捉到更加丰富的特征信息</strong>。</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/multi-head-%E6%8B%BC%E6%8E%A5.jpg\" alt=\"img\"></p>\n<h2 id=\"其他\"><a href=\"#其他\" class=\"headerlink\" title=\"其他\"></a>其他</h2><p>在Encoder-Decoder注意力层</p>\n<p>KV来自编码器，Q来自解码器</p>\n<p>掩码多头注意力时，点积设置为-∞</p>\n<p><img src=\"https://imgmd.oss-cn-shanghai.aliyuncs.com/BERT_IMG/mask-attention-map-softmax.jpg\" alt=\"img\"></p>\n<h2 id=\"前馈神经网络\"><a href=\"#前馈神经网络\" class=\"headerlink\" title=\"前馈神经网络\"></a>前馈神经网络</h2><p>由两个线性回归和一个ReLU的全连接层</p>\n<p>$FFN(x) &#x3D; ReLU(xW1 + b1)W2 + b2$</p>\n<p>内层的维度2048</p>\n<p>（先由attention计算相似度拿到感兴趣的信息，语义空间在MLP隐藏层里面转化为2048维）</p>\n<h2 id=\"嵌入层和Softmax\"><a href=\"#嵌入层和Softmax\" class=\"headerlink\" title=\"嵌入层和Softmax\"></a>嵌入层和Softmax</h2><p>在两个嵌入层的矩阵参数选择一样的，然后再乘以$\\sqrt{d_{model}}$</p>\n<p>（可能由于L2正则化权重值很小，下面还要和位置编码相加，保证两个向量的scale差不多，所以乘）</p>\n<h2 id=\"位置编码\"><a href=\"#位置编码\" class=\"headerlink\" title=\"位置编码\"></a>位置编码</h2><p><strong>由于 Attention 值的计算最终会被加权求和，也就是说两者最终计算的 Attention 值都是一样的，进而也就表明了 Attention 丢掉了 X1的序列顺序信息。</strong></p>\n<p>Attention自己是没有包含时序的信息的<br>所以要有位置编码</p>\n<p>$PE(pos,2i) &#x3D; sin(pos&#x2F;10000^{2i&#x2F;d_{model}})$</p>\n<p>$PE(pos,2i + 1) &#x3D; cos(pos&#x2F;10000^{2i&#x2F;d_{model}})$</p>\n<p>PE都在[-1,1]且$PE_{pos_k}$是$PE_{pos}$的线性组合</p>\n<p><strong>某个单词的位置信息是其他单词位置信息的线性组合，这种线性组合就意味着位置向量中蕴含了相对位置信息。</strong></p>\n<p>$X_{final_embedding}&#x3D;Embedding+PositionalEmbedding$</p>\n"},{"title":"Machine Learning Notes","mathjax":true,"date":"2023-11-25T12:46:25.000Z","img":"https://img2.baidu.com/it/u=1093757134,3274186314&fm=253&fmt=auto&app=120&f=JPEG?w=800&h=500","excerpt":"吴恩达机器学习视频笔记","_content":"\n# Course 1\n\n监督学习：输入特征x，输出目标y。对数据集进行预测，分为**回归**和**分类**\n\n无监督学习：输入特征x，没有目标y，对数据集进行**聚类预测**，**异常检测**，**降维**\n\n## 线性回归\n\n$$\ny^i = wx^i+b\n$$\n\n定义损失函数（成本函数），需要最小化损失函数\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2\n$$\n\n其中$y^i$为真实输出，$\\hat{y}$为预测输出\n\n- 为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差\n- 1/2是为了方便求导计算\n\nloss针对一个训练样本，cost是所有训练样本的均值\n\n### 梯度下降\n\n需要最小会损失函数，需要使用梯度下降算法\n\n定义学习率`learning_rate`为$\\alpha$,一般$\\alpha \\subseteq [0,1]$\n\n$w = w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$\n\n$b = b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$\n\n- 梯度下降时建议**同步**梯度下降，如下图\n\n![img](/img/machine-learning-notes/pic-1.png)\n\n如果$\\alpha$太小，可以得到答案，但是时间过长\n\n如果$\\alpha$太大，大交叉无法收敛，甚至发散\n\n当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解\n\n对导数项分别求导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = wx^i+b$\n\n对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值\n\n选择合适得到学习率，就可以得到$min(J(w,b))$\n\n线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例\n\n### 多元线性回归\n\n假设特征有$n$个，定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n则$f_{\\vec{w},b}=\\vec{w} \\cdot \\vec{x} +b$\n\n`·`为两个向量的点积(dot)。\n\n$$\n\\vec{w} \\cdot \\vec{x} = w_1*x_1+w_2*x_2+....+w_n*x_n\n$$\n\n\n**矢量化**：利用计算机的并行硬件，代码简洁、运行速度快\n\n```python\nf = np.dot(w, x) + b\n```\n\n**多元线性回归的梯度下降**\n\n![img](/img/machine-learning-notes/pic-2.png)\n\nPS: 正规方程：某些机器学习库在后端求$w,b$的方法，**只适用于线性回归**，而且速度慢，不要求掌握\n\n### 特征缩放\n\n不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动\n\n加快梯度下降速度\n\n避免特征的取值范围差异过大，将其进行缩放，几种常见方法：\n\n- **除以最大值**，$x_{1,scale} = \\frac{x_1}{max}$， $x \\in [0,1]$\n- **均值归一化Mean Normalization**\n  - 求均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{max-min}$\n- **`Z-score`归一化**\n  - 求标准差$\\sigma$，均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{\\sigma}$\n\n**判断梯度下降是否收敛：**\n\n1. 观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）\n\n**选择合适学习率**：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$\n\n### 特征工程\n\n利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测\n\n**多项式回归**：可以添加$x^q$项更好地拟合数据图像，$f(x)=w_1x^3+w_2x^2+w_1x^1+b$\n\n此时特征缩放尤为重要\n\n## 分类-逻辑回归\n\n解决二分类问题\n\n### sigmoid函数\n\n输出介于$(0,1)$\n\n$g(z)= \\frac{1}{1+e^{-z}},z \\subseteq R$\n\n**logistic regression**:\n\n$f_{\\vec{w},b}(\\vec{x})=g(\\vec{w} · \\vec{x}+b) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n输出值可以理解为分类为1的可能性\n\n$f_{\\vec{w},b}(\\vec{x})=P(y=1|\\vec{x};\\vec{w},b)$\n\n### 决策边界decision boundary\n\n以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b <0$，取值0\n\n$\\vec{w} · \\vec{x}+b = 0$称为决策边界\n\n多项式回归也适用于非线性的决策边界\n\n### 成本函数\n\n如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$**不是凸函数，不适用于逻辑回归**\n\n定义\n\n$$\nJ(w,b)=\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})\n$$\n其中L代表单个样例的loss，J代表总的cost\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}=1\n$$\n![img](/img/machine-learning-notes/pic-3.png)\n\n当y等于1，预测值越靠近1损失越小\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}=0\n$$\n![img](/img/machine-learning-notes/pic-4.png)\n\n当y等于0，预测值越靠近0损失越小 \n\n**简化**成本函数                                                                                                                                                                                                                                                                                          \n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n得到\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))\n$$\n\n\n成本函数是凸函数，便于实现梯度下降\n\n### 梯度下降\n\n对J求偏导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n可以使用相似方法进行特征缩放\n\n### 过拟合问题\n\n过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。\n\n![img](/img/machine-learning-notes/pic-5.png)\n\n**解决过拟合**\n\n- 收集更多训练数据\n- 特征筛选，选择特征的一个子集\n- 正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响\n\n### 正则化\n\n如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n这样使得$w_j$尽可能小，几乎为0\n\n参数$b$是否正则化无关紧要\n\n**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显\n\n- 正则化线性回归\n\n对$J(w,b)$求偏导不断同步更新w,b的值\n\n$$\n\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}\n$$\n\n$$\nw = w- \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}) = (1-\\alpha \\frac{\\lambda}{m})w+.....\n$$\n\n\n- 正则化逻辑回归\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n\n求导式和线性回归相同，只是需要注意**正则化项偏导数没有求和**\n\n$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n![img](/img/machine-learning-notes/pic-6.png)\n\n# Course 2\n\n## 神经网络\n\n起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为**深度学习**\n\n利用别人训练的神经网络参数称为推理或者预测\n\n为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数\n\n神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层->隐藏层->输出层）\n\n多层神经网络叫做多层感知机\n\n## 神经网络中的层\n\n讨论层数通常是隐藏层和输出层，不包括输入层\n\n每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层/输出结果\n\n即$a_j^{[l]} = g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$\n\n$j$表示神经元单元序号，$l$表示层数，$g(x)$为`sigmod`函数\n\n![img](/img/machine-learning-notes/pic-7.jpg)\n\n$a_j^{[l]}$构成$\\vec{a}^{[l]}$\n\n![img](/img/machine-learning-notes/pic-8.png)\n\n## 前向传播(forward prop)\n\n从输入初步传递到输出，即为前向传播\n\n**一般实现**\n\n```python\ndef dense(a_in, W, b, g):\n\tunits = W.shape[1] # 单元数等于W矩阵的列数，w_j向量是列向量\n\ta_out = np.zeros(units)\n\tfor j in range(units):\n\t\tw = W[:, j]\n\t\tz = np.dot(w, a_in) + b\n\t\ta_out[j] = g(z)\n\treturn a_out\ndef sequential(x):\n    a1 = dense(x, W1, b1)\n    a2 = dense(a1, W2, b2)\n    a3 = dense(a2, W3, b3)\n    f_x = a3\n    return f_x\n```\n\n**使用框架（TensorFlow/Pytorch)）进行矢量化加速**\n\n## 模型训练步骤\n\n1. 指定如何在给定输入X和参数的情况下计算输出(模型**结构**)\n2. 指定**损失函数**\n3. **训练**模型以最小化损失函数\n\n二元交叉熵损失\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n通过反向传播计算偏导数\n\n## 激活函数\n\n用`ReLU`函数代替`sigmoid`激活，$g(z) = max(0,z)$\n\n![img](/img/machine-learning-notes/pic-9.png)\n\n**如何选择合适的激活函数？**\n\n取决于要预测的y，对于神经网络的**输出层**：\n\n- 二分类——> sigmoid\n- y可正可负的回归——> linear\n- 回归中y大于等于0 ——> ReLU\n\n对于神经网络的**隐藏层**建议使用ReLU\n\n`ReLU`常用且更快\n\n- 不涉及指数运算\n- 当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x->-∞）,而sigmoid两端都是\n\n**为什么需要激活函数？**\n\n对于隐藏层，只使用线性的所有层等价于线性回归\n\n对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解\n\n## 多分类问题\n\n**softmax回归算法**（logistic 推广）\n\n$z_1=\\vec{w_1}·\\vec{x_1}+b_1$\n\n$a_1=\\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}} = P(y=1|\\vec{x})$\n\n即，设有N个分类\n\n$z_i=\\vec{w_1}·\\vec{x_i}+b_i$\n\n$$\na_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_i}}=P(y=i|\\vec{x})\n$$\n其中$a_1+a_2+...+a_N=1$\n\n**softmax损失函数**\n\n回顾logistic回归\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n二分类问题，设$a_1 = f_{w,b}(x^{(i)})$，即$y=1$的概率\n\n则$a_2 = 1-f_{w,b}(x^{(i)})$，即$y=0$的概率\n\n简化为\n\n$loss = -log(a_1)$ 如果$y=1$\n\n$loss = -log(a_2)$ 如果$y=0$\n\n对于softmax回归算法\n\n$$\nloss(a_1,a_2,...,a_N,y) = \\left\\{\\begin{matrix} -log(a_1) \\quad if \\quad y=1\\\\ -log(a_2) \\quad if \\quad y=2 \\\\ ... \\\\ -log(a_N) \\quad if \\quad y=N \\end{matrix}\\right.\n$$\n\n\n**神经网络中的softmax**\n\n输出层变为N个神经元\n\n注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 ... z_n$的函数\n\n**softmax改进**\n\n由于[数值溢出和精度问题](https://blog.csdn.net/muyuu/article/details/122757470)\n\n$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失\n\n不先计算出$a_i$，再带入损失函数\n\n而是**直接**\n$$\nloss_i=-log(\\frac{e^{z_i}}{e_{z_1}+...+e_{z_N}})\n$$\n\n\n此时输出层只需要`linear`即可（就是不计算$a_i$），同时开启`from_logits=True`\n\n```python\nmodel.compile(loss=SparseCategoricalCrossEntropy(from_logits=True)) #稀疏分类交叉熵损失\n```\n\n`from_logits=True`的[作用](https://blog.csdn.net/muyuu/article/details/122762442)\n\n需要概率时再调用`softmax`\n\n```python\nlogits = model(X)\nf_x = tf.nn.softmax(logits)\n```\n\n**多标签分类**\n\n![img](/img/machine-learning-notes/pic-10.png)\n\n将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。\n\n## 高级优化方法\n\n传统的梯度下降学习率固定\n\n### Adam（Adaptive Moment estimation）\n\n如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率\n\n如果看到学习率太大，某个参数值来回振荡，会自动减小学习率\n\n可以自动调整学习率$\\alpha$\n\n对于每个参数都有一个$\\alpha$\n\n选择optimizer=adam即可\n\n## 其他的网络层\n\n### 卷积层（Convolutional Layer）\n\n每个神经元只能看到前一个层输入的一部分\n\n- 加快计算速度\n- 需要更少的数据，不容易过拟合\n\n有多个卷积层，即卷积神经网络\n\n每一层的单元只查看输入的一部分 \n\n## 构建机器学习系统\n\n### 评估一个模型\n\n特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。\n\n将数据集分为训练集和测试集（73或者82开）\n\n分三步计算\n\n![img](/img/machine-learning-notes/pic-11.png)\n\n**注意计算error时不包括正则化项**\n\n过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力\n\n对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）\n\n### 如何选择模型\n\n数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$\n\n交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫**dev set**/validation set\n\n$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力\n\n数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。\n\n### **偏差和方差**\n\n![img](/img/machine-learning-notes/pic-12.png)\n\n![img](/img/machine-learning-notes/pic-13.png)\n\n高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多\n\n高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。\n\n**正则化项参数对偏差和方差的影响：**\n\n![img](/img/machine-learning-notes/pic-14.png)\n\n但是这些数值多少才算大/小呢？需要**建立基准性能标准**，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差/高偏差的问题\n\n![img](/img/machine-learning-notes/pic-15.png)\n\n**学习曲线**\n\n![img](/img/machine-learning-notes/pic-16.png)\n\n高偏差时 \n\n![img](/img/machine-learning-notes/pic-17.png)\n\n高方差时\n\n![img](/img/machine-learning-notes/pic-18.png)\n\n![img](/img/machine-learning-notes/pic-19.png)\n\n判断高方差或者高偏差决定下一步怎么做\n\n**神经网络中的偏差和方差**\n\n大型的神经网络有很小的偏差，所以只需要关注方差\n\n并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好\n\n但是大型网络计算比较昂贵\n\n```python\nlayer = Dense(unit=25, activation=\"relu\", kernel_regularizer=L2(0.01))\n```\n\n## 开发机器学习系统的迭代\n\n![img](/img/machine-learning-notes/pic-20.png)\n\n## 误差分析\n\n在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法\n\n## 添加数据\n\n由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用**数据增强**，用原有的数据样本创造新的样本\n\n例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等\n\n此外还有**数据合成**，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉\n\nAI = Code(algorithm/model) + Data\n\n## 迁移学习（Transfer Learning）\n\n对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。\n\n![img](/img/machine-learning-notes/pic-21.png)\n\n第一步叫做**监督预训练**(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做**微调**(fine tuning)，更改输出层的权重\n\n这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习\n\n通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样\n\n![img](/img/machine-learning-notes/pic-22.png)\n\n## 机器学习项目的完整周期\n\n![img](/img/machine-learning-notes/pic-23.png)\n\n部署\n\n![img](/img/machine-learning-notes/pic-24.png)\n\nMLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。\n\n## 关注公平、偏见、伦理\n\n## 倾斜数据集的误差指标\n\n某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造**混淆矩阵**，包括**真正例，假正例，真负例，假负例**\n\n常用的计算指标是**精确度(precision)**和**召回率(recall)**\n\n![img](/img/machine-learning-notes/pic-25.png)\n\n精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度\n\n权衡：\n\n当我们只有十分确信时才设置y=1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低\n\n当我们不希望错过实际上的y=1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高\n\n通过设置threshold权衡precision和recall\n\nF1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）\n\n$F1 score = \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} = 2\\frac{PR}{P+R}$\n\n## 决策树\n\n决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。\n\n![img](/img/machine-learning-notes/pic-26.png)\n\n**决策树学习**：\n\n- 如果选择每个节点选择什么特征来分类？\n\n应该最大化纯度，每一边的种类尽可能少\n\n- 什么时候停止分类？\n\n当一个节点100%是一个种类\n\n当分裂节点时会导致树超过最大高度（超参数）\n\n当提高的纯度分数低于一个门槛值\n\n当一个节点的样本数量低于一个门槛值\n\n### 衡量纯度（purity）\n\n熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 = 1 - p_1$\n\n$H(p_1)=-p_1log_2(p_1)-p_0log_2(p_0) = -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$\n\n注意：$0log(0) = 0$\n\n![img](/img/machine-learning-notes/pic-27.png)\n\n### 减小熵：信息增益（Information Gain）\n\n当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征\n\n实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值\n\n为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合\n\n![img](/img/machine-learning-notes/pic-28.png)\n\n更一般地\n\n![img](/img/machine-learning-notes/pic-29.png)\n\np是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本/上一节点样本）\n\n### 总结\n\n在根节点以所有数据样本开始\n\n计算所有特征的信息增益，选择最大的\n\n对选择的特征分裂，创建左右分支\n\n保持分裂直到遇到终止条件：\n\n- 当一个节点100%是一个类\n- 当分裂节点会导致树超过最大高度\n- 信息增益的值小于某个门槛值\n- 节点的样本数量小于某个门槛值\n\n实际上是一个递归的过程\n\n### 独热编码(One Hot Encoding)\n\n实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0/1）\n\n这样又转变为原来的左右分支分裂的情况\n\n### 连续值特征\n\n选定一个阈值，判断数据样本大于或者小于该阈值\n\n分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点\n\n对分割点分别计算信息增强来选择阈值\n\n### 回归树\n\n分裂时，改成尽量选取输出的方差(Variance)小的特征\n\nw还是从上一节点样本中选择的样本数（当前样本/上一节点样本），之后计算加权平均方差\n\n再用上一个节点所有数据的方差减去加权平均方差，选取最大的\n\n分类的结果是样本的平均值\n\n## 使用多个决策树\n\n单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮\n\n### 放回抽样\n\n从n个样本中放回地抽取n次，结果作为一个新的数据集\n\n### 随机森林（Random Forest）\n\n给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树\n\nb个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出\n\n对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k < n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k = \\sqrt{n}$\n\n### XGBoost（eXtreme Gradient Boosting）\n\n极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本\n\n思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分\n\n- 提升树的开源实现\n- 快速，有效\n- 很好的设定结束分裂的标准\n- 内置正则化\n\n### 什么时候使用决策树\n\n一个或多个决策树\n\n- 在表格化和结构化的数据上工作的很好\n- 不建议在非结构化的数据上，例如图片，音频，文本\n- 训练快速\n- 决策树是人类可以理解的（可解释性）\n\n神经网络\n\n- 对于所有类型的数据都能工作的很好\n- 比决策树更慢\n- 可以很好地使用迁移学习（预训练+微调）\n\n- 当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）\n\n# Course 3\n\n除了监督学习，机器学习还包括\n\n- 无监督学习\n  - 聚类\n  - 异常检测\n- 推荐系统\n- 强化学习\n\n## 聚类\n\n一堆数据点中自动查找相互关联或者相似的数据点\n\n### K-means\n\n首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2... \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度\n\n- 将每个点分配给离他最近的中心点（centroid质心）\n- 将中心点移动到分配的点的平均中心\n- 重复前两步，直到中心点不再移动，K-means算法收敛\n\n```bash\nRepeat{\n\tfor i = 1 to m\n\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）\n\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方\n\tfor i = 1 to k\n\t\tu_k更新为分配的点的中心（每个轴的点的平均值）\n\t\t如果簇中心点没有分配到点，就删除\n}\n```\n\n### 损失函数\n\n![img](/img/machine-learning-notes/pic-30.png)\n\n$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）\n\n$u_k$是簇k\n\n$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇\n\n损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是**欧几里得距离**\n\n也叫Distortion Function\n\n### 初始化\n\n选择$K<m$\n\n随机选择K个训练样本，将$\\mu_1 ,\\mu_2... \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点\n\n```bash\nfor i = 1 to 100{\n\t随机初始化\n\t获取c_i, u_i\n\t计算损失函数J\n}\n选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值\n```\n\n### 选择簇的个数\n\n**肘法（Elbow Method）**\n\n选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点\n\n所以K的选择还是按照之后的任务目的选择\n\n## 异常检测\n\n### 密度估计（Density estimation）\n\n根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})<\\epsilon$，认为出现了异常（anomaly）\n\n### 高斯分布\n\nGaussian Distribution，也叫正态分布(Normal Distribution)\n\n$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$\n\n其中$\\mu$是平均值，$\\sigma$是标准差\n\n![img](/img/machine-learning-notes/pic-31.png)\n\n### 算法实现\n\n对于有多个特征的输入$\\vec{x}$，$\\vec{x} = [x_1, x_2 ... x_n]$\n\n$$\np(\\vec{x}) = p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) *...* p(x_n;\\mu_n,\\sigma_n^2) = \\prod_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)\n$$\n\n\n### 开发和评估异常检测系统\n\n通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0/1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集\n\n**流程：**\n\n在训练集$x_1...x_m$上拟合模型$p(x)$\n\n在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）\n\n之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$\n\n### 权衡异常检测和监督学习\n\n异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样\n\n监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似\n\n### 特征选择\n\n监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要\n\n- 绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）\n- 检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取\n\n## 推荐系统\n\n$r(i,j) = 1$表示用户j为电影i打分\n\n$y^{(i,j)}$表示用户j为电影i打的分\n\n$w^{(j)}, b^{(j)}$是用户j的参数\n\n$x^{(i)}$是电影i的特征向量\n\n对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$\n\n$m^{(j)}$表示用户j打分的电影数量\n\n通过训练学习$w^{(j)}, b^{(j)}$\n\n$$\n\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)=\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$\n\n$$\n\\left.\\mathrm{J}\\left(\n\\begin{array}\n{cc}{w^{(1)},} & {...,w^{(n_{u})}} \\\\\n{b^{(1)},} & {...,b^{(n_{u})}}\n\\end{array}\\right.\\right)=\\frac{1}{2}\\sum_{j=1}^{n_{u}}\\sum_{i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j=1}^{n_{u}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n### 协同过滤算法\n\n在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少\n\n$$\n\\mathrm{J}(x^{(i)})=\\frac{1}{2}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n学习得到$x^{(1)},x^{(2)},...,x^{(n_m)}$\n\n$$\n\\mathrm{J}\\left(x^{(1)},x^{(2)},...,x^{(n_{m})}\\right)=\\frac{1}{2}\\sum_{i=1}^{n_{m}}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：\n\n![img](/img/machine-learning-notes/pic-32.png)\n\n梯度下降时，w，b，x都是参数\n\n![img](/img/machine-learning-notes/pic-33.png)\n\n[补充](https://blog.csdn.net/zhu_xian_gang/article/details/130243870)\n\n### 二进制标签\n\n1-用户看到物品之后参与点击，停留，添加喜欢，购买\n\n0-用户看到物品之后忽略\n\n?-用户没有看到物品\n\n预测$y^{(i,j)}=1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数\n\n![img](/img/machine-learning-notes/pic-34.png)\n\n### 均值归一化\n\n**Mean Normalization**\n\n- 求均值$\\mu$\n- $x_1 = \\frac{x_1-\\mu}{max-min}$\n\n求出每个电影的平均用户平方$\\mu_i$，构建向量$u$\n\n对于用户j，预测其在电影i的评分：\n\n$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$\n\n以至于不会当用户没有评分时认为评分接近0，而是接近平均值\n\n### 查找相关项目\n\n对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似\n\n选取小的$\\sum_{l=1}^n(x_l^{(k)} - x_l^{(i)})^2$\n\n也可以写作$||x^{(k)} - x^{(i)}||^2$\n\n### 协同过滤算法的限制\n\n**冷启动问题**\n\n- 如何对没有什么用户打分的项目评分？\n- 如何对没有对很多项目打分的用户推荐一些项目？\n\n**没有很多信息的时候利用辅助信息**\n\n### 基于内容的过滤算法\n\n协同过滤：基于用户的评分与你的评分的相似推荐项目\n\n基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目\n\n但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配\n\n对于v的获取，使用神经网络\n\n可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积\n\n也可以将两个网络合并，在内部进行点积输出结果\n\n$$\nJ=\\sum_{(i,j):r(i,j)=1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}\n$$\n\n\n为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似\n\n### Retrieval and Ranking\n\n通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名\n\n检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看\n\n排名：对这些检索出的有限个项目进行学习，根据结果进行排名\n\n权衡检索的项目数量\n\n## 强化学习\n\n强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。\n\n涉及状态，行动，奖励，折扣系数，回报，策略\n\n### 回报\n\n指的是系统获得的奖励总和\n\n折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+...$，直到终止状态\n\n### 策略\n\n状态state通过策略π实行行动a\n\n$\\pi(s) = a$，指明状态s情况下需要进行的决策a，从而最大化回报\n\n### 马尔科夫决策过程\n\nMarkov Decision Process(MDP)\n\n![img](/img/machine-learning-notes/pic-35.png)\n\n### 状态-动作价值函数\n\nState-action value function，也叫Q-function,Q*,Optimal Q function\n\n$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报\n\n在状态s的最好回报就是$max_aQ(s,a)$\n\n在状态s的最好动作的就能够提供$max_aQ(s,a)$的\n\n### Bellman方程\n\n$s$:当前状态\n\n$a$:当前状态的决策\n\n$R(s)$:当前状态的奖励\n\n$s'$:采取动作a后的状态\n\n$a'$:在状态s'采取的动作\n\n$Q(s,a) = R(s)+\\gamma max_{a'}Q(s',a')$\n\nR(s)也叫即时奖励，表示你可以立刻得到的奖励\n\n后一项是从状态s'表现得最好获得的回报\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+... = R_1 + \\gamma[R_2 + \\gamma R_3+...]$\n\n### 随机环境\n\n由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励\n\n所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望\n\n$\\text{Return} = \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+...) = \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+...)$\n\nBellman Equation变成：\n\n$Q(s,a) = R(s)+\\gamma \\text{E} [max_{a'}Q(s',a')]$\n\n### 连续状态空间\n\n状态参数可能是连续的，比如坐标，角度，速度\n\n同时状态可能有多个，比如xyz坐标，速度等\n\n此时也叫连续状态马尔科夫决策过程\n\n### 学习状态值函数\n\n![img](/img/machine-learning-notes/pic-36.png)\n\n以随机猜测$Q(s,a)$初始化神经网络\n\n重复：\n\n采取措施，得到$(s,a,R(s),s')$元组\n\n存储最近的10k个 $(s,a,R(s),s')$元组（Replay Buffer）\n\n训练网络：\n\n​\t创建10k个训练集，其中$x=(s,a)$，$y = R(s)+\\gamma max_{a'}Q(s',a')$\n\n​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$\n\n令$Q=Q_{new}$\n\n虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计\n\n**改进**\n\n- 神经网络架构\n\n可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行\n\n![img](/img/machine-learning-notes/pic-37.png)\n\n- $\\epsilon$贪心策略\n\n当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。\n\n应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）\n\n小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。\n\n- 小批量$mini-batch$\n\n将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优\n\n![img](/img/machine-learning-notes/pic-38.png)\n\n假设子集大小为1000；\n\n具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。\n\n在强化学习中，可以把10k的数据集分解训练多个模型\n\n- 软更新\n\n令$Q=Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$\n\n而是\n$$\nw = 0.01w_{new} + 0.99w\n$$\n\n$$\nb = 0.01b_{new} + 0.99b\n$$\n\n对参数进行微小调整\n","source":"_posts/machine-learning-notes.md","raw":"---\ntitle: Machine Learning Notes\nmathjax: true\ndate: 2023/11/25 20:46:25\nimg: https://img2.baidu.com/it/u=1093757134,3274186314&fm=253&fmt=auto&app=120&f=JPEG?w=800&h=500\nexcerpt: 吴恩达机器学习视频笔记\n---\n\n# Course 1\n\n监督学习：输入特征x，输出目标y。对数据集进行预测，分为**回归**和**分类**\n\n无监督学习：输入特征x，没有目标y，对数据集进行**聚类预测**，**异常检测**，**降维**\n\n## 线性回归\n\n$$\ny^i = wx^i+b\n$$\n\n定义损失函数（成本函数），需要最小化损失函数\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2\n$$\n\n其中$y^i$为真实输出，$\\hat{y}$为预测输出\n\n- 为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差\n- 1/2是为了方便求导计算\n\nloss针对一个训练样本，cost是所有训练样本的均值\n\n### 梯度下降\n\n需要最小会损失函数，需要使用梯度下降算法\n\n定义学习率`learning_rate`为$\\alpha$,一般$\\alpha \\subseteq [0,1]$\n\n$w = w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$\n\n$b = b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$\n\n- 梯度下降时建议**同步**梯度下降，如下图\n\n![img](/img/machine-learning-notes/pic-1.png)\n\n如果$\\alpha$太小，可以得到答案，但是时间过长\n\n如果$\\alpha$太大，大交叉无法收敛，甚至发散\n\n当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解\n\n对导数项分别求导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = wx^i+b$\n\n对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值\n\n选择合适得到学习率，就可以得到$min(J(w,b))$\n\n线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例\n\n### 多元线性回归\n\n假设特征有$n$个，定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n则$f_{\\vec{w},b}=\\vec{w} \\cdot \\vec{x} +b$\n\n`·`为两个向量的点积(dot)。\n\n$$\n\\vec{w} \\cdot \\vec{x} = w_1*x_1+w_2*x_2+....+w_n*x_n\n$$\n\n\n**矢量化**：利用计算机的并行硬件，代码简洁、运行速度快\n\n```python\nf = np.dot(w, x) + b\n```\n\n**多元线性回归的梯度下降**\n\n![img](/img/machine-learning-notes/pic-2.png)\n\nPS: 正规方程：某些机器学习库在后端求$w,b$的方法，**只适用于线性回归**，而且速度慢，不要求掌握\n\n### 特征缩放\n\n不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动\n\n加快梯度下降速度\n\n避免特征的取值范围差异过大，将其进行缩放，几种常见方法：\n\n- **除以最大值**，$x_{1,scale} = \\frac{x_1}{max}$， $x \\in [0,1]$\n- **均值归一化Mean Normalization**\n  - 求均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{max-min}$\n- **`Z-score`归一化**\n  - 求标准差$\\sigma$，均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{\\sigma}$\n\n**判断梯度下降是否收敛：**\n\n1. 观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）\n\n**选择合适学习率**：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$\n\n### 特征工程\n\n利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测\n\n**多项式回归**：可以添加$x^q$项更好地拟合数据图像，$f(x)=w_1x^3+w_2x^2+w_1x^1+b$\n\n此时特征缩放尤为重要\n\n## 分类-逻辑回归\n\n解决二分类问题\n\n### sigmoid函数\n\n输出介于$(0,1)$\n\n$g(z)= \\frac{1}{1+e^{-z}},z \\subseteq R$\n\n**logistic regression**:\n\n$f_{\\vec{w},b}(\\vec{x})=g(\\vec{w} · \\vec{x}+b) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n输出值可以理解为分类为1的可能性\n\n$f_{\\vec{w},b}(\\vec{x})=P(y=1|\\vec{x};\\vec{w},b)$\n\n### 决策边界decision boundary\n\n以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b <0$，取值0\n\n$\\vec{w} · \\vec{x}+b = 0$称为决策边界\n\n多项式回归也适用于非线性的决策边界\n\n### 成本函数\n\n如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$**不是凸函数，不适用于逻辑回归**\n\n定义\n\n$$\nJ(w,b)=\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})\n$$\n其中L代表单个样例的loss，J代表总的cost\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}=1\n$$\n![img](/img/machine-learning-notes/pic-3.png)\n\n当y等于1，预测值越靠近1损失越小\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}=0\n$$\n![img](/img/machine-learning-notes/pic-4.png)\n\n当y等于0，预测值越靠近0损失越小 \n\n**简化**成本函数                                                                                                                                                                                                                                                                                          \n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n得到\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))\n$$\n\n\n成本函数是凸函数，便于实现梯度下降\n\n### 梯度下降\n\n对J求偏导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n可以使用相似方法进行特征缩放\n\n### 过拟合问题\n\n过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。\n\n![img](/img/machine-learning-notes/pic-5.png)\n\n**解决过拟合**\n\n- 收集更多训练数据\n- 特征筛选，选择特征的一个子集\n- 正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响\n\n### 正则化\n\n如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n这样使得$w_j$尽可能小，几乎为0\n\n参数$b$是否正则化无关紧要\n\n**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显\n\n- 正则化线性回归\n\n对$J(w,b)$求偏导不断同步更新w,b的值\n\n$$\n\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}\n$$\n\n$$\nw = w- \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}) = (1-\\alpha \\frac{\\lambda}{m})w+.....\n$$\n\n\n- 正则化逻辑回归\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n\n求导式和线性回归相同，只是需要注意**正则化项偏导数没有求和**\n\n$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n![img](/img/machine-learning-notes/pic-6.png)\n\n# Course 2\n\n## 神经网络\n\n起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为**深度学习**\n\n利用别人训练的神经网络参数称为推理或者预测\n\n为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数\n\n神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层->隐藏层->输出层）\n\n多层神经网络叫做多层感知机\n\n## 神经网络中的层\n\n讨论层数通常是隐藏层和输出层，不包括输入层\n\n每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层/输出结果\n\n即$a_j^{[l]} = g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$\n\n$j$表示神经元单元序号，$l$表示层数，$g(x)$为`sigmod`函数\n\n![img](/img/machine-learning-notes/pic-7.jpg)\n\n$a_j^{[l]}$构成$\\vec{a}^{[l]}$\n\n![img](/img/machine-learning-notes/pic-8.png)\n\n## 前向传播(forward prop)\n\n从输入初步传递到输出，即为前向传播\n\n**一般实现**\n\n```python\ndef dense(a_in, W, b, g):\n\tunits = W.shape[1] # 单元数等于W矩阵的列数，w_j向量是列向量\n\ta_out = np.zeros(units)\n\tfor j in range(units):\n\t\tw = W[:, j]\n\t\tz = np.dot(w, a_in) + b\n\t\ta_out[j] = g(z)\n\treturn a_out\ndef sequential(x):\n    a1 = dense(x, W1, b1)\n    a2 = dense(a1, W2, b2)\n    a3 = dense(a2, W3, b3)\n    f_x = a3\n    return f_x\n```\n\n**使用框架（TensorFlow/Pytorch)）进行矢量化加速**\n\n## 模型训练步骤\n\n1. 指定如何在给定输入X和参数的情况下计算输出(模型**结构**)\n2. 指定**损失函数**\n3. **训练**模型以最小化损失函数\n\n二元交叉熵损失\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n通过反向传播计算偏导数\n\n## 激活函数\n\n用`ReLU`函数代替`sigmoid`激活，$g(z) = max(0,z)$\n\n![img](/img/machine-learning-notes/pic-9.png)\n\n**如何选择合适的激活函数？**\n\n取决于要预测的y，对于神经网络的**输出层**：\n\n- 二分类——> sigmoid\n- y可正可负的回归——> linear\n- 回归中y大于等于0 ——> ReLU\n\n对于神经网络的**隐藏层**建议使用ReLU\n\n`ReLU`常用且更快\n\n- 不涉及指数运算\n- 当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x->-∞）,而sigmoid两端都是\n\n**为什么需要激活函数？**\n\n对于隐藏层，只使用线性的所有层等价于线性回归\n\n对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解\n\n## 多分类问题\n\n**softmax回归算法**（logistic 推广）\n\n$z_1=\\vec{w_1}·\\vec{x_1}+b_1$\n\n$a_1=\\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}} = P(y=1|\\vec{x})$\n\n即，设有N个分类\n\n$z_i=\\vec{w_1}·\\vec{x_i}+b_i$\n\n$$\na_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_i}}=P(y=i|\\vec{x})\n$$\n其中$a_1+a_2+...+a_N=1$\n\n**softmax损失函数**\n\n回顾logistic回归\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n二分类问题，设$a_1 = f_{w,b}(x^{(i)})$，即$y=1$的概率\n\n则$a_2 = 1-f_{w,b}(x^{(i)})$，即$y=0$的概率\n\n简化为\n\n$loss = -log(a_1)$ 如果$y=1$\n\n$loss = -log(a_2)$ 如果$y=0$\n\n对于softmax回归算法\n\n$$\nloss(a_1,a_2,...,a_N,y) = \\left\\{\\begin{matrix} -log(a_1) \\quad if \\quad y=1\\\\ -log(a_2) \\quad if \\quad y=2 \\\\ ... \\\\ -log(a_N) \\quad if \\quad y=N \\end{matrix}\\right.\n$$\n\n\n**神经网络中的softmax**\n\n输出层变为N个神经元\n\n注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 ... z_n$的函数\n\n**softmax改进**\n\n由于[数值溢出和精度问题](https://blog.csdn.net/muyuu/article/details/122757470)\n\n$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失\n\n不先计算出$a_i$，再带入损失函数\n\n而是**直接**\n$$\nloss_i=-log(\\frac{e^{z_i}}{e_{z_1}+...+e_{z_N}})\n$$\n\n\n此时输出层只需要`linear`即可（就是不计算$a_i$），同时开启`from_logits=True`\n\n```python\nmodel.compile(loss=SparseCategoricalCrossEntropy(from_logits=True)) #稀疏分类交叉熵损失\n```\n\n`from_logits=True`的[作用](https://blog.csdn.net/muyuu/article/details/122762442)\n\n需要概率时再调用`softmax`\n\n```python\nlogits = model(X)\nf_x = tf.nn.softmax(logits)\n```\n\n**多标签分类**\n\n![img](/img/machine-learning-notes/pic-10.png)\n\n将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。\n\n## 高级优化方法\n\n传统的梯度下降学习率固定\n\n### Adam（Adaptive Moment estimation）\n\n如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率\n\n如果看到学习率太大，某个参数值来回振荡，会自动减小学习率\n\n可以自动调整学习率$\\alpha$\n\n对于每个参数都有一个$\\alpha$\n\n选择optimizer=adam即可\n\n## 其他的网络层\n\n### 卷积层（Convolutional Layer）\n\n每个神经元只能看到前一个层输入的一部分\n\n- 加快计算速度\n- 需要更少的数据，不容易过拟合\n\n有多个卷积层，即卷积神经网络\n\n每一层的单元只查看输入的一部分 \n\n## 构建机器学习系统\n\n### 评估一个模型\n\n特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。\n\n将数据集分为训练集和测试集（73或者82开）\n\n分三步计算\n\n![img](/img/machine-learning-notes/pic-11.png)\n\n**注意计算error时不包括正则化项**\n\n过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力\n\n对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）\n\n### 如何选择模型\n\n数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$\n\n交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫**dev set**/validation set\n\n$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力\n\n数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。\n\n### **偏差和方差**\n\n![img](/img/machine-learning-notes/pic-12.png)\n\n![img](/img/machine-learning-notes/pic-13.png)\n\n高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多\n\n高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。\n\n**正则化项参数对偏差和方差的影响：**\n\n![img](/img/machine-learning-notes/pic-14.png)\n\n但是这些数值多少才算大/小呢？需要**建立基准性能标准**，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差/高偏差的问题\n\n![img](/img/machine-learning-notes/pic-15.png)\n\n**学习曲线**\n\n![img](/img/machine-learning-notes/pic-16.png)\n\n高偏差时 \n\n![img](/img/machine-learning-notes/pic-17.png)\n\n高方差时\n\n![img](/img/machine-learning-notes/pic-18.png)\n\n![img](/img/machine-learning-notes/pic-19.png)\n\n判断高方差或者高偏差决定下一步怎么做\n\n**神经网络中的偏差和方差**\n\n大型的神经网络有很小的偏差，所以只需要关注方差\n\n并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好\n\n但是大型网络计算比较昂贵\n\n```python\nlayer = Dense(unit=25, activation=\"relu\", kernel_regularizer=L2(0.01))\n```\n\n## 开发机器学习系统的迭代\n\n![img](/img/machine-learning-notes/pic-20.png)\n\n## 误差分析\n\n在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法\n\n## 添加数据\n\n由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用**数据增强**，用原有的数据样本创造新的样本\n\n例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等\n\n此外还有**数据合成**，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉\n\nAI = Code(algorithm/model) + Data\n\n## 迁移学习（Transfer Learning）\n\n对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。\n\n![img](/img/machine-learning-notes/pic-21.png)\n\n第一步叫做**监督预训练**(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做**微调**(fine tuning)，更改输出层的权重\n\n这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习\n\n通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样\n\n![img](/img/machine-learning-notes/pic-22.png)\n\n## 机器学习项目的完整周期\n\n![img](/img/machine-learning-notes/pic-23.png)\n\n部署\n\n![img](/img/machine-learning-notes/pic-24.png)\n\nMLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。\n\n## 关注公平、偏见、伦理\n\n## 倾斜数据集的误差指标\n\n某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造**混淆矩阵**，包括**真正例，假正例，真负例，假负例**\n\n常用的计算指标是**精确度(precision)**和**召回率(recall)**\n\n![img](/img/machine-learning-notes/pic-25.png)\n\n精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度\n\n权衡：\n\n当我们只有十分确信时才设置y=1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低\n\n当我们不希望错过实际上的y=1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高\n\n通过设置threshold权衡precision和recall\n\nF1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）\n\n$F1 score = \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} = 2\\frac{PR}{P+R}$\n\n## 决策树\n\n决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。\n\n![img](/img/machine-learning-notes/pic-26.png)\n\n**决策树学习**：\n\n- 如果选择每个节点选择什么特征来分类？\n\n应该最大化纯度，每一边的种类尽可能少\n\n- 什么时候停止分类？\n\n当一个节点100%是一个种类\n\n当分裂节点时会导致树超过最大高度（超参数）\n\n当提高的纯度分数低于一个门槛值\n\n当一个节点的样本数量低于一个门槛值\n\n### 衡量纯度（purity）\n\n熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 = 1 - p_1$\n\n$H(p_1)=-p_1log_2(p_1)-p_0log_2(p_0) = -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$\n\n注意：$0log(0) = 0$\n\n![img](/img/machine-learning-notes/pic-27.png)\n\n### 减小熵：信息增益（Information Gain）\n\n当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征\n\n实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值\n\n为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合\n\n![img](/img/machine-learning-notes/pic-28.png)\n\n更一般地\n\n![img](/img/machine-learning-notes/pic-29.png)\n\np是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本/上一节点样本）\n\n### 总结\n\n在根节点以所有数据样本开始\n\n计算所有特征的信息增益，选择最大的\n\n对选择的特征分裂，创建左右分支\n\n保持分裂直到遇到终止条件：\n\n- 当一个节点100%是一个类\n- 当分裂节点会导致树超过最大高度\n- 信息增益的值小于某个门槛值\n- 节点的样本数量小于某个门槛值\n\n实际上是一个递归的过程\n\n### 独热编码(One Hot Encoding)\n\n实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0/1）\n\n这样又转变为原来的左右分支分裂的情况\n\n### 连续值特征\n\n选定一个阈值，判断数据样本大于或者小于该阈值\n\n分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点\n\n对分割点分别计算信息增强来选择阈值\n\n### 回归树\n\n分裂时，改成尽量选取输出的方差(Variance)小的特征\n\nw还是从上一节点样本中选择的样本数（当前样本/上一节点样本），之后计算加权平均方差\n\n再用上一个节点所有数据的方差减去加权平均方差，选取最大的\n\n分类的结果是样本的平均值\n\n## 使用多个决策树\n\n单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮\n\n### 放回抽样\n\n从n个样本中放回地抽取n次，结果作为一个新的数据集\n\n### 随机森林（Random Forest）\n\n给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树\n\nb个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出\n\n对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k < n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k = \\sqrt{n}$\n\n### XGBoost（eXtreme Gradient Boosting）\n\n极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本\n\n思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分\n\n- 提升树的开源实现\n- 快速，有效\n- 很好的设定结束分裂的标准\n- 内置正则化\n\n### 什么时候使用决策树\n\n一个或多个决策树\n\n- 在表格化和结构化的数据上工作的很好\n- 不建议在非结构化的数据上，例如图片，音频，文本\n- 训练快速\n- 决策树是人类可以理解的（可解释性）\n\n神经网络\n\n- 对于所有类型的数据都能工作的很好\n- 比决策树更慢\n- 可以很好地使用迁移学习（预训练+微调）\n\n- 当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）\n\n# Course 3\n\n除了监督学习，机器学习还包括\n\n- 无监督学习\n  - 聚类\n  - 异常检测\n- 推荐系统\n- 强化学习\n\n## 聚类\n\n一堆数据点中自动查找相互关联或者相似的数据点\n\n### K-means\n\n首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2... \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度\n\n- 将每个点分配给离他最近的中心点（centroid质心）\n- 将中心点移动到分配的点的平均中心\n- 重复前两步，直到中心点不再移动，K-means算法收敛\n\n```bash\nRepeat{\n\tfor i = 1 to m\n\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）\n\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方\n\tfor i = 1 to k\n\t\tu_k更新为分配的点的中心（每个轴的点的平均值）\n\t\t如果簇中心点没有分配到点，就删除\n}\n```\n\n### 损失函数\n\n![img](/img/machine-learning-notes/pic-30.png)\n\n$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）\n\n$u_k$是簇k\n\n$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇\n\n损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是**欧几里得距离**\n\n也叫Distortion Function\n\n### 初始化\n\n选择$K<m$\n\n随机选择K个训练样本，将$\\mu_1 ,\\mu_2... \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点\n\n```bash\nfor i = 1 to 100{\n\t随机初始化\n\t获取c_i, u_i\n\t计算损失函数J\n}\n选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值\n```\n\n### 选择簇的个数\n\n**肘法（Elbow Method）**\n\n选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点\n\n所以K的选择还是按照之后的任务目的选择\n\n## 异常检测\n\n### 密度估计（Density estimation）\n\n根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})<\\epsilon$，认为出现了异常（anomaly）\n\n### 高斯分布\n\nGaussian Distribution，也叫正态分布(Normal Distribution)\n\n$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$\n\n其中$\\mu$是平均值，$\\sigma$是标准差\n\n![img](/img/machine-learning-notes/pic-31.png)\n\n### 算法实现\n\n对于有多个特征的输入$\\vec{x}$，$\\vec{x} = [x_1, x_2 ... x_n]$\n\n$$\np(\\vec{x}) = p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) *...* p(x_n;\\mu_n,\\sigma_n^2) = \\prod_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)\n$$\n\n\n### 开发和评估异常检测系统\n\n通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0/1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集\n\n**流程：**\n\n在训练集$x_1...x_m$上拟合模型$p(x)$\n\n在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）\n\n之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$\n\n### 权衡异常检测和监督学习\n\n异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样\n\n监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似\n\n### 特征选择\n\n监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要\n\n- 绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）\n- 检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取\n\n## 推荐系统\n\n$r(i,j) = 1$表示用户j为电影i打分\n\n$y^{(i,j)}$表示用户j为电影i打的分\n\n$w^{(j)}, b^{(j)}$是用户j的参数\n\n$x^{(i)}$是电影i的特征向量\n\n对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$\n\n$m^{(j)}$表示用户j打分的电影数量\n\n通过训练学习$w^{(j)}, b^{(j)}$\n\n$$\n\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)=\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$\n\n$$\n\\left.\\mathrm{J}\\left(\n\\begin{array}\n{cc}{w^{(1)},} & {...,w^{(n_{u})}} \\\\\n{b^{(1)},} & {...,b^{(n_{u})}}\n\\end{array}\\right.\\right)=\\frac{1}{2}\\sum_{j=1}^{n_{u}}\\sum_{i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j=1}^{n_{u}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n### 协同过滤算法\n\n在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少\n\n$$\n\\mathrm{J}(x^{(i)})=\\frac{1}{2}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n学习得到$x^{(1)},x^{(2)},...,x^{(n_m)}$\n\n$$\n\\mathrm{J}\\left(x^{(1)},x^{(2)},...,x^{(n_{m})}\\right)=\\frac{1}{2}\\sum_{i=1}^{n_{m}}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：\n\n![img](/img/machine-learning-notes/pic-32.png)\n\n梯度下降时，w，b，x都是参数\n\n![img](/img/machine-learning-notes/pic-33.png)\n\n[补充](https://blog.csdn.net/zhu_xian_gang/article/details/130243870)\n\n### 二进制标签\n\n1-用户看到物品之后参与点击，停留，添加喜欢，购买\n\n0-用户看到物品之后忽略\n\n?-用户没有看到物品\n\n预测$y^{(i,j)}=1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数\n\n![img](/img/machine-learning-notes/pic-34.png)\n\n### 均值归一化\n\n**Mean Normalization**\n\n- 求均值$\\mu$\n- $x_1 = \\frac{x_1-\\mu}{max-min}$\n\n求出每个电影的平均用户平方$\\mu_i$，构建向量$u$\n\n对于用户j，预测其在电影i的评分：\n\n$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$\n\n以至于不会当用户没有评分时认为评分接近0，而是接近平均值\n\n### 查找相关项目\n\n对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似\n\n选取小的$\\sum_{l=1}^n(x_l^{(k)} - x_l^{(i)})^2$\n\n也可以写作$||x^{(k)} - x^{(i)}||^2$\n\n### 协同过滤算法的限制\n\n**冷启动问题**\n\n- 如何对没有什么用户打分的项目评分？\n- 如何对没有对很多项目打分的用户推荐一些项目？\n\n**没有很多信息的时候利用辅助信息**\n\n### 基于内容的过滤算法\n\n协同过滤：基于用户的评分与你的评分的相似推荐项目\n\n基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目\n\n但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配\n\n对于v的获取，使用神经网络\n\n可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积\n\n也可以将两个网络合并，在内部进行点积输出结果\n\n$$\nJ=\\sum_{(i,j):r(i,j)=1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}\n$$\n\n\n为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似\n\n### Retrieval and Ranking\n\n通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名\n\n检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看\n\n排名：对这些检索出的有限个项目进行学习，根据结果进行排名\n\n权衡检索的项目数量\n\n## 强化学习\n\n强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。\n\n涉及状态，行动，奖励，折扣系数，回报，策略\n\n### 回报\n\n指的是系统获得的奖励总和\n\n折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+...$，直到终止状态\n\n### 策略\n\n状态state通过策略π实行行动a\n\n$\\pi(s) = a$，指明状态s情况下需要进行的决策a，从而最大化回报\n\n### 马尔科夫决策过程\n\nMarkov Decision Process(MDP)\n\n![img](/img/machine-learning-notes/pic-35.png)\n\n### 状态-动作价值函数\n\nState-action value function，也叫Q-function,Q*,Optimal Q function\n\n$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报\n\n在状态s的最好回报就是$max_aQ(s,a)$\n\n在状态s的最好动作的就能够提供$max_aQ(s,a)$的\n\n### Bellman方程\n\n$s$:当前状态\n\n$a$:当前状态的决策\n\n$R(s)$:当前状态的奖励\n\n$s'$:采取动作a后的状态\n\n$a'$:在状态s'采取的动作\n\n$Q(s,a) = R(s)+\\gamma max_{a'}Q(s',a')$\n\nR(s)也叫即时奖励，表示你可以立刻得到的奖励\n\n后一项是从状态s'表现得最好获得的回报\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+... = R_1 + \\gamma[R_2 + \\gamma R_3+...]$\n\n### 随机环境\n\n由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励\n\n所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望\n\n$\\text{Return} = \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+...) = \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+...)$\n\nBellman Equation变成：\n\n$Q(s,a) = R(s)+\\gamma \\text{E} [max_{a'}Q(s',a')]$\n\n### 连续状态空间\n\n状态参数可能是连续的，比如坐标，角度，速度\n\n同时状态可能有多个，比如xyz坐标，速度等\n\n此时也叫连续状态马尔科夫决策过程\n\n### 学习状态值函数\n\n![img](/img/machine-learning-notes/pic-36.png)\n\n以随机猜测$Q(s,a)$初始化神经网络\n\n重复：\n\n采取措施，得到$(s,a,R(s),s')$元组\n\n存储最近的10k个 $(s,a,R(s),s')$元组（Replay Buffer）\n\n训练网络：\n\n​\t创建10k个训练集，其中$x=(s,a)$，$y = R(s)+\\gamma max_{a'}Q(s',a')$\n\n​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$\n\n令$Q=Q_{new}$\n\n虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计\n\n**改进**\n\n- 神经网络架构\n\n可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行\n\n![img](/img/machine-learning-notes/pic-37.png)\n\n- $\\epsilon$贪心策略\n\n当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。\n\n应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）\n\n小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。\n\n- 小批量$mini-batch$\n\n将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优\n\n![img](/img/machine-learning-notes/pic-38.png)\n\n假设子集大小为1000；\n\n具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。\n\n在强化学习中，可以把10k的数据集分解训练多个模型\n\n- 软更新\n\n令$Q=Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$\n\n而是\n$$\nw = 0.01w_{new} + 0.99w\n$$\n\n$$\nb = 0.01b_{new} + 0.99b\n$$\n\n对参数进行微小调整\n","slug":"machine-learning-notes","published":1,"updated":"2025-02-28T02:56:54.489Z","comments":1,"layout":"post","photos":[],"_id":"cma198q9h000ebs994iocbwvv","content":"<h1 id=\"Course-1\"><a href=\"#Course-1\" class=\"headerlink\" title=\"Course 1\"></a>Course 1</h1><p>监督学习：输入特征x，输出目标y。对数据集进行预测，分为<strong>回归</strong>和<strong>分类</strong></p>\n<p>无监督学习：输入特征x，没有目标y，对数据集进行<strong>聚类预测</strong>，<strong>异常检测</strong>，<strong>降维</strong></p>\n<h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><p>$$<br>y^i &#x3D; wx^i+b<br>$$</p>\n<p>定义损失函数（成本函数），需要最小化损失函数</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2<br>$$</p>\n<p>其中$y^i$为真实输出，$\\hat{y}$为预测输出</p>\n<ul>\n<li>为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差</li>\n<li>1&#x2F;2是为了方便求导计算</li>\n</ul>\n<p>loss针对一个训练样本，cost是所有训练样本的均值</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>需要最小会损失函数，需要使用梯度下降算法</p>\n<p>定义学习率<code>learning_rate</code>为$\\alpha$,一般$\\alpha \\subseteq [0,1]$</p>\n<p>$w &#x3D; w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$</p>\n<p>$b &#x3D; b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$</p>\n<ul>\n<li>梯度下降时建议<strong>同步</strong>梯度下降，如下图</li>\n</ul>\n<p><img src=\"/img/machine-learning-notes/pic-1.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>如果$\\alpha$太小，可以得到答案，但是时间过长</p>\n<p>如果$\\alpha$太大，大交叉无法收敛，甚至发散</p>\n<p>当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解</p>\n<p>对导数项分别求导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; wx^i+b$</p>\n<p>对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值</p>\n<p>选择合适得到学习率，就可以得到$min(J(w,b))$</p>\n<p>线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例</p>\n<h3 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h3><p>假设特征有$n$个，定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>则$f_{\\vec{w},b}&#x3D;\\vec{w} \\cdot \\vec{x} +b$</p>\n<p><code>·</code>为两个向量的点积(dot)。</p>\n<p>$$<br>\\vec{w} \\cdot \\vec{x} &#x3D; w_1<em>x_1+w_2</em>x_2+….+w_n*x_n<br>$$</p>\n<p><strong>矢量化</strong>：利用计算机的并行硬件，代码简洁、运行速度快</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>\n\n<p><strong>多元线性回归的梯度下降</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-2.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>PS: 正规方程：某些机器学习库在后端求$w,b$的方法，<strong>只适用于线性回归</strong>，而且速度慢，不要求掌握</p>\n<h3 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h3><p>不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动</p>\n<p>加快梯度下降速度</p>\n<p>避免特征的取值范围差异过大，将其进行缩放，几种常见方法：</p>\n<ul>\n<li><strong>除以最大值</strong>，$x_{1,scale} &#x3D; \\frac{x_1}{max}$， $x \\in [0,1]$</li>\n<li><strong>均值归一化Mean Normalization</strong><ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n</li>\n<li><strong><code>Z-score</code>归一化</strong><ul>\n<li>求标准差$\\sigma$，均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{\\sigma}$</li>\n</ul>\n</li>\n</ul>\n<p><strong>判断梯度下降是否收敛：</strong></p>\n<ol>\n<li>观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）</li>\n</ol>\n<p><strong>选择合适学习率</strong>：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$</p>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测</p>\n<p><strong>多项式回归</strong>：可以添加$x^q$项更好地拟合数据图像，$f(x)&#x3D;w_1x^3+w_2x^2+w_1x^1+b$</p>\n<p>此时特征缩放尤为重要</p>\n<h2 id=\"分类-逻辑回归\"><a href=\"#分类-逻辑回归\" class=\"headerlink\" title=\"分类-逻辑回归\"></a>分类-逻辑回归</h2><p>解决二分类问题</p>\n<h3 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h3><p>输出介于$(0,1)$</p>\n<p>$g(z)&#x3D; \\frac{1}{1+e^{-z}},z \\subseteq R$</p>\n<p><strong>logistic regression</strong>:</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;g(\\vec{w} · \\vec{x}+b) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>输出值可以理解为分类为1的可能性</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;P(y&#x3D;1|\\vec{x};\\vec{w},b)$</p>\n<h3 id=\"决策边界decision-boundary\"><a href=\"#决策边界decision-boundary\" class=\"headerlink\" title=\"决策边界decision boundary\"></a>决策边界decision boundary</h3><p>以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b &lt;0$，取值0</p>\n<p>$\\vec{w} · \\vec{x}+b &#x3D; 0$称为决策边界</p>\n<p>多项式回归也适用于非线性的决策边界</p>\n<h3 id=\"成本函数\"><a href=\"#成本函数\" class=\"headerlink\" title=\"成本函数\"></a>成本函数</h3><p>如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$<strong>不是凸函数，不适用于逻辑回归</strong></p>\n<p>定义</p>\n<p>$$<br>J(w,b)&#x3D;\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})<br>$$<br>其中L代表单个样例的loss，J代表总的cost</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}&#x3D;1<br>$$<br><img src=\"/img/machine-learning-notes/pic-3.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-3.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>当y等于1，预测值越靠近1损失越小</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}&#x3D;0<br>$$<br><img src=\"/img/machine-learning-notes/pic-4.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-4.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>当y等于0，预测值越靠近0损失越小 </p>\n<p><strong>简化</strong>成本函数                                                                                                                                                                                                                                                                                          </p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>得到</p>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))<br>$$</p>\n<p>成本函数是凸函数，便于实现梯度下降</p>\n<h3 id=\"梯度下降-1\"><a href=\"#梯度下降-1\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>对J求偏导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>可以使用相似方法进行特征缩放</p>\n<h3 id=\"过拟合问题\"><a href=\"#过拟合问题\" class=\"headerlink\" title=\"过拟合问题\"></a>过拟合问题</h3><p>过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。</p>\n<p><img src=\"/img/machine-learning-notes/pic-5.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-5.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>解决过拟合</strong></p>\n<ul>\n<li>收集更多训练数据</li>\n<li>特征筛选，选择特征的一个子集</li>\n<li>正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响</li>\n</ul>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><p>如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>这样使得$w_j$尽可能小，几乎为0</p>\n<p>参数$b$是否正则化无关紧要</p>\n<p>**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显</p>\n<ul>\n<li>正则化线性回归</li>\n</ul>\n<p>对$J(w,b)$求偏导不断同步更新w,b的值</p>\n<p>$$<br>\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}<br>$$</p>\n<p>$$<br>w &#x3D; w- \\alpha (\\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}) &#x3D; (1-\\alpha \\frac{\\lambda}{m})w+…..<br>$$</p>\n<ul>\n<li>正则化逻辑回归</li>\n</ul>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>求导式和线性回归相同，只是需要注意<strong>正则化项偏导数没有求和</strong></p>\n<p>$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-6.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-6.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h1 id=\"Course-2\"><a href=\"#Course-2\" class=\"headerlink\" title=\"Course 2\"></a>Course 2</h1><h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><p>起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为<strong>深度学习</strong></p>\n<p>利用别人训练的神经网络参数称为推理或者预测</p>\n<p>为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数</p>\n<p>神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层-&gt;隐藏层-&gt;输出层）</p>\n<p>多层神经网络叫做多层感知机</p>\n<h2 id=\"神经网络中的层\"><a href=\"#神经网络中的层\" class=\"headerlink\" title=\"神经网络中的层\"></a>神经网络中的层</h2><p>讨论层数通常是隐藏层和输出层，不包括输入层</p>\n<p>每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层&#x2F;输出结果</p>\n<p>即$a_j^{[l]} &#x3D; g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$</p>\n<p>$j$表示神经元单元序号，$l$表示层数，$g(x)$为<code>sigmod</code>函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-7.jpg\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-7.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>$a_j^{[l]}$构成$\\vec{a}^{[l]}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-8.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-8.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"前向传播-forward-prop\"><a href=\"#前向传播-forward-prop\" class=\"headerlink\" title=\"前向传播(forward prop)\"></a>前向传播(forward prop)</h2><p>从输入初步传递到输出，即为前向传播</p>\n<p><strong>一般实现</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dense</span>(<span class=\"params\">a_in, W, b, g</span>):</span><br><span class=\"line\">\tunits = W.shape[<span class=\"number\">1</span>] <span class=\"comment\"># 单元数等于W矩阵的列数，w_j向量是列向量</span></span><br><span class=\"line\">\ta_out = np.zeros(units)</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(units):</span><br><span class=\"line\">\t\tw = W[:, j]</span><br><span class=\"line\">\t\tz = np.dot(w, a_in) + b</span><br><span class=\"line\">\t\ta_out[j] = g(z)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a_out</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sequential</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    a1 = dense(x, W1, b1)</span><br><span class=\"line\">    a2 = dense(a1, W2, b2)</span><br><span class=\"line\">    a3 = dense(a2, W3, b3)</span><br><span class=\"line\">    f_x = a3</span><br><span class=\"line\">    <span class=\"keyword\">return</span> f_x</span><br></pre></td></tr></table></figure>\n\n<p><strong>使用框架（TensorFlow&#x2F;Pytorch)）进行矢量化加速</strong></p>\n<h2 id=\"模型训练步骤\"><a href=\"#模型训练步骤\" class=\"headerlink\" title=\"模型训练步骤\"></a>模型训练步骤</h2><ol>\n<li>指定如何在给定输入X和参数的情况下计算输出(模型<strong>结构</strong>)</li>\n<li>指定<strong>损失函数</strong></li>\n<li><strong>训练</strong>模型以最小化损失函数</li>\n</ol>\n<p>二元交叉熵损失</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$<br>通过反向传播计算偏导数</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>用<code>ReLU</code>函数代替<code>sigmoid</code>激活，$g(z) &#x3D; max(0,z)$</p>\n<p><img src=\"/img/machine-learning-notes/pic-9.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-9.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>如何选择合适的激活函数？</strong></p>\n<p>取决于要预测的y，对于神经网络的<strong>输出层</strong>：</p>\n<ul>\n<li>二分类——&gt; sigmoid</li>\n<li>y可正可负的回归——&gt; linear</li>\n<li>回归中y大于等于0 ——&gt; ReLU</li>\n</ul>\n<p>对于神经网络的<strong>隐藏层</strong>建议使用ReLU</p>\n<p><code>ReLU</code>常用且更快</p>\n<ul>\n<li>不涉及指数运算</li>\n<li>当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x-&gt;-∞）,而sigmoid两端都是</li>\n</ul>\n<p><strong>为什么需要激活函数？</strong></p>\n<p>对于隐藏层，只使用线性的所有层等价于线性回归</p>\n<p>对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解</p>\n<h2 id=\"多分类问题\"><a href=\"#多分类问题\" class=\"headerlink\" title=\"多分类问题\"></a>多分类问题</h2><p><strong>softmax回归算法</strong>（logistic 推广）</p>\n<p>$z_1&#x3D;\\vec{w_1}·\\vec{x_1}+b_1$</p>\n<p>$a_1&#x3D;\\frac{e^{z_1}}{e^{z_1}+…+e^{z_n}} &#x3D; P(y&#x3D;1|\\vec{x})$</p>\n<p>即，设有N个分类</p>\n<p>$z_i&#x3D;\\vec{w_1}·\\vec{x_i}+b_i$</p>\n<p>$$<br>a_i &#x3D; \\frac{e^{z_i}}{\\sum_{k&#x3D;1}^{N} e^{z_i}}&#x3D;P(y&#x3D;i|\\vec{x})<br>$$<br>其中$a_1+a_2+…+a_N&#x3D;1$</p>\n<p><strong>softmax损失函数</strong></p>\n<p>回顾logistic回归</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>二分类问题，设$a_1 &#x3D; f_{w,b}(x^{(i)})$，即$y&#x3D;1$的概率</p>\n<p>则$a_2 &#x3D; 1-f_{w,b}(x^{(i)})$，即$y&#x3D;0$的概率</p>\n<p>简化为</p>\n<p>$loss &#x3D; -log(a_1)$ 如果$y&#x3D;1$</p>\n<p>$loss &#x3D; -log(a_2)$ 如果$y&#x3D;0$</p>\n<p>对于softmax回归算法</p>\n<p>$$<br>loss(a_1,a_2,…,a_N,y) &#x3D; \\left{\\begin{matrix} -log(a_1) \\quad if \\quad y&#x3D;1\\ -log(a_2) \\quad if \\quad y&#x3D;2 \\ … \\ -log(a_N) \\quad if \\quad y&#x3D;N \\end{matrix}\\right.<br>$$</p>\n<p><strong>神经网络中的softmax</strong></p>\n<p>输出层变为N个神经元</p>\n<p>注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 … z_n$的函数</p>\n<p><strong>softmax改进</strong></p>\n<p>由于<a href=\"https://blog.csdn.net/muyuu/article/details/122757470\">数值溢出和精度问题</a></p>\n<p>$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失</p>\n<p>不先计算出$a_i$，再带入损失函数</p>\n<p>而是<strong>直接</strong><br>$$<br>loss_i&#x3D;-log(\\frac{e^{z_i}}{e_{z_1}+…+e_{z_N}})<br>$$</p>\n<p>此时输出层只需要<code>linear</code>即可（就是不计算$a_i$），同时开启<code>from_logits=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.<span class=\"built_in\">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class=\"literal\">True</span>)) <span class=\"comment\">#稀疏分类交叉熵损失</span></span><br></pre></td></tr></table></figure>\n\n<p><code>from_logits=True</code>的<a href=\"https://blog.csdn.net/muyuu/article/details/122762442\">作用</a></p>\n<p>需要概率时再调用<code>softmax</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logits = model(X)</span><br><span class=\"line\">f_x = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>\n\n<p><strong>多标签分类</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-10.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-10.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。</p>\n<h2 id=\"高级优化方法\"><a href=\"#高级优化方法\" class=\"headerlink\" title=\"高级优化方法\"></a>高级优化方法</h2><p>传统的梯度下降学习率固定</p>\n<h3 id=\"Adam（Adaptive-Moment-estimation）\"><a href=\"#Adam（Adaptive-Moment-estimation）\" class=\"headerlink\" title=\"Adam（Adaptive Moment estimation）\"></a>Adam（Adaptive Moment estimation）</h3><p>如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率</p>\n<p>如果看到学习率太大，某个参数值来回振荡，会自动减小学习率</p>\n<p>可以自动调整学习率$\\alpha$</p>\n<p>对于每个参数都有一个$\\alpha$</p>\n<p>选择optimizer&#x3D;adam即可</p>\n<h2 id=\"其他的网络层\"><a href=\"#其他的网络层\" class=\"headerlink\" title=\"其他的网络层\"></a>其他的网络层</h2><h3 id=\"卷积层（Convolutional-Layer）\"><a href=\"#卷积层（Convolutional-Layer）\" class=\"headerlink\" title=\"卷积层（Convolutional Layer）\"></a>卷积层（Convolutional Layer）</h3><p>每个神经元只能看到前一个层输入的一部分</p>\n<ul>\n<li>加快计算速度</li>\n<li>需要更少的数据，不容易过拟合</li>\n</ul>\n<p>有多个卷积层，即卷积神经网络</p>\n<p>每一层的单元只查看输入的一部分 </p>\n<h2 id=\"构建机器学习系统\"><a href=\"#构建机器学习系统\" class=\"headerlink\" title=\"构建机器学习系统\"></a>构建机器学习系统</h2><h3 id=\"评估一个模型\"><a href=\"#评估一个模型\" class=\"headerlink\" title=\"评估一个模型\"></a>评估一个模型</h3><p>特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。</p>\n<p>将数据集分为训练集和测试集（73或者82开）</p>\n<p>分三步计算</p>\n<p><img src=\"/img/machine-learning-notes/pic-11.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-11.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>注意计算error时不包括正则化项</strong></p>\n<p>过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力</p>\n<p>对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）</p>\n<h3 id=\"如何选择模型\"><a href=\"#如何选择模型\" class=\"headerlink\" title=\"如何选择模型\"></a>如何选择模型</h3><p>数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$</p>\n<p>交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫<strong>dev set</strong>&#x2F;validation set</p>\n<p>$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力</p>\n<p>数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。</p>\n<h3 id=\"偏差和方差\"><a href=\"#偏差和方差\" class=\"headerlink\" title=\"偏差和方差\"></a><strong>偏差和方差</strong></h3><p><img src=\"/img/machine-learning-notes/pic-12.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-12.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-13.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-13.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多</p>\n<p>高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。</p>\n<p><strong>正则化项参数对偏差和方差的影响：</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-14.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-14.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>但是这些数值多少才算大&#x2F;小呢？需要<strong>建立基准性能标准</strong>，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差&#x2F;高偏差的问题</p>\n<p><img src=\"/img/machine-learning-notes/pic-15.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-15.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>学习曲线</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-16.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-16.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高偏差时 </p>\n<p><img src=\"/img/machine-learning-notes/pic-17.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-17.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高方差时</p>\n<p><img src=\"/img/machine-learning-notes/pic-18.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-18.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-19.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-19.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>判断高方差或者高偏差决定下一步怎么做</p>\n<p><strong>神经网络中的偏差和方差</strong></p>\n<p>大型的神经网络有很小的偏差，所以只需要关注方差</p>\n<p>并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好</p>\n<p>但是大型网络计算比较昂贵</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Dense(unit=<span class=\"number\">25</span>, activation=<span class=\"string\">&quot;relu&quot;</span>, kernel_regularizer=L2(<span class=\"number\">0.01</span>))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"开发机器学习系统的迭代\"><a href=\"#开发机器学习系统的迭代\" class=\"headerlink\" title=\"开发机器学习系统的迭代\"></a>开发机器学习系统的迭代</h2><p><img src=\"/img/machine-learning-notes/pic-20.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-20.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h2><p>在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法</p>\n<h2 id=\"添加数据\"><a href=\"#添加数据\" class=\"headerlink\" title=\"添加数据\"></a>添加数据</h2><p>由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用<strong>数据增强</strong>，用原有的数据样本创造新的样本</p>\n<p>例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等</p>\n<p>此外还有<strong>数据合成</strong>，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉</p>\n<p>AI &#x3D; Code(algorithm&#x2F;model) + Data</p>\n<h2 id=\"迁移学习（Transfer-Learning）\"><a href=\"#迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"迁移学习（Transfer Learning）\"></a>迁移学习（Transfer Learning）</h2><p>对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。</p>\n<p><img src=\"/img/machine-learning-notes/pic-21.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-21.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>第一步叫做<strong>监督预训练</strong>(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做<strong>微调</strong>(fine tuning)，更改输出层的权重</p>\n<p>这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习</p>\n<p>通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样</p>\n<p><img src=\"/img/machine-learning-notes/pic-22.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-22.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"机器学习项目的完整周期\"><a href=\"#机器学习项目的完整周期\" class=\"headerlink\" title=\"机器学习项目的完整周期\"></a>机器学习项目的完整周期</h2><p><img src=\"/img/machine-learning-notes/pic-23.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-23.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>部署</p>\n<p><img src=\"/img/machine-learning-notes/pic-24.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-24.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>MLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。</p>\n<h2 id=\"关注公平、偏见、伦理\"><a href=\"#关注公平、偏见、伦理\" class=\"headerlink\" title=\"关注公平、偏见、伦理\"></a>关注公平、偏见、伦理</h2><h2 id=\"倾斜数据集的误差指标\"><a href=\"#倾斜数据集的误差指标\" class=\"headerlink\" title=\"倾斜数据集的误差指标\"></a>倾斜数据集的误差指标</h2><p>某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造<strong>混淆矩阵</strong>，包括<strong>真正例，假正例，真负例，假负例</strong></p>\n<p>常用的计算指标是<strong>精确度(precision)<strong>和</strong>召回率(recall)</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-25.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-25.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度</p>\n<p>权衡：</p>\n<p>当我们只有十分确信时才设置y&#x3D;1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低</p>\n<p>当我们不希望错过实际上的y&#x3D;1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高</p>\n<p>通过设置threshold权衡precision和recall</p>\n<p>F1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）</p>\n<p>$F1 score &#x3D; \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} &#x3D; 2\\frac{PR}{P+R}$</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>\n<p><img src=\"/img/machine-learning-notes/pic-26.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-26.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>决策树学习</strong>：</p>\n<ul>\n<li>如果选择每个节点选择什么特征来分类？</li>\n</ul>\n<p>应该最大化纯度，每一边的种类尽可能少</p>\n<ul>\n<li>什么时候停止分类？</li>\n</ul>\n<p>当一个节点100%是一个种类</p>\n<p>当分裂节点时会导致树超过最大高度（超参数）</p>\n<p>当提高的纯度分数低于一个门槛值</p>\n<p>当一个节点的样本数量低于一个门槛值</p>\n<h3 id=\"衡量纯度（purity）\"><a href=\"#衡量纯度（purity）\" class=\"headerlink\" title=\"衡量纯度（purity）\"></a>衡量纯度（purity）</h3><p>熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 &#x3D; 1 - p_1$</p>\n<p>$H(p_1)&#x3D;-p_1log_2(p_1)-p_0log_2(p_0) &#x3D; -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$</p>\n<p>注意：$0log(0) &#x3D; 0$</p>\n<p><img src=\"/img/machine-learning-notes/pic-27.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-27.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"减小熵：信息增益（Information-Gain）\"><a href=\"#减小熵：信息增益（Information-Gain）\" class=\"headerlink\" title=\"减小熵：信息增益（Information Gain）\"></a>减小熵：信息增益（Information Gain）</h3><p>当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征</p>\n<p>实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值</p>\n<p>为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合</p>\n<p><img src=\"/img/machine-learning-notes/pic-28.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-28.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>更一般地</p>\n<p><img src=\"/img/machine-learning-notes/pic-29.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-29.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>p是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本）</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在根节点以所有数据样本开始</p>\n<p>计算所有特征的信息增益，选择最大的</p>\n<p>对选择的特征分裂，创建左右分支</p>\n<p>保持分裂直到遇到终止条件：</p>\n<ul>\n<li>当一个节点100%是一个类</li>\n<li>当分裂节点会导致树超过最大高度</li>\n<li>信息增益的值小于某个门槛值</li>\n<li>节点的样本数量小于某个门槛值</li>\n</ul>\n<p>实际上是一个递归的过程</p>\n<h3 id=\"独热编码-One-Hot-Encoding\"><a href=\"#独热编码-One-Hot-Encoding\" class=\"headerlink\" title=\"独热编码(One Hot Encoding)\"></a>独热编码(One Hot Encoding)</h3><p>实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0&#x2F;1）</p>\n<p>这样又转变为原来的左右分支分裂的情况</p>\n<h3 id=\"连续值特征\"><a href=\"#连续值特征\" class=\"headerlink\" title=\"连续值特征\"></a>连续值特征</h3><p>选定一个阈值，判断数据样本大于或者小于该阈值</p>\n<p>分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点</p>\n<p>对分割点分别计算信息增强来选择阈值</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>分裂时，改成尽量选取输出的方差(Variance)小的特征</p>\n<p>w还是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本），之后计算加权平均方差</p>\n<p>再用上一个节点所有数据的方差减去加权平均方差，选取最大的</p>\n<p>分类的结果是样本的平均值</p>\n<h2 id=\"使用多个决策树\"><a href=\"#使用多个决策树\" class=\"headerlink\" title=\"使用多个决策树\"></a>使用多个决策树</h2><p>单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮</p>\n<h3 id=\"放回抽样\"><a href=\"#放回抽样\" class=\"headerlink\" title=\"放回抽样\"></a>放回抽样</h3><p>从n个样本中放回地抽取n次，结果作为一个新的数据集</p>\n<h3 id=\"随机森林（Random-Forest）\"><a href=\"#随机森林（Random-Forest）\" class=\"headerlink\" title=\"随机森林（Random Forest）\"></a>随机森林（Random Forest）</h3><p>给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树</p>\n<p>b个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出</p>\n<p>对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k &lt; n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k &#x3D; \\sqrt{n}$</p>\n<h3 id=\"XGBoost（eXtreme-Gradient-Boosting）\"><a href=\"#XGBoost（eXtreme-Gradient-Boosting）\" class=\"headerlink\" title=\"XGBoost（eXtreme Gradient Boosting）\"></a>XGBoost（eXtreme Gradient Boosting）</h3><p>极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本</p>\n<p>思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分</p>\n<ul>\n<li>提升树的开源实现</li>\n<li>快速，有效</li>\n<li>很好的设定结束分裂的标准</li>\n<li>内置正则化</li>\n</ul>\n<h3 id=\"什么时候使用决策树\"><a href=\"#什么时候使用决策树\" class=\"headerlink\" title=\"什么时候使用决策树\"></a>什么时候使用决策树</h3><p>一个或多个决策树</p>\n<ul>\n<li>在表格化和结构化的数据上工作的很好</li>\n<li>不建议在非结构化的数据上，例如图片，音频，文本</li>\n<li>训练快速</li>\n<li>决策树是人类可以理解的（可解释性）</li>\n</ul>\n<p>神经网络</p>\n<ul>\n<li><p>对于所有类型的数据都能工作的很好</p>\n</li>\n<li><p>比决策树更慢</p>\n</li>\n<li><p>可以很好地使用迁移学习（预训练+微调）</p>\n</li>\n<li><p>当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）</p>\n</li>\n</ul>\n<h1 id=\"Course-3\"><a href=\"#Course-3\" class=\"headerlink\" title=\"Course 3\"></a>Course 3</h1><p>除了监督学习，机器学习还包括</p>\n<ul>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>异常检测</li>\n</ul>\n</li>\n<li>推荐系统</li>\n<li>强化学习</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><p>一堆数据点中自动查找相互关联或者相似的数据点</p>\n<h3 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means\"></a>K-means</h3><p>首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2… \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度</p>\n<ul>\n<li>将每个点分配给离他最近的中心点（centroid质心）</li>\n<li>将中心点移动到分配的点的平均中心</li>\n<li>重复前两步，直到中心点不再移动，K-means算法收敛</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Repeat&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to m</span><br><span class=\"line\">\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）</span><br><span class=\"line\">\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to k</span><br><span class=\"line\">\t\tu_k更新为分配的点的中心（每个轴的点的平均值）</span><br><span class=\"line\">\t\t如果簇中心点没有分配到点，就删除</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p><img src=\"/img/machine-learning-notes/pic-30.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-30.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）</p>\n<p>$u_k$是簇k</p>\n<p>$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇</p>\n<p>损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是<strong>欧几里得距离</strong></p>\n<p>也叫Distortion Function</p>\n<h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>选择$K&lt;m$</p>\n<p>随机选择K个训练样本，将$\\mu_1 ,\\mu_2… \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i = 1 to 100&#123;</span><br><span class=\"line\">\t随机初始化</span><br><span class=\"line\">\t获取c_i, u_i</span><br><span class=\"line\">\t计算损失函数J</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"选择簇的个数\"><a href=\"#选择簇的个数\" class=\"headerlink\" title=\"选择簇的个数\"></a>选择簇的个数</h3><p><strong>肘法（Elbow Method）</strong></p>\n<p>选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点</p>\n<p>所以K的选择还是按照之后的任务目的选择</p>\n<h2 id=\"异常检测\"><a href=\"#异常检测\" class=\"headerlink\" title=\"异常检测\"></a>异常检测</h2><h3 id=\"密度估计（Density-estimation）\"><a href=\"#密度估计（Density-estimation）\" class=\"headerlink\" title=\"密度估计（Density estimation）\"></a>密度估计（Density estimation）</h3><p>根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})&lt;\\epsilon$，认为出现了异常（anomaly）</p>\n<h3 id=\"高斯分布\"><a href=\"#高斯分布\" class=\"headerlink\" title=\"高斯分布\"></a>高斯分布</h3><p>Gaussian Distribution，也叫正态分布(Normal Distribution)</p>\n<p>$p(x)&#x3D;\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$</p>\n<p>其中$\\mu$是平均值，$\\sigma$是标准差</p>\n<p><img src=\"/img/machine-learning-notes/pic-31.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-31.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>对于有多个特征的输入$\\vec{x}$，$\\vec{x} &#x3D; [x_1, x_2 … x_n]$</p>\n<p>$$<br>p(\\vec{x}) &#x3D; p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) <em>…</em> p(x_n;\\mu_n,\\sigma_n^2) &#x3D; \\prod_{j&#x3D;1}^np(x_j;\\mu_j,\\sigma_j^2)<br>$$</p>\n<h3 id=\"开发和评估异常检测系统\"><a href=\"#开发和评估异常检测系统\" class=\"headerlink\" title=\"开发和评估异常检测系统\"></a>开发和评估异常检测系统</h3><p>通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0&#x2F;1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集</p>\n<p><strong>流程：</strong></p>\n<p>在训练集$x_1…x_m$上拟合模型$p(x)$</p>\n<p>在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）</p>\n<p>之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$</p>\n<h3 id=\"权衡异常检测和监督学习\"><a href=\"#权衡异常检测和监督学习\" class=\"headerlink\" title=\"权衡异常检测和监督学习\"></a>权衡异常检测和监督学习</h3><p>异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样</p>\n<p>监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><p>监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要</p>\n<ul>\n<li>绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）</li>\n<li>检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取</li>\n</ul>\n<h2 id=\"推荐系统\"><a href=\"#推荐系统\" class=\"headerlink\" title=\"推荐系统\"></a>推荐系统</h2><p>$r(i,j) &#x3D; 1$表示用户j为电影i打分</p>\n<p>$y^{(i,j)}$表示用户j为电影i打的分</p>\n<p>$w^{(j)}, b^{(j)}$是用户j的参数</p>\n<p>$x^{(i)}$是电影i的特征向量</p>\n<p>对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$</p>\n<p>$m^{(j)}$表示用户j打分的电影数量</p>\n<p>通过训练学习$w^{(j)}, b^{(j)}$</p>\n<p>$$<br>\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)&#x3D;\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<p>对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},…,w^{(n_u)},b^{(n_u)}$</p>\n<p>$$<br>\\left.\\mathrm{J}\\left(<br>\\begin{array}<br>{cc}{w^{(1)},} &amp; {…,w^{(n_{u})}} \\<br>{b^{(1)},} &amp; {…,b^{(n_{u})}}<br>\\end{array}\\right.\\right)&#x3D;\\frac{1}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<h3 id=\"协同过滤算法\"><a href=\"#协同过滤算法\" class=\"headerlink\" title=\"协同过滤算法\"></a>协同过滤算法</h3><p>在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少</p>\n<p>$$<br>\\mathrm{J}(x^{(i)})&#x3D;\\frac{1}{2}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>学习得到$x^{(1)},x^{(2)},…,x^{(n_m)}$</p>\n<p>$$<br>\\mathrm{J}\\left(x^{(1)},x^{(2)},…,x^{(n_{m})}\\right)&#x3D;\\frac{1}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：</p>\n<p><img src=\"/img/machine-learning-notes/pic-32.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-32.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>梯度下降时，w，b，x都是参数</p>\n<p><img src=\"/img/machine-learning-notes/pic-33.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-33.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><a href=\"https://blog.csdn.net/zhu_xian_gang/article/details/130243870\">补充</a></p>\n<h3 id=\"二进制标签\"><a href=\"#二进制标签\" class=\"headerlink\" title=\"二进制标签\"></a>二进制标签</h3><p>1-用户看到物品之后参与点击，停留，添加喜欢，购买</p>\n<p>0-用户看到物品之后忽略</p>\n<p>?-用户没有看到物品</p>\n<p>预测$y^{(i,j)}&#x3D;1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-34.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-34.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"均值归一化\"><a href=\"#均值归一化\" class=\"headerlink\" title=\"均值归一化\"></a>均值归一化</h3><p><strong>Mean Normalization</strong></p>\n<ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n<p>求出每个电影的平均用户平方$\\mu_i$，构建向量$u$</p>\n<p>对于用户j，预测其在电影i的评分：</p>\n<p>$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$</p>\n<p>以至于不会当用户没有评分时认为评分接近0，而是接近平均值</p>\n<h3 id=\"查找相关项目\"><a href=\"#查找相关项目\" class=\"headerlink\" title=\"查找相关项目\"></a>查找相关项目</h3><p>对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似</p>\n<p>选取小的$\\sum_{l&#x3D;1}^n(x_l^{(k)} - x_l^{(i)})^2$</p>\n<p>也可以写作$||x^{(k)} - x^{(i)}||^2$</p>\n<h3 id=\"协同过滤算法的限制\"><a href=\"#协同过滤算法的限制\" class=\"headerlink\" title=\"协同过滤算法的限制\"></a>协同过滤算法的限制</h3><p><strong>冷启动问题</strong></p>\n<ul>\n<li>如何对没有什么用户打分的项目评分？</li>\n<li>如何对没有对很多项目打分的用户推荐一些项目？</li>\n</ul>\n<p><strong>没有很多信息的时候利用辅助信息</strong></p>\n<h3 id=\"基于内容的过滤算法\"><a href=\"#基于内容的过滤算法\" class=\"headerlink\" title=\"基于内容的过滤算法\"></a>基于内容的过滤算法</h3><p>协同过滤：基于用户的评分与你的评分的相似推荐项目</p>\n<p>基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目</p>\n<p>但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配</p>\n<p>对于v的获取，使用神经网络</p>\n<p>可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积</p>\n<p>也可以将两个网络合并，在内部进行点积输出结果</p>\n<p>$$<br>J&#x3D;\\sum_{(i,j):r(i,j)&#x3D;1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}<br>$$</p>\n<p>为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似</p>\n<h3 id=\"Retrieval-and-Ranking\"><a href=\"#Retrieval-and-Ranking\" class=\"headerlink\" title=\"Retrieval and Ranking\"></a>Retrieval and Ranking</h3><p>通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名</p>\n<p>检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看</p>\n<p>排名：对这些检索出的有限个项目进行学习，根据结果进行排名</p>\n<p>权衡检索的项目数量</p>\n<h2 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h2><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。</p>\n<p>涉及状态，行动，奖励，折扣系数，回报，策略</p>\n<h3 id=\"回报\"><a href=\"#回报\" class=\"headerlink\" title=\"回报\"></a>回报</h3><p>指的是系统获得的奖励总和</p>\n<p>折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+…$，直到终止状态</p>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>状态state通过策略π实行行动a</p>\n<p>$\\pi(s) &#x3D; a$，指明状态s情况下需要进行的决策a，从而最大化回报</p>\n<h3 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h3><p>Markov Decision Process(MDP)</p>\n<p><img src=\"/img/machine-learning-notes/pic-35.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-35.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"状态-动作价值函数\"><a href=\"#状态-动作价值函数\" class=\"headerlink\" title=\"状态-动作价值函数\"></a>状态-动作价值函数</h3><p>State-action value function，也叫Q-function,Q*,Optimal Q function</p>\n<p>$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报</p>\n<p>在状态s的最好回报就是$max_aQ(s,a)$</p>\n<p>在状态s的最好动作的就能够提供$max_aQ(s,a)$的</p>\n<h3 id=\"Bellman方程\"><a href=\"#Bellman方程\" class=\"headerlink\" title=\"Bellman方程\"></a>Bellman方程</h3><p>$s$:当前状态</p>\n<p>$a$:当前状态的决策</p>\n<p>$R(s)$:当前状态的奖励</p>\n<p>$s’$:采取动作a后的状态</p>\n<p>$a’$:在状态s’采取的动作</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>R(s)也叫即时奖励，表示你可以立刻得到的奖励</p>\n<p>后一项是从状态s’表现得最好获得的回报</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+… &#x3D; R_1 + \\gamma[R_2 + \\gamma R_3+…]$</p>\n<h3 id=\"随机环境\"><a href=\"#随机环境\" class=\"headerlink\" title=\"随机环境\"></a>随机环境</h3><p>由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励</p>\n<p>所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望</p>\n<p>$\\text{Return} &#x3D; \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+…) &#x3D; \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+…)$</p>\n<p>Bellman Equation变成：</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma \\text{E} [max_{a’}Q(s’,a’)]$</p>\n<h3 id=\"连续状态空间\"><a href=\"#连续状态空间\" class=\"headerlink\" title=\"连续状态空间\"></a>连续状态空间</h3><p>状态参数可能是连续的，比如坐标，角度，速度</p>\n<p>同时状态可能有多个，比如xyz坐标，速度等</p>\n<p>此时也叫连续状态马尔科夫决策过程</p>\n<h3 id=\"学习状态值函数\"><a href=\"#学习状态值函数\" class=\"headerlink\" title=\"学习状态值函数\"></a>学习状态值函数</h3><p><img src=\"/img/machine-learning-notes/pic-36.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-36.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>以随机猜测$Q(s,a)$初始化神经网络</p>\n<p>重复：</p>\n<p>采取措施，得到$(s,a,R(s),s’)$元组</p>\n<p>存储最近的10k个 $(s,a,R(s),s’)$元组（Replay Buffer）</p>\n<p>训练网络：</p>\n<p>​\t创建10k个训练集，其中$x&#x3D;(s,a)$，$y &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$</p>\n<p>令$Q&#x3D;Q_{new}$</p>\n<p>虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计</p>\n<p><strong>改进</strong></p>\n<ul>\n<li>神经网络架构</li>\n</ul>\n<p>可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行</p>\n<p><img src=\"/img/machine-learning-notes/pic-37.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-37.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<ul>\n<li>$\\epsilon$贪心策略</li>\n</ul>\n<p>当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。</p>\n<p>应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）</p>\n<p>小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。</p>\n<ul>\n<li>小批量$mini-batch$</li>\n</ul>\n<p>将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优</p>\n<p><img src=\"/img/machine-learning-notes/pic-38.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-38.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>假设子集大小为1000；</p>\n<p>具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。</p>\n<p>在强化学习中，可以把10k的数据集分解训练多个模型</p>\n<ul>\n<li>软更新</li>\n</ul>\n<p>令$Q&#x3D;Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$</p>\n<p>而是<br>$$<br>w &#x3D; 0.01w_{new} + 0.99w<br>$$</p>\n<p>$$<br>b &#x3D; 0.01b_{new} + 0.99b<br>$$</p>\n<p>对参数进行微小调整</p>\n","more":"<h1 id=\"Course-1\"><a href=\"#Course-1\" class=\"headerlink\" title=\"Course 1\"></a>Course 1</h1><p>监督学习：输入特征x，输出目标y。对数据集进行预测，分为<strong>回归</strong>和<strong>分类</strong></p>\n<p>无监督学习：输入特征x，没有目标y，对数据集进行<strong>聚类预测</strong>，<strong>异常检测</strong>，<strong>降维</strong></p>\n<h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><p>$$<br>y^i &#x3D; wx^i+b<br>$$</p>\n<p>定义损失函数（成本函数），需要最小化损失函数</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2<br>$$</p>\n<p>其中$y^i$为真实输出，$\\hat{y}$为预测输出</p>\n<ul>\n<li>为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差</li>\n<li>1&#x2F;2是为了方便求导计算</li>\n</ul>\n<p>loss针对一个训练样本，cost是所有训练样本的均值</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>需要最小会损失函数，需要使用梯度下降算法</p>\n<p>定义学习率<code>learning_rate</code>为$\\alpha$,一般$\\alpha \\subseteq [0,1]$</p>\n<p>$w &#x3D; w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$</p>\n<p>$b &#x3D; b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$</p>\n<ul>\n<li>梯度下降时建议<strong>同步</strong>梯度下降，如下图</li>\n</ul>\n<p><img src=\"/img/machine-learning-notes/pic-1.png\" alt=\"img\"></p>\n<p>如果$\\alpha$太小，可以得到答案，但是时间过长</p>\n<p>如果$\\alpha$太大，大交叉无法收敛，甚至发散</p>\n<p>当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解</p>\n<p>对导数项分别求导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; wx^i+b$</p>\n<p>对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值</p>\n<p>选择合适得到学习率，就可以得到$min(J(w,b))$</p>\n<p>线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例</p>\n<h3 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h3><p>假设特征有$n$个，定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>则$f_{\\vec{w},b}&#x3D;\\vec{w} \\cdot \\vec{x} +b$</p>\n<p><code>·</code>为两个向量的点积(dot)。</p>\n<p>$$<br>\\vec{w} \\cdot \\vec{x} &#x3D; w_1<em>x_1+w_2</em>x_2+….+w_n*x_n<br>$$</p>\n<p><strong>矢量化</strong>：利用计算机的并行硬件，代码简洁、运行速度快</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>\n\n<p><strong>多元线性回归的梯度下降</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-2.png\" alt=\"img\"></p>\n<p>PS: 正规方程：某些机器学习库在后端求$w,b$的方法，<strong>只适用于线性回归</strong>，而且速度慢，不要求掌握</p>\n<h3 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h3><p>不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动</p>\n<p>加快梯度下降速度</p>\n<p>避免特征的取值范围差异过大，将其进行缩放，几种常见方法：</p>\n<ul>\n<li><strong>除以最大值</strong>，$x_{1,scale} &#x3D; \\frac{x_1}{max}$， $x \\in [0,1]$</li>\n<li><strong>均值归一化Mean Normalization</strong><ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n</li>\n<li><strong><code>Z-score</code>归一化</strong><ul>\n<li>求标准差$\\sigma$，均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{\\sigma}$</li>\n</ul>\n</li>\n</ul>\n<p><strong>判断梯度下降是否收敛：</strong></p>\n<ol>\n<li>观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）</li>\n</ol>\n<p><strong>选择合适学习率</strong>：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$</p>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测</p>\n<p><strong>多项式回归</strong>：可以添加$x^q$项更好地拟合数据图像，$f(x)&#x3D;w_1x^3+w_2x^2+w_1x^1+b$</p>\n<p>此时特征缩放尤为重要</p>\n<h2 id=\"分类-逻辑回归\"><a href=\"#分类-逻辑回归\" class=\"headerlink\" title=\"分类-逻辑回归\"></a>分类-逻辑回归</h2><p>解决二分类问题</p>\n<h3 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h3><p>输出介于$(0,1)$</p>\n<p>$g(z)&#x3D; \\frac{1}{1+e^{-z}},z \\subseteq R$</p>\n<p><strong>logistic regression</strong>:</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;g(\\vec{w} · \\vec{x}+b) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>输出值可以理解为分类为1的可能性</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;P(y&#x3D;1|\\vec{x};\\vec{w},b)$</p>\n<h3 id=\"决策边界decision-boundary\"><a href=\"#决策边界decision-boundary\" class=\"headerlink\" title=\"决策边界decision boundary\"></a>决策边界decision boundary</h3><p>以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b &lt;0$，取值0</p>\n<p>$\\vec{w} · \\vec{x}+b &#x3D; 0$称为决策边界</p>\n<p>多项式回归也适用于非线性的决策边界</p>\n<h3 id=\"成本函数\"><a href=\"#成本函数\" class=\"headerlink\" title=\"成本函数\"></a>成本函数</h3><p>如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$<strong>不是凸函数，不适用于逻辑回归</strong></p>\n<p>定义</p>\n<p>$$<br>J(w,b)&#x3D;\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})<br>$$<br>其中L代表单个样例的loss，J代表总的cost</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}&#x3D;1<br>$$<br><img src=\"/img/machine-learning-notes/pic-3.png\" alt=\"img\"></p>\n<p>当y等于1，预测值越靠近1损失越小</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}&#x3D;0<br>$$<br><img src=\"/img/machine-learning-notes/pic-4.png\" alt=\"img\"></p>\n<p>当y等于0，预测值越靠近0损失越小 </p>\n<p><strong>简化</strong>成本函数                                                                                                                                                                                                                                                                                          </p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>得到</p>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))<br>$$</p>\n<p>成本函数是凸函数，便于实现梯度下降</p>\n<h3 id=\"梯度下降-1\"><a href=\"#梯度下降-1\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>对J求偏导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>可以使用相似方法进行特征缩放</p>\n<h3 id=\"过拟合问题\"><a href=\"#过拟合问题\" class=\"headerlink\" title=\"过拟合问题\"></a>过拟合问题</h3><p>过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。</p>\n<p><img src=\"/img/machine-learning-notes/pic-5.png\" alt=\"img\"></p>\n<p><strong>解决过拟合</strong></p>\n<ul>\n<li>收集更多训练数据</li>\n<li>特征筛选，选择特征的一个子集</li>\n<li>正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响</li>\n</ul>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><p>如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>这样使得$w_j$尽可能小，几乎为0</p>\n<p>参数$b$是否正则化无关紧要</p>\n<p>**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显</p>\n<ul>\n<li>正则化线性回归</li>\n</ul>\n<p>对$J(w,b)$求偏导不断同步更新w,b的值</p>\n<p>$$<br>\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}<br>$$</p>\n<p>$$<br>w &#x3D; w- \\alpha (\\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}) &#x3D; (1-\\alpha \\frac{\\lambda}{m})w+…..<br>$$</p>\n<ul>\n<li>正则化逻辑回归</li>\n</ul>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>求导式和线性回归相同，只是需要注意<strong>正则化项偏导数没有求和</strong></p>\n<p>$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-6.png\" alt=\"img\"></p>\n<h1 id=\"Course-2\"><a href=\"#Course-2\" class=\"headerlink\" title=\"Course 2\"></a>Course 2</h1><h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><p>起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为<strong>深度学习</strong></p>\n<p>利用别人训练的神经网络参数称为推理或者预测</p>\n<p>为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数</p>\n<p>神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层-&gt;隐藏层-&gt;输出层）</p>\n<p>多层神经网络叫做多层感知机</p>\n<h2 id=\"神经网络中的层\"><a href=\"#神经网络中的层\" class=\"headerlink\" title=\"神经网络中的层\"></a>神经网络中的层</h2><p>讨论层数通常是隐藏层和输出层，不包括输入层</p>\n<p>每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层&#x2F;输出结果</p>\n<p>即$a_j^{[l]} &#x3D; g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$</p>\n<p>$j$表示神经元单元序号，$l$表示层数，$g(x)$为<code>sigmod</code>函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-7.jpg\" alt=\"img\"></p>\n<p>$a_j^{[l]}$构成$\\vec{a}^{[l]}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-8.png\" alt=\"img\"></p>\n<h2 id=\"前向传播-forward-prop\"><a href=\"#前向传播-forward-prop\" class=\"headerlink\" title=\"前向传播(forward prop)\"></a>前向传播(forward prop)</h2><p>从输入初步传递到输出，即为前向传播</p>\n<p><strong>一般实现</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dense</span>(<span class=\"params\">a_in, W, b, g</span>):</span><br><span class=\"line\">\tunits = W.shape[<span class=\"number\">1</span>] <span class=\"comment\"># 单元数等于W矩阵的列数，w_j向量是列向量</span></span><br><span class=\"line\">\ta_out = np.zeros(units)</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(units):</span><br><span class=\"line\">\t\tw = W[:, j]</span><br><span class=\"line\">\t\tz = np.dot(w, a_in) + b</span><br><span class=\"line\">\t\ta_out[j] = g(z)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a_out</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sequential</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    a1 = dense(x, W1, b1)</span><br><span class=\"line\">    a2 = dense(a1, W2, b2)</span><br><span class=\"line\">    a3 = dense(a2, W3, b3)</span><br><span class=\"line\">    f_x = a3</span><br><span class=\"line\">    <span class=\"keyword\">return</span> f_x</span><br></pre></td></tr></table></figure>\n\n<p><strong>使用框架（TensorFlow&#x2F;Pytorch)）进行矢量化加速</strong></p>\n<h2 id=\"模型训练步骤\"><a href=\"#模型训练步骤\" class=\"headerlink\" title=\"模型训练步骤\"></a>模型训练步骤</h2><ol>\n<li>指定如何在给定输入X和参数的情况下计算输出(模型<strong>结构</strong>)</li>\n<li>指定<strong>损失函数</strong></li>\n<li><strong>训练</strong>模型以最小化损失函数</li>\n</ol>\n<p>二元交叉熵损失</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$<br>通过反向传播计算偏导数</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>用<code>ReLU</code>函数代替<code>sigmoid</code>激活，$g(z) &#x3D; max(0,z)$</p>\n<p><img src=\"/img/machine-learning-notes/pic-9.png\" alt=\"img\"></p>\n<p><strong>如何选择合适的激活函数？</strong></p>\n<p>取决于要预测的y，对于神经网络的<strong>输出层</strong>：</p>\n<ul>\n<li>二分类——&gt; sigmoid</li>\n<li>y可正可负的回归——&gt; linear</li>\n<li>回归中y大于等于0 ——&gt; ReLU</li>\n</ul>\n<p>对于神经网络的<strong>隐藏层</strong>建议使用ReLU</p>\n<p><code>ReLU</code>常用且更快</p>\n<ul>\n<li>不涉及指数运算</li>\n<li>当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x-&gt;-∞）,而sigmoid两端都是</li>\n</ul>\n<p><strong>为什么需要激活函数？</strong></p>\n<p>对于隐藏层，只使用线性的所有层等价于线性回归</p>\n<p>对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解</p>\n<h2 id=\"多分类问题\"><a href=\"#多分类问题\" class=\"headerlink\" title=\"多分类问题\"></a>多分类问题</h2><p><strong>softmax回归算法</strong>（logistic 推广）</p>\n<p>$z_1&#x3D;\\vec{w_1}·\\vec{x_1}+b_1$</p>\n<p>$a_1&#x3D;\\frac{e^{z_1}}{e^{z_1}+…+e^{z_n}} &#x3D; P(y&#x3D;1|\\vec{x})$</p>\n<p>即，设有N个分类</p>\n<p>$z_i&#x3D;\\vec{w_1}·\\vec{x_i}+b_i$</p>\n<p>$$<br>a_i &#x3D; \\frac{e^{z_i}}{\\sum_{k&#x3D;1}^{N} e^{z_i}}&#x3D;P(y&#x3D;i|\\vec{x})<br>$$<br>其中$a_1+a_2+…+a_N&#x3D;1$</p>\n<p><strong>softmax损失函数</strong></p>\n<p>回顾logistic回归</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>二分类问题，设$a_1 &#x3D; f_{w,b}(x^{(i)})$，即$y&#x3D;1$的概率</p>\n<p>则$a_2 &#x3D; 1-f_{w,b}(x^{(i)})$，即$y&#x3D;0$的概率</p>\n<p>简化为</p>\n<p>$loss &#x3D; -log(a_1)$ 如果$y&#x3D;1$</p>\n<p>$loss &#x3D; -log(a_2)$ 如果$y&#x3D;0$</p>\n<p>对于softmax回归算法</p>\n<p>$$<br>loss(a_1,a_2,…,a_N,y) &#x3D; \\left{\\begin{matrix} -log(a_1) \\quad if \\quad y&#x3D;1\\ -log(a_2) \\quad if \\quad y&#x3D;2 \\ … \\ -log(a_N) \\quad if \\quad y&#x3D;N \\end{matrix}\\right.<br>$$</p>\n<p><strong>神经网络中的softmax</strong></p>\n<p>输出层变为N个神经元</p>\n<p>注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 … z_n$的函数</p>\n<p><strong>softmax改进</strong></p>\n<p>由于<a href=\"https://blog.csdn.net/muyuu/article/details/122757470\">数值溢出和精度问题</a></p>\n<p>$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失</p>\n<p>不先计算出$a_i$，再带入损失函数</p>\n<p>而是<strong>直接</strong><br>$$<br>loss_i&#x3D;-log(\\frac{e^{z_i}}{e_{z_1}+…+e_{z_N}})<br>$$</p>\n<p>此时输出层只需要<code>linear</code>即可（就是不计算$a_i$），同时开启<code>from_logits=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.<span class=\"built_in\">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class=\"literal\">True</span>)) <span class=\"comment\">#稀疏分类交叉熵损失</span></span><br></pre></td></tr></table></figure>\n\n<p><code>from_logits=True</code>的<a href=\"https://blog.csdn.net/muyuu/article/details/122762442\">作用</a></p>\n<p>需要概率时再调用<code>softmax</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logits = model(X)</span><br><span class=\"line\">f_x = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>\n\n<p><strong>多标签分类</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-10.png\" alt=\"img\"></p>\n<p>将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。</p>\n<h2 id=\"高级优化方法\"><a href=\"#高级优化方法\" class=\"headerlink\" title=\"高级优化方法\"></a>高级优化方法</h2><p>传统的梯度下降学习率固定</p>\n<h3 id=\"Adam（Adaptive-Moment-estimation）\"><a href=\"#Adam（Adaptive-Moment-estimation）\" class=\"headerlink\" title=\"Adam（Adaptive Moment estimation）\"></a>Adam（Adaptive Moment estimation）</h3><p>如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率</p>\n<p>如果看到学习率太大，某个参数值来回振荡，会自动减小学习率</p>\n<p>可以自动调整学习率$\\alpha$</p>\n<p>对于每个参数都有一个$\\alpha$</p>\n<p>选择optimizer&#x3D;adam即可</p>\n<h2 id=\"其他的网络层\"><a href=\"#其他的网络层\" class=\"headerlink\" title=\"其他的网络层\"></a>其他的网络层</h2><h3 id=\"卷积层（Convolutional-Layer）\"><a href=\"#卷积层（Convolutional-Layer）\" class=\"headerlink\" title=\"卷积层（Convolutional Layer）\"></a>卷积层（Convolutional Layer）</h3><p>每个神经元只能看到前一个层输入的一部分</p>\n<ul>\n<li>加快计算速度</li>\n<li>需要更少的数据，不容易过拟合</li>\n</ul>\n<p>有多个卷积层，即卷积神经网络</p>\n<p>每一层的单元只查看输入的一部分 </p>\n<h2 id=\"构建机器学习系统\"><a href=\"#构建机器学习系统\" class=\"headerlink\" title=\"构建机器学习系统\"></a>构建机器学习系统</h2><h3 id=\"评估一个模型\"><a href=\"#评估一个模型\" class=\"headerlink\" title=\"评估一个模型\"></a>评估一个模型</h3><p>特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。</p>\n<p>将数据集分为训练集和测试集（73或者82开）</p>\n<p>分三步计算</p>\n<p><img src=\"/img/machine-learning-notes/pic-11.png\" alt=\"img\"></p>\n<p><strong>注意计算error时不包括正则化项</strong></p>\n<p>过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力</p>\n<p>对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）</p>\n<h3 id=\"如何选择模型\"><a href=\"#如何选择模型\" class=\"headerlink\" title=\"如何选择模型\"></a>如何选择模型</h3><p>数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$</p>\n<p>交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫<strong>dev set</strong>&#x2F;validation set</p>\n<p>$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力</p>\n<p>数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。</p>\n<h3 id=\"偏差和方差\"><a href=\"#偏差和方差\" class=\"headerlink\" title=\"偏差和方差\"></a><strong>偏差和方差</strong></h3><p><img src=\"/img/machine-learning-notes/pic-12.png\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-13.png\" alt=\"img\"></p>\n<p>高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多</p>\n<p>高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。</p>\n<p><strong>正则化项参数对偏差和方差的影响：</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-14.png\" alt=\"img\"></p>\n<p>但是这些数值多少才算大&#x2F;小呢？需要<strong>建立基准性能标准</strong>，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差&#x2F;高偏差的问题</p>\n<p><img src=\"/img/machine-learning-notes/pic-15.png\" alt=\"img\"></p>\n<p><strong>学习曲线</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-16.png\" alt=\"img\"></p>\n<p>高偏差时 </p>\n<p><img src=\"/img/machine-learning-notes/pic-17.png\" alt=\"img\"></p>\n<p>高方差时</p>\n<p><img src=\"/img/machine-learning-notes/pic-18.png\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-19.png\" alt=\"img\"></p>\n<p>判断高方差或者高偏差决定下一步怎么做</p>\n<p><strong>神经网络中的偏差和方差</strong></p>\n<p>大型的神经网络有很小的偏差，所以只需要关注方差</p>\n<p>并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好</p>\n<p>但是大型网络计算比较昂贵</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Dense(unit=<span class=\"number\">25</span>, activation=<span class=\"string\">&quot;relu&quot;</span>, kernel_regularizer=L2(<span class=\"number\">0.01</span>))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"开发机器学习系统的迭代\"><a href=\"#开发机器学习系统的迭代\" class=\"headerlink\" title=\"开发机器学习系统的迭代\"></a>开发机器学习系统的迭代</h2><p><img src=\"/img/machine-learning-notes/pic-20.png\" alt=\"img\"></p>\n<h2 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h2><p>在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法</p>\n<h2 id=\"添加数据\"><a href=\"#添加数据\" class=\"headerlink\" title=\"添加数据\"></a>添加数据</h2><p>由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用<strong>数据增强</strong>，用原有的数据样本创造新的样本</p>\n<p>例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等</p>\n<p>此外还有<strong>数据合成</strong>，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉</p>\n<p>AI &#x3D; Code(algorithm&#x2F;model) + Data</p>\n<h2 id=\"迁移学习（Transfer-Learning）\"><a href=\"#迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"迁移学习（Transfer Learning）\"></a>迁移学习（Transfer Learning）</h2><p>对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。</p>\n<p><img src=\"/img/machine-learning-notes/pic-21.png\" alt=\"img\"></p>\n<p>第一步叫做<strong>监督预训练</strong>(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做<strong>微调</strong>(fine tuning)，更改输出层的权重</p>\n<p>这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习</p>\n<p>通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样</p>\n<p><img src=\"/img/machine-learning-notes/pic-22.png\" alt=\"img\"></p>\n<h2 id=\"机器学习项目的完整周期\"><a href=\"#机器学习项目的完整周期\" class=\"headerlink\" title=\"机器学习项目的完整周期\"></a>机器学习项目的完整周期</h2><p><img src=\"/img/machine-learning-notes/pic-23.png\" alt=\"img\"></p>\n<p>部署</p>\n<p><img src=\"/img/machine-learning-notes/pic-24.png\" alt=\"img\"></p>\n<p>MLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。</p>\n<h2 id=\"关注公平、偏见、伦理\"><a href=\"#关注公平、偏见、伦理\" class=\"headerlink\" title=\"关注公平、偏见、伦理\"></a>关注公平、偏见、伦理</h2><h2 id=\"倾斜数据集的误差指标\"><a href=\"#倾斜数据集的误差指标\" class=\"headerlink\" title=\"倾斜数据集的误差指标\"></a>倾斜数据集的误差指标</h2><p>某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造<strong>混淆矩阵</strong>，包括<strong>真正例，假正例，真负例，假负例</strong></p>\n<p>常用的计算指标是<strong>精确度(precision)<strong>和</strong>召回率(recall)</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-25.png\" alt=\"img\"></p>\n<p>精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度</p>\n<p>权衡：</p>\n<p>当我们只有十分确信时才设置y&#x3D;1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低</p>\n<p>当我们不希望错过实际上的y&#x3D;1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高</p>\n<p>通过设置threshold权衡precision和recall</p>\n<p>F1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）</p>\n<p>$F1 score &#x3D; \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} &#x3D; 2\\frac{PR}{P+R}$</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>\n<p><img src=\"/img/machine-learning-notes/pic-26.png\" alt=\"img\"></p>\n<p><strong>决策树学习</strong>：</p>\n<ul>\n<li>如果选择每个节点选择什么特征来分类？</li>\n</ul>\n<p>应该最大化纯度，每一边的种类尽可能少</p>\n<ul>\n<li>什么时候停止分类？</li>\n</ul>\n<p>当一个节点100%是一个种类</p>\n<p>当分裂节点时会导致树超过最大高度（超参数）</p>\n<p>当提高的纯度分数低于一个门槛值</p>\n<p>当一个节点的样本数量低于一个门槛值</p>\n<h3 id=\"衡量纯度（purity）\"><a href=\"#衡量纯度（purity）\" class=\"headerlink\" title=\"衡量纯度（purity）\"></a>衡量纯度（purity）</h3><p>熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 &#x3D; 1 - p_1$</p>\n<p>$H(p_1)&#x3D;-p_1log_2(p_1)-p_0log_2(p_0) &#x3D; -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$</p>\n<p>注意：$0log(0) &#x3D; 0$</p>\n<p><img src=\"/img/machine-learning-notes/pic-27.png\" alt=\"img\"></p>\n<h3 id=\"减小熵：信息增益（Information-Gain）\"><a href=\"#减小熵：信息增益（Information-Gain）\" class=\"headerlink\" title=\"减小熵：信息增益（Information Gain）\"></a>减小熵：信息增益（Information Gain）</h3><p>当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征</p>\n<p>实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值</p>\n<p>为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合</p>\n<p><img src=\"/img/machine-learning-notes/pic-28.png\" alt=\"img\"></p>\n<p>更一般地</p>\n<p><img src=\"/img/machine-learning-notes/pic-29.png\" alt=\"img\"></p>\n<p>p是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本）</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在根节点以所有数据样本开始</p>\n<p>计算所有特征的信息增益，选择最大的</p>\n<p>对选择的特征分裂，创建左右分支</p>\n<p>保持分裂直到遇到终止条件：</p>\n<ul>\n<li>当一个节点100%是一个类</li>\n<li>当分裂节点会导致树超过最大高度</li>\n<li>信息增益的值小于某个门槛值</li>\n<li>节点的样本数量小于某个门槛值</li>\n</ul>\n<p>实际上是一个递归的过程</p>\n<h3 id=\"独热编码-One-Hot-Encoding\"><a href=\"#独热编码-One-Hot-Encoding\" class=\"headerlink\" title=\"独热编码(One Hot Encoding)\"></a>独热编码(One Hot Encoding)</h3><p>实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0&#x2F;1）</p>\n<p>这样又转变为原来的左右分支分裂的情况</p>\n<h3 id=\"连续值特征\"><a href=\"#连续值特征\" class=\"headerlink\" title=\"连续值特征\"></a>连续值特征</h3><p>选定一个阈值，判断数据样本大于或者小于该阈值</p>\n<p>分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点</p>\n<p>对分割点分别计算信息增强来选择阈值</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>分裂时，改成尽量选取输出的方差(Variance)小的特征</p>\n<p>w还是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本），之后计算加权平均方差</p>\n<p>再用上一个节点所有数据的方差减去加权平均方差，选取最大的</p>\n<p>分类的结果是样本的平均值</p>\n<h2 id=\"使用多个决策树\"><a href=\"#使用多个决策树\" class=\"headerlink\" title=\"使用多个决策树\"></a>使用多个决策树</h2><p>单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮</p>\n<h3 id=\"放回抽样\"><a href=\"#放回抽样\" class=\"headerlink\" title=\"放回抽样\"></a>放回抽样</h3><p>从n个样本中放回地抽取n次，结果作为一个新的数据集</p>\n<h3 id=\"随机森林（Random-Forest）\"><a href=\"#随机森林（Random-Forest）\" class=\"headerlink\" title=\"随机森林（Random Forest）\"></a>随机森林（Random Forest）</h3><p>给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树</p>\n<p>b个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出</p>\n<p>对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k &lt; n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k &#x3D; \\sqrt{n}$</p>\n<h3 id=\"XGBoost（eXtreme-Gradient-Boosting）\"><a href=\"#XGBoost（eXtreme-Gradient-Boosting）\" class=\"headerlink\" title=\"XGBoost（eXtreme Gradient Boosting）\"></a>XGBoost（eXtreme Gradient Boosting）</h3><p>极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本</p>\n<p>思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分</p>\n<ul>\n<li>提升树的开源实现</li>\n<li>快速，有效</li>\n<li>很好的设定结束分裂的标准</li>\n<li>内置正则化</li>\n</ul>\n<h3 id=\"什么时候使用决策树\"><a href=\"#什么时候使用决策树\" class=\"headerlink\" title=\"什么时候使用决策树\"></a>什么时候使用决策树</h3><p>一个或多个决策树</p>\n<ul>\n<li>在表格化和结构化的数据上工作的很好</li>\n<li>不建议在非结构化的数据上，例如图片，音频，文本</li>\n<li>训练快速</li>\n<li>决策树是人类可以理解的（可解释性）</li>\n</ul>\n<p>神经网络</p>\n<ul>\n<li><p>对于所有类型的数据都能工作的很好</p>\n</li>\n<li><p>比决策树更慢</p>\n</li>\n<li><p>可以很好地使用迁移学习（预训练+微调）</p>\n</li>\n<li><p>当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）</p>\n</li>\n</ul>\n<h1 id=\"Course-3\"><a href=\"#Course-3\" class=\"headerlink\" title=\"Course 3\"></a>Course 3</h1><p>除了监督学习，机器学习还包括</p>\n<ul>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>异常检测</li>\n</ul>\n</li>\n<li>推荐系统</li>\n<li>强化学习</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><p>一堆数据点中自动查找相互关联或者相似的数据点</p>\n<h3 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means\"></a>K-means</h3><p>首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2… \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度</p>\n<ul>\n<li>将每个点分配给离他最近的中心点（centroid质心）</li>\n<li>将中心点移动到分配的点的平均中心</li>\n<li>重复前两步，直到中心点不再移动，K-means算法收敛</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Repeat&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to m</span><br><span class=\"line\">\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）</span><br><span class=\"line\">\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to k</span><br><span class=\"line\">\t\tu_k更新为分配的点的中心（每个轴的点的平均值）</span><br><span class=\"line\">\t\t如果簇中心点没有分配到点，就删除</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p><img src=\"/img/machine-learning-notes/pic-30.png\" alt=\"img\"></p>\n<p>$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）</p>\n<p>$u_k$是簇k</p>\n<p>$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇</p>\n<p>损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是<strong>欧几里得距离</strong></p>\n<p>也叫Distortion Function</p>\n<h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>选择$K&lt;m$</p>\n<p>随机选择K个训练样本，将$\\mu_1 ,\\mu_2… \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i = 1 to 100&#123;</span><br><span class=\"line\">\t随机初始化</span><br><span class=\"line\">\t获取c_i, u_i</span><br><span class=\"line\">\t计算损失函数J</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"选择簇的个数\"><a href=\"#选择簇的个数\" class=\"headerlink\" title=\"选择簇的个数\"></a>选择簇的个数</h3><p><strong>肘法（Elbow Method）</strong></p>\n<p>选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点</p>\n<p>所以K的选择还是按照之后的任务目的选择</p>\n<h2 id=\"异常检测\"><a href=\"#异常检测\" class=\"headerlink\" title=\"异常检测\"></a>异常检测</h2><h3 id=\"密度估计（Density-estimation）\"><a href=\"#密度估计（Density-estimation）\" class=\"headerlink\" title=\"密度估计（Density estimation）\"></a>密度估计（Density estimation）</h3><p>根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})&lt;\\epsilon$，认为出现了异常（anomaly）</p>\n<h3 id=\"高斯分布\"><a href=\"#高斯分布\" class=\"headerlink\" title=\"高斯分布\"></a>高斯分布</h3><p>Gaussian Distribution，也叫正态分布(Normal Distribution)</p>\n<p>$p(x)&#x3D;\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$</p>\n<p>其中$\\mu$是平均值，$\\sigma$是标准差</p>\n<p><img src=\"/img/machine-learning-notes/pic-31.png\" alt=\"img\"></p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>对于有多个特征的输入$\\vec{x}$，$\\vec{x} &#x3D; [x_1, x_2 … x_n]$</p>\n<p>$$<br>p(\\vec{x}) &#x3D; p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) <em>…</em> p(x_n;\\mu_n,\\sigma_n^2) &#x3D; \\prod_{j&#x3D;1}^np(x_j;\\mu_j,\\sigma_j^2)<br>$$</p>\n<h3 id=\"开发和评估异常检测系统\"><a href=\"#开发和评估异常检测系统\" class=\"headerlink\" title=\"开发和评估异常检测系统\"></a>开发和评估异常检测系统</h3><p>通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0&#x2F;1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集</p>\n<p><strong>流程：</strong></p>\n<p>在训练集$x_1…x_m$上拟合模型$p(x)$</p>\n<p>在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）</p>\n<p>之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$</p>\n<h3 id=\"权衡异常检测和监督学习\"><a href=\"#权衡异常检测和监督学习\" class=\"headerlink\" title=\"权衡异常检测和监督学习\"></a>权衡异常检测和监督学习</h3><p>异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样</p>\n<p>监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><p>监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要</p>\n<ul>\n<li>绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）</li>\n<li>检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取</li>\n</ul>\n<h2 id=\"推荐系统\"><a href=\"#推荐系统\" class=\"headerlink\" title=\"推荐系统\"></a>推荐系统</h2><p>$r(i,j) &#x3D; 1$表示用户j为电影i打分</p>\n<p>$y^{(i,j)}$表示用户j为电影i打的分</p>\n<p>$w^{(j)}, b^{(j)}$是用户j的参数</p>\n<p>$x^{(i)}$是电影i的特征向量</p>\n<p>对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$</p>\n<p>$m^{(j)}$表示用户j打分的电影数量</p>\n<p>通过训练学习$w^{(j)}, b^{(j)}$</p>\n<p>$$<br>\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)&#x3D;\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<p>对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},…,w^{(n_u)},b^{(n_u)}$</p>\n<p>$$<br>\\left.\\mathrm{J}\\left(<br>\\begin{array}<br>{cc}{w^{(1)},} &amp; {…,w^{(n_{u})}} \\<br>{b^{(1)},} &amp; {…,b^{(n_{u})}}<br>\\end{array}\\right.\\right)&#x3D;\\frac{1}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<h3 id=\"协同过滤算法\"><a href=\"#协同过滤算法\" class=\"headerlink\" title=\"协同过滤算法\"></a>协同过滤算法</h3><p>在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少</p>\n<p>$$<br>\\mathrm{J}(x^{(i)})&#x3D;\\frac{1}{2}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>学习得到$x^{(1)},x^{(2)},…,x^{(n_m)}$</p>\n<p>$$<br>\\mathrm{J}\\left(x^{(1)},x^{(2)},…,x^{(n_{m})}\\right)&#x3D;\\frac{1}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：</p>\n<p><img src=\"/img/machine-learning-notes/pic-32.png\" alt=\"img\"></p>\n<p>梯度下降时，w，b，x都是参数</p>\n<p><img src=\"/img/machine-learning-notes/pic-33.png\" alt=\"img\"></p>\n<p><a href=\"https://blog.csdn.net/zhu_xian_gang/article/details/130243870\">补充</a></p>\n<h3 id=\"二进制标签\"><a href=\"#二进制标签\" class=\"headerlink\" title=\"二进制标签\"></a>二进制标签</h3><p>1-用户看到物品之后参与点击，停留，添加喜欢，购买</p>\n<p>0-用户看到物品之后忽略</p>\n<p>?-用户没有看到物品</p>\n<p>预测$y^{(i,j)}&#x3D;1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-34.png\" alt=\"img\"></p>\n<h3 id=\"均值归一化\"><a href=\"#均值归一化\" class=\"headerlink\" title=\"均值归一化\"></a>均值归一化</h3><p><strong>Mean Normalization</strong></p>\n<ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n<p>求出每个电影的平均用户平方$\\mu_i$，构建向量$u$</p>\n<p>对于用户j，预测其在电影i的评分：</p>\n<p>$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$</p>\n<p>以至于不会当用户没有评分时认为评分接近0，而是接近平均值</p>\n<h3 id=\"查找相关项目\"><a href=\"#查找相关项目\" class=\"headerlink\" title=\"查找相关项目\"></a>查找相关项目</h3><p>对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似</p>\n<p>选取小的$\\sum_{l&#x3D;1}^n(x_l^{(k)} - x_l^{(i)})^2$</p>\n<p>也可以写作$||x^{(k)} - x^{(i)}||^2$</p>\n<h3 id=\"协同过滤算法的限制\"><a href=\"#协同过滤算法的限制\" class=\"headerlink\" title=\"协同过滤算法的限制\"></a>协同过滤算法的限制</h3><p><strong>冷启动问题</strong></p>\n<ul>\n<li>如何对没有什么用户打分的项目评分？</li>\n<li>如何对没有对很多项目打分的用户推荐一些项目？</li>\n</ul>\n<p><strong>没有很多信息的时候利用辅助信息</strong></p>\n<h3 id=\"基于内容的过滤算法\"><a href=\"#基于内容的过滤算法\" class=\"headerlink\" title=\"基于内容的过滤算法\"></a>基于内容的过滤算法</h3><p>协同过滤：基于用户的评分与你的评分的相似推荐项目</p>\n<p>基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目</p>\n<p>但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配</p>\n<p>对于v的获取，使用神经网络</p>\n<p>可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积</p>\n<p>也可以将两个网络合并，在内部进行点积输出结果</p>\n<p>$$<br>J&#x3D;\\sum_{(i,j):r(i,j)&#x3D;1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}<br>$$</p>\n<p>为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似</p>\n<h3 id=\"Retrieval-and-Ranking\"><a href=\"#Retrieval-and-Ranking\" class=\"headerlink\" title=\"Retrieval and Ranking\"></a>Retrieval and Ranking</h3><p>通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名</p>\n<p>检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看</p>\n<p>排名：对这些检索出的有限个项目进行学习，根据结果进行排名</p>\n<p>权衡检索的项目数量</p>\n<h2 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h2><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。</p>\n<p>涉及状态，行动，奖励，折扣系数，回报，策略</p>\n<h3 id=\"回报\"><a href=\"#回报\" class=\"headerlink\" title=\"回报\"></a>回报</h3><p>指的是系统获得的奖励总和</p>\n<p>折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+…$，直到终止状态</p>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>状态state通过策略π实行行动a</p>\n<p>$\\pi(s) &#x3D; a$，指明状态s情况下需要进行的决策a，从而最大化回报</p>\n<h3 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h3><p>Markov Decision Process(MDP)</p>\n<p><img src=\"/img/machine-learning-notes/pic-35.png\" alt=\"img\"></p>\n<h3 id=\"状态-动作价值函数\"><a href=\"#状态-动作价值函数\" class=\"headerlink\" title=\"状态-动作价值函数\"></a>状态-动作价值函数</h3><p>State-action value function，也叫Q-function,Q*,Optimal Q function</p>\n<p>$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报</p>\n<p>在状态s的最好回报就是$max_aQ(s,a)$</p>\n<p>在状态s的最好动作的就能够提供$max_aQ(s,a)$的</p>\n<h3 id=\"Bellman方程\"><a href=\"#Bellman方程\" class=\"headerlink\" title=\"Bellman方程\"></a>Bellman方程</h3><p>$s$:当前状态</p>\n<p>$a$:当前状态的决策</p>\n<p>$R(s)$:当前状态的奖励</p>\n<p>$s’$:采取动作a后的状态</p>\n<p>$a’$:在状态s’采取的动作</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>R(s)也叫即时奖励，表示你可以立刻得到的奖励</p>\n<p>后一项是从状态s’表现得最好获得的回报</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+… &#x3D; R_1 + \\gamma[R_2 + \\gamma R_3+…]$</p>\n<h3 id=\"随机环境\"><a href=\"#随机环境\" class=\"headerlink\" title=\"随机环境\"></a>随机环境</h3><p>由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励</p>\n<p>所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望</p>\n<p>$\\text{Return} &#x3D; \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+…) &#x3D; \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+…)$</p>\n<p>Bellman Equation变成：</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma \\text{E} [max_{a’}Q(s’,a’)]$</p>\n<h3 id=\"连续状态空间\"><a href=\"#连续状态空间\" class=\"headerlink\" title=\"连续状态空间\"></a>连续状态空间</h3><p>状态参数可能是连续的，比如坐标，角度，速度</p>\n<p>同时状态可能有多个，比如xyz坐标，速度等</p>\n<p>此时也叫连续状态马尔科夫决策过程</p>\n<h3 id=\"学习状态值函数\"><a href=\"#学习状态值函数\" class=\"headerlink\" title=\"学习状态值函数\"></a>学习状态值函数</h3><p><img src=\"/img/machine-learning-notes/pic-36.png\" alt=\"img\"></p>\n<p>以随机猜测$Q(s,a)$初始化神经网络</p>\n<p>重复：</p>\n<p>采取措施，得到$(s,a,R(s),s’)$元组</p>\n<p>存储最近的10k个 $(s,a,R(s),s’)$元组（Replay Buffer）</p>\n<p>训练网络：</p>\n<p>​\t创建10k个训练集，其中$x&#x3D;(s,a)$，$y &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$</p>\n<p>令$Q&#x3D;Q_{new}$</p>\n<p>虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计</p>\n<p><strong>改进</strong></p>\n<ul>\n<li>神经网络架构</li>\n</ul>\n<p>可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行</p>\n<p><img src=\"/img/machine-learning-notes/pic-37.png\" alt=\"img\"></p>\n<ul>\n<li>$\\epsilon$贪心策略</li>\n</ul>\n<p>当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。</p>\n<p>应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）</p>\n<p>小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。</p>\n<ul>\n<li>小批量$mini-batch$</li>\n</ul>\n<p>将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优</p>\n<p><img src=\"/img/machine-learning-notes/pic-38.png\" alt=\"img\"></p>\n<p>假设子集大小为1000；</p>\n<p>具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。</p>\n<p>在强化学习中，可以把10k的数据集分解训练多个模型</p>\n<ul>\n<li>软更新</li>\n</ul>\n<p>令$Q&#x3D;Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$</p>\n<p>而是<br>$$<br>w &#x3D; 0.01w_{new} + 0.99w<br>$$</p>\n<p>$$<br>b &#x3D; 0.01b_{new} + 0.99b<br>$$</p>\n<p>对参数进行微小调整</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}