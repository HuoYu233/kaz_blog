{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"themes/bamboo1/source/favicon.ico","path":"favicon.ico","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/css/animate.min.css","path":"css/animate.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/css/style.styl","path":"css/style.styl","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/activate-power-mode.js","path":"js/activate-power-mode.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/app.js","path":"js/app.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/goTop.js","path":"js/goTop.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/jquery3.5.1.js","path":"js/jquery3.5.1.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/local_search.js","path":"js/local_search.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/ribbon.min.js","path":"js/ribbon.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue2.6.11.js","path":"js/vue2.6.11.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/wrapImage.js","path":"js/wrapImage.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/medias/logo.png","path":"medias/logo.png","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/aplayer/APlayer@1.10.1.min.css","path":"js/aplayer/APlayer@1.10.1.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/bubble/bubble.js","path":"js/bubble/bubble.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/bubble/homeBubble.js","path":"js/bubble/homeBubble.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/clipboard/clipboard.min.js","path":"js/clipboard/clipboard.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/clicklove.js","path":"js/cursor/clicklove.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/explosion.min.js","path":"js/cursor/explosion.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/fireworks.js","path":"js/cursor/fireworks.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/cursor/text.js","path":"js/cursor/text.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/barrager.css","path":"js/danmu/barrager.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/close.png","path":"js/danmu/close.png","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/danmu/jquery.barrager.js","path":"js/danmu/jquery.barrager.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getPhotoOnline/index.js","path":"js/getPhotoOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/falling/sakura.js","path":"js/falling/sakura.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/falling/snow.js","path":"js/falling/snow.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getSiteOnline/index.js","path":"js/getSiteOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/issues/index.js","path":"js/issues/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/getTalkOnline/index.js","path":"js/getTalkOnline/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/pjax@0.2.8/index.js","path":"js/pjax@0.2.8/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-coy.min.css","path":"js/prism/prism-coy.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-dark.min.css","path":"js/prism/prism-dark.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-funky.min.css","path":"js/prism/prism-funky.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-line-numbers.css","path":"js/prism/prism-line-numbers.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-okaidia.min.css","path":"js/prism/prism-okaidia.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-solarizedlight.min.css","path":"js/prism/prism-solarizedlight.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-tomorrow.min.css","path":"js/prism/prism-tomorrow.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism-twilight.min.css","path":"js/prism/prism-twilight.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/prism/prism.min.css","path":"js/prism/prism.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/font.css","path":"js/shareJs/font.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/share.min.css","path":"js/shareJs/share.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/social-share.min.js","path":"js/shareJs/social-share.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper.animate1.0.3.min.js","path":"js/swiper/swiper.animate1.0.3.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper.min.js","path":"js/swiper/swiper.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/swiper@5.4.1.min.css","path":"js/swiper/swiper@5.4.1.min.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/swiper/vue-awesome-swiper.js","path":"js/swiper/vue-awesome-swiper.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.css","path":"js/tocbot/tocbot.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.min.js","path":"js/tocbot/tocbot.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/utils/index.js","path":"js/utils/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/valine/index.js","path":"js/valine/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-seamless-scroll/index.js","path":"js/vue-seamless-scroll/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.css","path":"js/vue-typed-js/index.css","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.js","path":"js/vue-typed-js/index.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/waline/waline.min.js","path":"js/waline/waline.min.js","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/medias/cursor/Horizontal.cur","path":"medias/cursor/Horizontal.cur","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.eot","path":"js/shareJs/fonts/iconfont.eot","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.svg","path":"js/shareJs/fonts/iconfont.svg","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.ttf","path":"js/shareJs/fonts/iconfont.ttf","modified":1,"renderable":1},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.woff","path":"js/shareJs/fonts/iconfont.woff","modified":1,"renderable":1},{"_id":"source/img/Kaz.jpg","path":"img/Kaz.jpg","modified":1,"renderable":0},{"_id":"source/img/bg.jpg","path":"img/bg.jpg","modified":1,"renderable":0},{"_id":"source/img/favicon.ico","path":"img/favicon.ico","modified":1,"renderable":0},{"_id":"source/img/ai-mianjing/pic-1.png","path":"img/ai-mianjing/pic-1.png","modified":1,"renderable":0},{"_id":"source/img/ai-mianjing/pic-2.png","path":"img/ai-mianjing/pic-2.png","modified":1,"renderable":0},{"_id":"source/img/ai-mianjing/pic-3.png","path":"img/ai-mianjing/pic-3.png","modified":1,"renderable":0},{"_id":"source/img/ai-mianjing/pic-4.png","path":"img/ai-mianjing/pic-4.png","modified":1,"renderable":0},{"_id":"source/img/ai-mianjing/pic-5.png","path":"img/ai-mianjing/pic-5.png","modified":1,"renderable":0},{"_id":"source/img/hello-world/Kaz.jpg","path":"img/hello-world/Kaz.jpg","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-1.png","path":"img/machine-learning-notes/pic-1.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-10.png","path":"img/machine-learning-notes/pic-10.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-11.png","path":"img/machine-learning-notes/pic-11.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-12.png","path":"img/machine-learning-notes/pic-12.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-13.png","path":"img/machine-learning-notes/pic-13.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-14.png","path":"img/machine-learning-notes/pic-14.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-15.png","path":"img/machine-learning-notes/pic-15.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-16.png","path":"img/machine-learning-notes/pic-16.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-17.png","path":"img/machine-learning-notes/pic-17.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-18.png","path":"img/machine-learning-notes/pic-18.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-19.png","path":"img/machine-learning-notes/pic-19.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-2.png","path":"img/machine-learning-notes/pic-2.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-20.png","path":"img/machine-learning-notes/pic-20.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-21.png","path":"img/machine-learning-notes/pic-21.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-22.png","path":"img/machine-learning-notes/pic-22.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-23.png","path":"img/machine-learning-notes/pic-23.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-24.png","path":"img/machine-learning-notes/pic-24.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-25.png","path":"img/machine-learning-notes/pic-25.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-26.png","path":"img/machine-learning-notes/pic-26.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-27.png","path":"img/machine-learning-notes/pic-27.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-28.png","path":"img/machine-learning-notes/pic-28.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-29.png","path":"img/machine-learning-notes/pic-29.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-3.png","path":"img/machine-learning-notes/pic-3.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-30.png","path":"img/machine-learning-notes/pic-30.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-31.png","path":"img/machine-learning-notes/pic-31.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-32.png","path":"img/machine-learning-notes/pic-32.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-33.png","path":"img/machine-learning-notes/pic-33.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-34.png","path":"img/machine-learning-notes/pic-34.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-35.png","path":"img/machine-learning-notes/pic-35.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-36.png","path":"img/machine-learning-notes/pic-36.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-37.png","path":"img/machine-learning-notes/pic-37.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-38.png","path":"img/machine-learning-notes/pic-38.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-4.png","path":"img/machine-learning-notes/pic-4.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-5.png","path":"img/machine-learning-notes/pic-5.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-6.png","path":"img/machine-learning-notes/pic-6.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-7.jpg","path":"img/machine-learning-notes/pic-7.jpg","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-8.png","path":"img/machine-learning-notes/pic-8.png","modified":1,"renderable":0},{"_id":"source/img/machine-learning-notes/pic-9.png","path":"img/machine-learning-notes/pic-9.png","modified":1,"renderable":0}],"Cache":[{"_id":"source/index.md","hash":"c22b5f9178342609428d6f51b2c5af4c0bde6a42","modified":1740671002749},{"_id":"source/_posts/ai-mianjing.md","hash":"935082913f229bd252d6891fbd7f1f8c3913edfa","modified":1739795309610},{"_id":"source/_posts/ai4chem.md","hash":"6cec9012e69a22d11dfde18d8b51da2f6b8043c1","modified":1740672859705},{"_id":"source/about/index.md","hash":"3082ba55ba80136ec54bce802397ca54156dffd7","modified":1740672919066},{"_id":"source/_posts/hello-world.md","hash":"662795e1685a6493db59dd5eafab94cdf24dd0ff","modified":1739795982971},{"_id":"source/_posts/algorithm-data-structure.md","hash":"7086b1498dae97bf727896740ec9c29317e08af8","modified":1740033940656},{"_id":"source/_posts/machine-learning-notes.md","hash":"40738afc5fdad8b470350a45e4bde4b0037f5ca3","modified":1740673137885},{"_id":"source/log/index.md","hash":"5deeb5b2f9716755f91cfcc3e6dda56bb45e951d","modified":1740672551623},{"_id":"source/img/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/img/ai-mianjing/pic-3.png","hash":"f0343f80af2588a206bc3e38195a0f78df6284c8","modified":1739456213996},{"_id":"source/img/ai-mianjing/pic-4.png","hash":"969c96fe6aa1ef57bb8ee346e0e816deabdac40b","modified":1739456245619},{"_id":"source/img/favicon.ico","hash":"a3a1b2c8f5bcd3faf1871d57d7923d6b45f35d34","modified":1737556566666},{"_id":"source/img/ai-mianjing/pic-5.png","hash":"15e53c748fb7d4bab9e358e7373db08bb37622ef","modified":1739456252616},{"_id":"source/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1739458454213},{"_id":"source/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1739622620308},{"_id":"source/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1738425420819},{"_id":"source/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1740055846274},{"_id":"source/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1738425452378},{"_id":"source/_posts/img/ai-mianjing/pic-3.png","hash":"f0343f80af2588a206bc3e38195a0f78df6284c8","modified":1739456213996},{"_id":"source/_posts/img/ai-mianjing/pic-5.png","hash":"15e53c748fb7d4bab9e358e7373db08bb37622ef","modified":1739456252616},{"_id":"source/_posts/img/ai-mianjing/pic-4.png","hash":"969c96fe6aa1ef57bb8ee346e0e816deabdac40b","modified":1739456245619},{"_id":"source/_posts/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1736256189325},{"_id":"source/_posts/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1739458454213},{"_id":"source/_posts/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1739622620308},{"_id":"source/_posts/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1738425420819},{"_id":"source/_posts/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1740055846274},{"_id":"source/_posts/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1738425452378},{"_id":"source/img/ai-mianjing/pic-1.png","hash":"8fc9a4161172a62c30fece73e605d4cf7efccd56","modified":1739456122134},{"_id":"source/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1739458598505},{"_id":"source/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1739461102564},{"_id":"source/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1739537460909},{"_id":"source/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1739619946353},{"_id":"source/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1740143318932},{"_id":"source/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1740144875402},{"_id":"source/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1738564459827},{"_id":"source/_posts/img/ai-mianjing/pic-1.png","hash":"8fc9a4161172a62c30fece73e605d4cf7efccd56","modified":1739456122134},{"_id":"source/_posts/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1739458598505},{"_id":"source/_posts/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1739461102564},{"_id":"source/_posts/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1739537460909},{"_id":"source/_posts/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1739619946353},{"_id":"source/_posts/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1740144875402},{"_id":"source/_posts/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1740143318932},{"_id":"source/_posts/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1738564459827},{"_id":"source/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1739453444035},{"_id":"source/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1739456947233},{"_id":"source/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1739458743567},{"_id":"source/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1739459181242},{"_id":"source/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1739535680777},{"_id":"source/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1739542099791},{"_id":"source/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1739624989401},{"_id":"source/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1739946176241},{"_id":"source/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1739888230194},{"_id":"source/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1739947190822},{"_id":"source/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1738910656446},{"_id":"source/_posts/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1739453444035},{"_id":"source/_posts/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1739456947233},{"_id":"source/_posts/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1739458743567},{"_id":"source/_posts/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1739459181242},{"_id":"source/_posts/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1739535680777},{"_id":"source/_posts/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1739542099791},{"_id":"source/_posts/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1739624989401},{"_id":"source/_posts/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1739946176241},{"_id":"source/_posts/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1739888230194},{"_id":"source/_posts/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1739947190822},{"_id":"source/_posts/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1738910656446},{"_id":"source/img/ai-mianjing/pic-2.png","hash":"ffadc84e916d5a876b13de026623a52858242d1a","modified":1739456153116},{"_id":"source/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1739449524427},{"_id":"source/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1737897790662},{"_id":"source/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1739541957085},{"_id":"themes/bamboo1/source/css/_tag/ghcard.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1740671571328},{"_id":"source/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1739624877518},{"_id":"source/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1739800893470},{"_id":"source/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1738564587991},{"_id":"source/_posts/img/ai-mianjing/pic-2.png","hash":"ffadc84e916d5a876b13de026623a52858242d1a","modified":1739456153116},{"_id":"source/_posts/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1739449524427},{"_id":"source/_posts/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1737897790662},{"_id":"source/_posts/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1739541957085},{"_id":"source/_posts/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1739624877518},{"_id":"source/_posts/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1739800893470},{"_id":"source/_posts/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1738564587991},{"_id":"source/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1739023761909},{"_id":"source/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1739453338824},{"_id":"source/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1739455810944},{"_id":"themes/bamboo1/package.json","hash":"1db58d63b57e63d88e0a060e5b164db225cf2599","modified":1740671571298},{"_id":"themes/bamboo1/LICENSE","hash":"2f9d4d3c41f055757f8c86567cfe838846446e7b","modified":1740671571262},{"_id":"themes/bamboo1/README.md","hash":"2b81d4346abf63bd58483420ea54220c8d1101b3","modified":1740671571262},{"_id":"themes/bamboo1/_config.yml","hash":"12b2d30c750c6d956262964d469bfeb43a19d54d","modified":1740672984347},{"_id":"themes/bamboo1/layout/archive.ejs","hash":"2703b07cc8ac64ae46d1d263f4653013c7e1666b","modified":1740671571295},{"_id":"themes/bamboo1/languages/zh-TW.yml","hash":"bd3ef201b7dcbeeee54107301550d60e71d72ba7","modified":1740671571264},{"_id":"themes/bamboo1/languages/zh-CN.yml","hash":"2bf66fefa219ee8152d35bb5f823ca5096fedcf2","modified":1740671571264},{"_id":"themes/bamboo1/languages/default.yml","hash":"4c604dc1344630ae5ab50edc282a3e46982884c1","modified":1740671571263},{"_id":"themes/bamboo1/layout/categories.ejs","hash":"b0f71816ca4c0899eb82b6fa100abf91b56508ab","modified":1740671571295},{"_id":"themes/bamboo1/layout/index.ejs","hash":"fea918c473fe66846b7a7f94da4617610cff3d07","modified":1740671571297},{"_id":"themes/bamboo1/layout/category.ejs","hash":"d0d19ac565414123c24b312f7158dbe1c9e275f8","modified":1740671571296},{"_id":"themes/bamboo1/layout/layout.ejs","hash":"a7dbe3f3f900e30c49f02b4d3a99803a4aba59ec","modified":1740671571297},{"_id":"themes/bamboo1/layout/post.ejs","hash":"206c60b92ec1ce6332e75e85761ce0c6947f5dae","modified":1740671571297},{"_id":"themes/bamboo1/layout/tag.ejs","hash":"1061f8a9b51d983590a3efc536142a9b10acebf5","modified":1740671571297},{"_id":"themes/bamboo1/layout/tags.ejs","hash":"0c6a171fa56cf8bfa180db32f10f75c6760fd983","modified":1740671571297},{"_id":"themes/bamboo1/source/favicon.ico","hash":"801ff7b3f358b77a813787a97ef59148eec93fd8","modified":1740671571332},{"_id":"themes/bamboo1/layout/_partial/archive.ejs","hash":"246967d57c31bd873f98603536df79c1f67c96e2","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/dark.ejs","hash":"774216ad95828bc349310b4cad02660018ca06e9","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/home_widget.ejs","hash":"0882e6992cfc2fbf1e0b05acbc1463de68f77aa4","modified":1740671571277},{"_id":"themes/bamboo1/layout/_partial/goTop.ejs","hash":"fa802c5b70f3ac8cf98dad040d05867891d7d4ff","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/lantern.ejs","hash":"225044aa82bf305e27a00adf1a8368146e860394","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/loaded.ejs","hash":"8e29d9924d2ee35c3da7c2f3e27652112252a185","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/notice.ejs","hash":"ed4cad963e1a9b747864bd3ceb76bf9e763c1150","modified":1740671571280},{"_id":"themes/bamboo1/layout/_partial/motto.ejs","hash":"535af08125435651591be103f8e6d98c7222907d","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/swiper.ejs","hash":"767c0fcae79bf94fa718ba26ea4ad1fdb610fe10","modified":1740671571295},{"_id":"themes/bamboo1/layout/_partial/paginator.ejs","hash":"26655627ce5b1eb7050b5e24cc262cf3fc46c400","modified":1740671571280},{"_id":"themes/bamboo1/layout/_partial/side.ejs","hash":"17f52c2fa6d771da94644d4fd0983d054a606384","modified":1740671571292},{"_id":"themes/bamboo1/scripts/events/index.js","hash":"514fb117a0c526de85c0338de7f66c23abc58b48","modified":1740671571298},{"_id":"themes/bamboo1/scripts/helpers/side_archives.js","hash":"a292f0a9e9242556b83219f519e3e92a4d85e904","modified":1740671571299},{"_id":"themes/bamboo1/scripts/tag/btn.js","hash":"0e628fa28e03f60e28f257af895b2e72a0cb8449","modified":1740671571300},{"_id":"themes/bamboo1/layout/_partial/topArticle.ejs","hash":"9b321c75dbcbc424b2392e90426127182539a86d","modified":1740671571295},{"_id":"themes/bamboo1/scripts/tag/btns.js","hash":"618e2f77ec244d8814f2b38c9820d1356580bbcd","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/checkbox.js","hash":"49da9041bd41c57a547d42fb7a6741757b848f1c","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/folding.js","hash":"832c55a45cfeeabcd2d317d42faaee09ee54d2a4","modified":1740671571301},{"_id":"themes/bamboo1/scripts/tag/file.js","hash":"260333b277073ba8f41472cdddb35ee3e8212267","modified":1740671571300},{"_id":"themes/bamboo1/scripts/tag/gallery.js","hash":"694a6a81dd3b3aa4a37e39b35402e99322941ec1","modified":1740671571301},{"_id":"themes/bamboo1/scripts/tag/getPhoto.js","hash":"e78765e6156ff261e564d8a22c6307ea98990a0d","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/getPhotoOnline.js","hash":"16478a1a0d642b92cc4f86114d185bf79cbd0bf9","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/getSiteOnline.js","hash":"2dce91bf40a1e6f856e317eba777fc29399ec2fc","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/ghcard.js","hash":"4e893d79abc1e8e1e5b3bfe08249ff32b250314d","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/image.js","hash":"faa1d83114bc255cffc18bd0ab037f08b430f515","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/inline-labels.js","hash":"eaaedc3d65384e0beb4306534ef4ed202b46da18","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/issues.js","hash":"7dcb40af462e4131f6a52d354ed3b147b4e874af","modified":1740671571303},{"_id":"themes/bamboo1/scripts/tag/link.js","hash":"a11fe06f20669f4b64a1a0dcc9f005a9f32e29dc","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/media.js","hash":"1d163ee349818baeb95504f82d3497da6f6556e2","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/mermaid.js","hash":"1e69a5e4a4a5f88fdb76d0fe55ea651c14301816","modified":1740671571304},{"_id":"themes/bamboo1/scripts/tag/note.js","hash":"9e990caa1fd815a760e31f1eaa02015d357fcef8","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/site.js","hash":"1cb487b1435925a55eaf957d761bc08254092c36","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/progress.js","hash":"99a10305e3924aaab05135ef25afd10d04574bfe","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/swiper.js","hash":"26a587371f7d2f6715cdb0e5f4f7b63a7f7921cd","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/span.js","hash":"d617b5a0056c4a0c983225513c89eed6f5b56833","modified":1740671571305},{"_id":"themes/bamboo1/scripts/tag/tabs.js","hash":"133310460bdf70a7932b44c3ccca509b3f221e1c","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/getTalkOnline.js","hash":"30f63677757c051835fec668ec928bcd47f6ba66","modified":1740671571302},{"_id":"themes/bamboo1/scripts/tag/timeline.js","hash":"da2b0d7760dea698429f370aba5cded5bb24501e","modified":1740671571306},{"_id":"themes/bamboo1/scripts/tag/title.js","hash":"8cfce58425366f805a5f2c88f01b76dca44f91ce","modified":1740671571306},{"_id":"themes/bamboo1/scripts/z-lazyload/index.js","hash":"58b935fb699a98f0a9ceb741d2105a977e24cf59","modified":1740671571307},{"_id":"themes/bamboo1/scripts/tag/titleB.js","hash":"3dde507bf20477cd89e71549be8ddfc4964a76ed","modified":1740671571307},{"_id":"themes/bamboo1/source/css/animate.min.css","hash":"dc47ce9b8438909921b14e766febdabf3018e3c2","modified":1740671571331},{"_id":"themes/bamboo1/source/css/style.styl","hash":"29848c643d866b6b3ae76bd7d4238c3ab7343618","modified":1740671571332},{"_id":"themes/bamboo1/source/js/activate-power-mode.js","hash":"2e14b0f48c55eaec543d96ec0eb2f16e80c20c01","modified":1740671571332},{"_id":"themes/bamboo1/source/js/app.js","hash":"57f824da5f893a0c83c80522114797f09868ccf1","modified":1740671571333},{"_id":"themes/bamboo1/source/js/goTop.js","hash":"ae548538475ddea2aae8949194935582cc0ae972","modified":1740671571340},{"_id":"themes/bamboo1/source/js/local_search.js","hash":"131d74198aa41bdb74dc27ef3ed856bc3d752f8d","modified":1740671571342},{"_id":"themes/bamboo1/source/js/ribbon.min.js","hash":"e6136a6243e04faca95844f47c21b070ade3661a","modified":1740671571345},{"_id":"themes/bamboo1/source/medias/logo.png","hash":"d08165f945567a08bd74d36b1241a0b8f1618536","modified":1740671571358},{"_id":"themes/bamboo1/source/js/wrapImage.js","hash":"08fa22ecfb8a93bdb96e1063e37367fcc97be29c","modified":1740671571357},{"_id":"themes/bamboo1/layout/_partial/analytics/baidu-analytics.ejs","hash":"589ac42358e8f2e4b22aac4353caa0c2c462a332","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/card/post.ejs","hash":"37fd6f4443620ff3b2963c19fc42bd21891428b0","modified":1740671571267},{"_id":"themes/bamboo1/layout/_partial/footer/busuanzi.ejs","hash":"bc3d2f6abc95b329dfe0186fa0364c48aab3772e","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/analytics/baidu-push.ejs","hash":"e7fcc44a10565505e85417ad4416d0d5d5839523","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/analytics/google-analytics.ejs","hash":"cb7d5c76508fe8db43dbd4af9a691398fffccadb","modified":1740671571265},{"_id":"themes/bamboo1/layout/_partial/footer/fish.ejs","hash":"c90e55153d7e83f5c054dd98add7dd9bbb6e095f","modified":1740671571275},{"_id":"themes/bamboo1/layout/_partial/footer/footer.ejs","hash":"3ed9b13b849c5a54dc667b9137beaa769d382980","modified":1740671571275},{"_id":"themes/bamboo1/layout/_partial/head/drawer.ejs","hash":"1f78c957b472a14c13301ea6a7ec59d5ecc777e6","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/head/head.ejs","hash":"aea1a32f47edda23b22b4e3444c13cedb4f3e5a6","modified":1740671571276},{"_id":"themes/bamboo1/layout/_partial/head/search.ejs","hash":"7cbf73c577874de0b6cc89180680b1e19c5e8348","modified":1740671571277},{"_id":"themes/bamboo1/layout/_partial/head/header.ejs","hash":"0583e814ef8af4cce35382a2b2634488e432de75","modified":1740671571277},{"_id":"themes/bamboo1/layout/_partial/math/mermaid.ejs","hash":"2c6894abc259167170e274728467c7c7aa1ef8e5","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/math/mathjax.ejs","hash":"dc9a1270d34448606e87e52a3b003a89f4f5b3aa","modified":1740671571278},{"_id":"themes/bamboo1/layout/_partial/meta/aplayer.ejs","hash":"33aeb95a90093ce66816676f9ac63f1f02d27852","modified":1740671571279},{"_id":"themes/bamboo1/layout/_partial/post/bgSwiper.ejs","hash":"77ade59920c57fda25c2be421b77adf7b0c943d9","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/pjax/animate.ejs","hash":"8a60b1b8ec8340f712ed6539bacba62d137232cc","modified":1740671571280},{"_id":"themes/bamboo1/layout/_partial/pjax/index.ejs","hash":"6ac774c816f9dcb7099612e2ef13bb0e7893476c","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/post/categories.ejs","hash":"15f33099ef5f653b9ceb3e27f089b36bff50cc4f","modified":1740671571281},{"_id":"themes/bamboo1/layout/_partial/post/comment.ejs","hash":"364875140582c1b702e01b63791f97011ca24b40","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/copyright.ejs","hash":"fd3af5c33895f907b1e5daa56d8d7266549dd019","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/donate.ejs","hash":"b54c1be4bf9a4b28a8c39d2835e8b4d9ee1e56ff","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/post-detail-header.ejs","hash":"34ded5a37233689e991bb7292dc785d626430106","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/post-nav.ejs","hash":"1e92a0ca46977f94ce27540ceb09ce05bc75accd","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_1.ejs","hash":"e82a5a888ba376080b21d4e39ac9b4fff2623d24","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/post/prismjs.ejs","hash":"198a472f69517829caf2f2cb542d32996a8fed74","modified":1740671571282},{"_id":"themes/bamboo1/layout/_partial/post/share.ejs","hash":"c414ae139680a2e2a50e776de4e137265fd0178d","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/post/tags.ejs","hash":"f92692427de2caa48033f975f193f9a8e4b02613","modified":1740671571283},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_2.ejs","hash":"d1df3a9050bae7cb7cf17d44359292999a1d0664","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_3.ejs","hash":"1ef0876d3a2f3ae43eccdf3c88dbd9c642d22d62","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_5.ejs","hash":"7db609c64eeabb8b68771097663d1ff427667e14","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_4.ejs","hash":"458f5c179670b029178c037f1feb99f566b9408e","modified":1740671571284},{"_id":"themes/bamboo1/layout/_partial/scripts/copy.ejs","hash":"fda84bf47a5e7c5692f682a45f8fdcfab90900b0","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_6.ejs","hash":"7ce6bd6bf765d23acb30f1a054e56f265099a4dd","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_8.ejs","hash":"2ab0a97498e0f35516c1db65949afd831a2800da","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/preLoader/loader_7.ejs","hash":"3b6d1da24786682b5d248f5e59f8f48510a55d4d","modified":1740671571285},{"_id":"themes/bamboo1/layout/_partial/scripts/cursor_effect.ejs","hash":"0877a20709f046f693e4536fe70354ad1b67f195","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/scripts/danmu.ejs","hash":"ca20ae64fbd2527f8b54fc8618a3a0502728420c","modified":1740671571287},{"_id":"themes/bamboo1/layout/_partial/scripts/falling.ejs","hash":"79d1c7cc0de290ac35e41312ef62fc4bf04be766","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/dark.ejs","hash":"25558a06394cb251a140b5a8998069f9f776f67e","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/getPhotoOnline.ejs","hash":"bd17b91d841055cef0fe7e4a70736f57a55e030b","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/getSiteOnline.ejs","hash":"7735a6b702c32447e4667cbc7cab86cd28d5badf","modified":1740671571288},{"_id":"themes/bamboo1/layout/_partial/scripts/getTalkOnline.ejs","hash":"5e5806d276dcfad3702d460d43c4ee826d9ce9ea","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/global.ejs","hash":"16851de5516d2908787ed902b778c8f03f9ae63b","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/head.ejs","hash":"4536885880315fb90efd75935c133051a729d45c","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/index.ejs","hash":"00a70fce835bc65801fd407236f72315e6df5934","modified":1740671571289},{"_id":"themes/bamboo1/layout/_partial/scripts/inputEffects.ejs","hash":"765c69436e021412dd8bbb852ea2403c97fc6adf","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/issues.ejs","hash":"cf5436f10fb9a2fb7238a1c528a5ed64a2345840","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/lazyload.ejs","hash":"b7e8071598a8dc70406aba8da949c5d6f5403e47","modified":1740671571290},{"_id":"themes/bamboo1/layout/_partial/scripts/scrollreveal.ejs","hash":"4aba6d5b506b3a8fc7d84abe0d42875e0107d64e","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/scripts/setHeader.ejs","hash":"c082c910282c3c96465b0812222874fbe87c58cd","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/scripts/swiperTag.ejs","hash":"020d6cd78cd9ace5477a79e57e958a2f5a6d43f3","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/scripts/toc.ejs","hash":"af49e146385d28f5cbdaa6d7a5167fbbde25a917","modified":1740671571291},{"_id":"themes/bamboo1/layout/_partial/scripts/typed.ejs","hash":"262725f946bb3ae8957b8e3d50c1a7cd564f8866","modified":1740671571292},{"_id":"themes/bamboo1/layout/_partial/side/sideHeader.ejs","hash":"3985bd41e1310500d44d42b02863d4b16815154b","modified":1740671571292},{"_id":"themes/bamboo1/layout/_partial/side/side_archives.ejs","hash":"c82b7c669aaaa2b9575cf6daf6d0a133b64acfd0","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_blogger.ejs","hash":"a2c637abf7943b273edfb69d0d8e0dc812c84127","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_category.ejs","hash":"dccd4537a94cf6e210803d6ab2789e6cd57d755e","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_recent_post.ejs","hash":"7fe61d14865e97988ff5d42047abb4a4ee8af7f0","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_tagcloud.ejs","hash":"7e716ce939cb8dd4004896adf41559d57bcfff0e","modified":1740671571293},{"_id":"themes/bamboo1/layout/_partial/side/side_toc.ejs","hash":"670c9c8bbc0f8cadfaa1e5caca73fd641d2fe831","modified":1740671571294},{"_id":"themes/bamboo1/layout/_partial/side/side_webinfo.ejs","hash":"1efe51b2685b7c4ad2ab42849c5ebe59e2d20def","modified":1740671571294},{"_id":"themes/bamboo1/layout/_partial/side/widget_library_sticky.ejs","hash":"e3929f7edf85900e7becc71e0cbe14da2333f621","modified":1740671571294},{"_id":"themes/bamboo1/scripts/events/lib/stellar-tag-utils.js","hash":"315d9e8a8261e760e1001970e09c32a660c969e0","modified":1740671571299},{"_id":"themes/bamboo1/scripts/z-lazyload/lib/process.js","hash":"48a29bdb7026c4a9c8a58190d044140a8a05a64c","modified":1740671571308},{"_id":"themes/bamboo1/source/css/_defines/variable.styl","hash":"ff4fff39224138f8cce1fe82dd8e0ab4077804b3","modified":1740671571308},{"_id":"themes/bamboo1/source/css/_partial/about.styl","hash":"0673f15fbb3649e221da3b20ba091d03bbd1cc3e","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/archive.styl","hash":"caf5c83ba9897644582e60e29770f1bb7362ad5a","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/base.styl","hash":"e3c5b4f828552be20a6f773f6a8c242d58a2dc23","modified":1740671571309},{"_id":"themes/bamboo1/source/css/_partial/categories.styl","hash":"ad6d70243be366677293ff88c2eec715d2c29e9e","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/category.styl","hash":"327d6d1f71d782f69fe0b365137b0abc331ca3bb","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/comment.styl","hash":"6d70e8ec7481d11558b430ed3bbf437805b42bc0","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/copyRyght.styl","hash":"ed1377ceb86204fa6b6c7430d14a1366d9ca568e","modified":1740671571310},{"_id":"themes/bamboo1/source/css/_partial/custom.styl","hash":"badd12c63cb9bb5f38c829f00be2509fb546e2cd","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/danmu.styl","hash":"8aaa764bb2b1c6a49c2f6c9ee868da24a0359669","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/dark.styl","hash":"28d5269fae1cbaec4f40c21daa0378c098c7d801","modified":1740671571311},{"_id":"themes/bamboo1/source/css/_partial/donate.styl","hash":"a880996ca61f96ba1280d581a132deb924c4ff62","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/drawer.styl","hash":"0498b0cf2819b681eeeec35193e491d1d039302d","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/footer.styl","hash":"598d193754645b22a0f1406303c1df66d95ffd9d","modified":1740671571312},{"_id":"themes/bamboo1/source/css/_partial/friends.styl","hash":"f7018f99210ccab74b2d315a55ba9c4350a12fc9","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/goTop.styl","hash":"08c3dc03570ca3738f18b99ebe95c79ec3d0ce0a","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/header.styl","hash":"db725d2648fc740aa59360502628e6de959504a6","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/home.styl","hash":"0bd1214d90fcf7ae020ee0673da4a9fd7c225274","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/highlight.styl","hash":"55fc39472aba296434fab0ffdc6be5684f01778a","modified":1740671571313},{"_id":"themes/bamboo1/source/css/_partial/motto.styl","hash":"cb484d25bc6f0bcda4cadffb5f1cdbe5df93919e","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/notice.styl","hash":"8fd57b791e518c14e88c36510e1132c10288b86b","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/pace.styl","hash":"b666b9079262d2dcc2a7b6023f97f79c8535db0e","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/paginator.styl","hash":"df1fd26976fd5be8418cd49a5c65ec651a680496","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/post-detail-header.styl","hash":"b316f4bcb9964bbfa4f6829e550578aa27d509a8","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/lantern.styl","hash":"04acde311d7b9f7a732340626dbe677814ab502f","modified":1740671571314},{"_id":"themes/bamboo1/source/css/_partial/post-nav.styl","hash":"a7c7b33ad813885af485815d8c787c0fb3b6c8c8","modified":1740671571315},{"_id":"themes/bamboo1/source/css/_partial/post.styl","hash":"32655204cccbf9861097efad65c1cf69dac11fd5","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/posts.styl","hash":"9464726d962229149d38841c087f6207cd8c2adc","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/search.styl","hash":"2f67103cd8cb9b92d1ca4f334e41c195e01c3ce3","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_partial/side.styl","hash":"2999120872bff96f84381f76ad0a65015bd3f549","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_partial/tags.styl","hash":"6fc3915d4a0f5d551b23f2281df868e0399bc13d","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_partial/tag.styl","hash":"f2b741dddc033f1989d3c4710f339ee122900e58","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_partial/topArticle.styl","hash":"910e24383b1009e27c0ebf26a5958051451da47c","modified":1740671571320},{"_id":"themes/bamboo1/source/css/_partial/transition.styl","hash":"809b40b7214cda6691b2f22ae827cbdbfaf8c303","modified":1740671571321},{"_id":"themes/bamboo1/source/css/_plugins/mathjax.styl","hash":"499f59db53e9c57d99bebe4722156aeca7adb8b7","modified":1740671571321},{"_id":"themes/bamboo1/source/css/_plugins/pjaxanimate.styl","hash":"f8c2d14c041bb87bc7f37d82ac939320e3d110bf","modified":1740671571321},{"_id":"themes/bamboo1/source/css/_tag/btn.styl","hash":"dbba1c1f7d374bd7c69c5b9758a61371db334d87","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_tag/checkbox.styl","hash":"dbc18a5685879493b06016c85993d4522fe48564","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_tag/circle.styl","hash":"c2adc73eab52952140420c2b5fc8bf134432b695","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_tag/folding.styl","hash":"7a88c350d302c6a89ab008b6ce2a98ed6f19c007","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/gallery.styl","hash":"440ae8b7cd2802e068a82e044c35a3273eb98668","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/galleryGroup.styl","hash":"ea9f387bc1bc00b4d6d4bd34e2df9046bda3610a","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/image.styl","hash":"ce0c9f758f0f0be385c38d65e9bf4fb708cbaf5c","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/inline-label.styl","hash":"1903a258c5829c8370c4eb53fcb60df7f7921f08","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/media.styl","hash":"6727008f95ad9b3146c609a2e890af009472f9e4","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/link.styl","hash":"7181435bed445840bb61d655451494f83ac4d7e9","modified":1740671571328},{"_id":"themes/bamboo1/source/css/_tag/note.styl","hash":"4487702c5348bf691e329fa8a9bbb6f42808436f","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/progress.styl","hash":"de1e1b08d23f95493ffda2a5375888e9e678891b","modified":1740671571329},{"_id":"themes/bamboo1/source/css/_tag/site-card.styl","hash":"ee95cbf6072dbe3ae11e6f73a3b38a9c09e31994","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/swiper.styl","hash":"ff02b78ba54cb71eafad141c3e4ef4a9cd9085cd","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/span.styl","hash":"bede49e1edf1049d4ea2f3dd0a17787fe084b2d2","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/tabs.styl","hash":"ca2dac222da40e13aa3b117d55b2da74d7ce9a35","modified":1740671571330},{"_id":"themes/bamboo1/source/css/_tag/talkByJson.styl","hash":"2affd0fe4a640ab92c07198cd4df13bef1ea1575","modified":1740671571331},{"_id":"themes/bamboo1/source/css/_tag/timeline.styl","hash":"ae8e4487a32606127d26dc27c74df592b2175f82","modified":1740671571331},{"_id":"themes/bamboo1/source/css/_tag/title.styl","hash":"e17ce9da2937d314c71b459c43de5f01441fe421","modified":1740671571331},{"_id":"themes/bamboo1/source/js/bubble/bubble.js","hash":"57f116efe2418a389913a46909e018fa4c9b9e84","modified":1740671571334},{"_id":"themes/bamboo1/source/js/aplayer/APlayer@1.10.1.min.css","hash":"7f4f8913f2d46ade2def5134e2cc8684a4b87939","modified":1740671571333},{"_id":"themes/bamboo1/source/js/clipboard/clipboard.min.js","hash":"76fd19c15b1d0a2d7afc7b66ca5f80c9061aabe2","modified":1740671571335},{"_id":"themes/bamboo1/source/js/cursor/clicklove.js","hash":"9e8e79d69ad8338761272f86fe5cad0ad5e503cc","modified":1740671571335},{"_id":"themes/bamboo1/source/js/cursor/explosion.min.js","hash":"ed2d0a5ad306a2745b7c8180b69e36b78d4b0698","modified":1740671571336},{"_id":"themes/bamboo1/source/js/bubble/homeBubble.js","hash":"8475e7ed2004b9791b3f7ad4162b7a2b89467874","modified":1740671571334},{"_id":"themes/bamboo1/source/js/cursor/fireworks.js","hash":"86ad9484e40268952b5e32c240fb04d0268f86dd","modified":1740671571336},{"_id":"themes/bamboo1/source/js/cursor/text.js","hash":"7dd898cb00b46ceda065c92f2ac092c4ef41b4e4","modified":1740671571336},{"_id":"themes/bamboo1/source/js/danmu/barrager.css","hash":"9de985f20d314f3f1182f30d1b0666e5eb9ca9b5","modified":1740671571337},{"_id":"themes/bamboo1/source/js/danmu/close.png","hash":"2c3ed4345f91dc1b74a57b6dcd1e1efa9e279dbb","modified":1740671571337},{"_id":"themes/bamboo1/source/js/getPhotoOnline/index.js","hash":"3b6354c11105aba544b08ded11295d83219d59ec","modified":1740671571339},{"_id":"themes/bamboo1/source/js/falling/sakura.js","hash":"b1566483a7d0deda2dd35db3d5a46f13aa5f1a86","modified":1740671571338},{"_id":"themes/bamboo1/source/js/getSiteOnline/index.js","hash":"733f75aff00e0a62013089cb3e869878d6fcc535","modified":1740671571339},{"_id":"themes/bamboo1/source/js/danmu/jquery.barrager.js","hash":"72ec0d8bbd0811973152fcbb316b0dd839ffb8f3","modified":1740671571337},{"_id":"themes/bamboo1/source/js/issues/index.js","hash":"f02538ab609541489396a682879ce854519487ca","modified":1740671571340},{"_id":"themes/bamboo1/source/js/falling/snow.js","hash":"99222d79ff36b05200b3ff7f54f8209d8f0a364b","modified":1740671571338},{"_id":"themes/bamboo1/source/js/getTalkOnline/index.js","hash":"ba714c7ffe4abde553d0c54ce5d528453f279c06","modified":1740671571340},{"_id":"themes/bamboo1/source/js/pjax@0.2.8/index.js","hash":"efb9166635c18f09f2c7604a8b15d6ac8aae4870","modified":1740671571342},{"_id":"themes/bamboo1/source/js/prism/prism-coy.min.css","hash":"fe1246de39c25eaa7ad1b0c997ee530dbdd39ad8","modified":1740671571343},{"_id":"themes/bamboo1/source/js/prism/prism-dark.min.css","hash":"a3f604a19e9a46f83a2fde49dfb45782748957ca","modified":1740671571343},{"_id":"themes/bamboo1/source/js/prism/prism-funky.min.css","hash":"0220f68ccda78c2b5d1109e58f3879674c93b587","modified":1740671571344},{"_id":"themes/bamboo1/source/js/prism/prism-line-numbers.css","hash":"c42732535ac61ac59a4356af3d89186a3071edf1","modified":1740671571344},{"_id":"themes/bamboo1/source/js/prism/prism-okaidia.min.css","hash":"50be6cc15d883ff3fa5d0885fed47241695a986c","modified":1740671571344},{"_id":"themes/bamboo1/source/js/prism/prism-solarizedlight.min.css","hash":"927b757cd8030d12953b5c0fa6eed5de15dda8ad","modified":1740671571344},{"_id":"themes/bamboo1/source/js/prism/prism-tomorrow.min.css","hash":"7b4247bc4d3b719afe5957779d0e5c8fb716c8ea","modified":1740671571345},{"_id":"themes/bamboo1/source/js/shareJs/font.css","hash":"f6407017418989fb0ced993509543fb07c6b0b33","modified":1740671571346},{"_id":"themes/bamboo1/source/js/prism/prism-twilight.min.css","hash":"ff4a6e3c4f1cb9bb59ec061656eacb750d238c15","modified":1740671571345},{"_id":"themes/bamboo1/source/js/prism/prism.min.css","hash":"aa405e2bcb571595c822a80f5482454c1536fa52","modified":1740671571345},{"_id":"themes/bamboo1/source/js/shareJs/share.min.css","hash":"9bd0cd6c81b60e10085cdda6aa724f147ee76599","modified":1740671571348},{"_id":"themes/bamboo1/source/js/shareJs/social-share.min.js","hash":"efdfa6b695ac6f0dd04cd8153d3e3a1a1edd90c2","modified":1740671571348},{"_id":"themes/bamboo1/source/js/swiper/swiper.animate1.0.3.min.js","hash":"6a8d6aa926e552a563356c36d52d1e0e0c83521e","modified":1740671571349},{"_id":"themes/bamboo1/source/js/swiper/swiper@5.4.1.min.css","hash":"fd618d2bdf929821d9fa70ae377b840ffc47d756","modified":1740671571350},{"_id":"themes/bamboo1/source/js/swiper/vue-awesome-swiper.js","hash":"e6f36537ed091a6b69945b1acf49e426426f1cf0","modified":1740671571350},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.css","hash":"45e469dffa7b9ebc03f99fd09fb97274cdc5e9b4","modified":1740671571350},{"_id":"themes/bamboo1/source/js/tocbot/tocbot.min.js","hash":"bc45d3586a21f7e364cd6efe58844932c00cf11c","modified":1740671571351},{"_id":"themes/bamboo1/source/js/utils/index.js","hash":"fcea598ed253006d79f78d34cc36fdc6649639f3","modified":1740671571351},{"_id":"themes/bamboo1/source/js/vue-seamless-scroll/index.js","hash":"f2aaf3f9b1ab7362f7cc158e5360cb1d62a57172","modified":1740671571353},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.css","hash":"b9dac4cfc5f0dc8854393d670b525fb63092fd38","modified":1740671571353},{"_id":"themes/bamboo1/source/js/vue-typed-js/index.js","hash":"c8e6f4510eb5fe55015401510ce03f5307556b1a","modified":1740671571354},{"_id":"themes/bamboo1/source/medias/cursor/Horizontal.cur","hash":"c3c5e8485a67b7ab16079a96b53aff7ff52de756","modified":1740671571357},{"_id":"themes/bamboo1/layout/_partial/comment/beaudar/layout.ejs","hash":"52b9a55b6e83bd9a10fc3f66a18be98e3965475b","modified":1740671571268},{"_id":"themes/bamboo1/layout/_partial/comment/beaudar/script.ejs","hash":"58b914569fbc9d5bf706674c1ea4d7a83b5540d1","modified":1740671571268},{"_id":"themes/bamboo1/layout/_partial/comment/changyan/layout.ejs","hash":"46192143a90303d8924b3d07d28df116bc833894","modified":1740671571268},{"_id":"themes/bamboo1/layout/_partial/comment/changyan/script.ejs","hash":"c357b9052564e12754cf21a1cb1debd3bdfe1eac","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/giscus/layout.ejs","hash":"a17970930b1064def6f5ad5f67a1afd3ed3169a0","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/gitalk/layout.ejs","hash":"8a4c57646ee0d4a4e94d568708fb85a8f9ac97e7","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/giscus/script.ejs","hash":"69ee0d2750ff779f754c8ffe6a5c44960420e2b7","modified":1740671571269},{"_id":"themes/bamboo1/layout/_partial/comment/gitalk/script.ejs","hash":"733947cad238d89c5a5694ecf19a83b1d5648ab9","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/livere/script.ejs","hash":"f545bc19b874f684bc4369ef1c6bfc4427b13b6b","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/gitment/script.ejs","hash":"1e02cf43a347a612796aa188446605e213e0dd51","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/gitment/layout.ejs","hash":"353820d6d6aade09cd21b31585afa20485008083","modified":1740671571270},{"_id":"themes/bamboo1/layout/_partial/comment/twikoo/layout.ejs","hash":"bdfd72b519f3bd8f3f78fd631a8326cbd0b20a98","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/twikoo/script.ejs","hash":"c09ef940a9d99d4e567e4891644241dd0ee40135","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/utterance/layout.ejs","hash":"eab867c580f6184d068d5fcc545a763a2919eb16","modified":1740671571272},{"_id":"themes/bamboo1/layout/_partial/comment/livere/layout.ejs","hash":"f88b32604056721e658c25f775866a1519e714f2","modified":1740671571271},{"_id":"themes/bamboo1/layout/_partial/comment/utterance/script.ejs","hash":"b8206894169b3e5ad1d38b448db5b9f1e4717987","modified":1740671571273},{"_id":"themes/bamboo1/layout/_partial/comment/valine/layout.ejs","hash":"453fa08c310cef7d00a12bb7cf448aef34c7728a","modified":1740671571273},{"_id":"themes/bamboo1/layout/_partial/comment/valine/script.ejs","hash":"c39459b0fb46bb1eb49823cc62375f04d3b4a48e","modified":1740671571273},{"_id":"themes/bamboo1/layout/_partial/comment/waline/layout.ejs","hash":"a82f1c7819cadca142b7f3436957ddce5adf7fa0","modified":1740671571274},{"_id":"themes/bamboo1/layout/_partial/comment/waline/script.ejs","hash":"ddfc35b05d22d2ad0f5b41444ed47793546f4b9d","modified":1740671571274},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_1.styl","hash":"ac20f1e2c9337396d590ceae03f9845b382ad534","modified":1740671571316},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_2.styl","hash":"112c765730edc8d143b503b8407046ab8deb3835","modified":1740671571317},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_3.styl","hash":"be27e1a9f2c0d78ab31f7a6b59341dc8c4393f88","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_4.styl","hash":"8dacae32f9ed4460546a75d313388b1b2497e097","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_6.styl","hash":"8a33ff2ed45cb2539743fd12b8ea4fc0d3873b98","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_5.styl","hash":"7e63625a1fc42f0403c3f933b9f642362f44ff46","modified":1740671571318},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_8.styl","hash":"1826bd092b9ea3a028d41cacc993927899406deb","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_1.styl","hash":"f7fc1257c6b402b1ddec85d45ac8e665580dc14d","modified":1740671571322},{"_id":"themes/bamboo1/source/css/_partial/preLoader/loader_7.styl","hash":"955a96e5829684f0021ef2ad6bdf42b928433ad1","modified":1740671571319},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_12.styl","hash":"27792c767fa345b0dbe735a681c87cf790e19a8b","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_10.styl","hash":"01bcf630c126f7fa273892245e6e6c59b654bf56","modified":1740671571323},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_11.styl","hash":"f7b6a4ab283029f649a0ae2732fa6e7079ecc435","modified":1740671571323},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_13.styl","hash":"a5015270d9d79fa2f4ed246939d48bf4c9c7f7f6","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_14.styl","hash":"0ad1c7d9faaf46bc201d8c7c9b34e39ae7efec48","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_3.styl","hash":"083991d97f004f1f657c7a7649bd7b319dee652e","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_2.styl","hash":"dbf766a7086bfb35a7fabc635edeb67a32d1828f","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_4.styl","hash":"ea0a0fbfb605d7d0592a06bb94e38f386830aa24","modified":1740671571325},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_6.styl","hash":"7f6b7d34933921dbabee0937053cb288fcad9647","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_5.styl","hash":"6fea986bd4c37188ce7da86b0839749ac188bd02","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_15.styl","hash":"fd9b1e87dfaed5db7d9d7b9dc272a5669056c278","modified":1740671571324},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_7.styl","hash":"d75280def0358da644945744da22f6a5f2abd745","modified":1740671571326},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_9.styl","hash":"6cdaa72cab01a2ca483eb7092372bcfcb2dc9b25","modified":1740671571327},{"_id":"themes/bamboo1/source/css/_tag/coolBtn/coolBtn_8.styl","hash":"4ce4323bc8a183533bbb1ab9ea2bf946350e5713","modified":1740671571326},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1740671571346},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.svg","hash":"337b4f156f6d8f4beb32c32a3db46fef361cff74","modified":1740671571347},{"_id":"source/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1739543974084},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1740671571347},{"_id":"themes/bamboo1/source/js/shareJs/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1740671571347},{"_id":"source/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1740307931753},{"_id":"source/_posts/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1739453338824},{"_id":"source/_posts/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1739023761909},{"_id":"source/_posts/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1739455810944},{"_id":"source/_posts/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1739543974084},{"_id":"source/_posts/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1740307931753},{"_id":"themes/bamboo1/source/js/jquery3.5.1.js","hash":"29fa5ad995e9ec866ece1d3d0b698fc556580eee","modified":1740671571341},{"_id":"source/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1738910330380},{"_id":"source/_posts/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1738910330380},{"_id":"themes/bamboo1/source/js/swiper/swiper.min.js","hash":"674fa0bd5973cc8124d6a711c725b119c025da0c","modified":1740671571349},{"_id":"themes/bamboo1/source/js/valine/index.js","hash":"d520897b1bd3788aacb672b5cd9ff7ab0c81fc80","modified":1740671571352},{"_id":"themes/bamboo1/source/js/waline/waline.min.js","hash":"3a17de5f24e0437c3c681b15f147ceef3980736f","modified":1740671571356},{"_id":"source/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1739945847505},{"_id":"source/_posts/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1739945847505},{"_id":"source/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1737530573994},{"_id":"source/_posts/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1737530573994},{"_id":"themes/bamboo1/source/js/vue2.6.11.js","hash":"1159f02f3f7191a5cf4c109734d0268173fab96d","modified":1740671571356},{"_id":"source/img/bg.jpg","hash":"91cb967d8e17bb6304049f800b236e59ae4cc752","modified":1736254693039},{"_id":"source/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1737530573974},{"_id":"source/_posts/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1737530573974},{"_id":"public/index.html","hash":"7cf843a81cd06dd74d5e343800db2204943d12fd","modified":1740708238994},{"_id":"public/2025/02/25/ai4chem/index.html","hash":"246e3180cb2da9f817be9ac70d468385d3790e83","modified":1740708238994},{"_id":"public/log/index.html","hash":"df99907fca2adc882c69e6d9022dc94cca59373e","modified":1740708238994},{"_id":"public/about/index.html","hash":"2ac5f12f28608148186215f1c916962ecd40b8c7","modified":1740708238994},{"_id":"public/2025/01/26/ai-mianjing/index.html","hash":"73ec90b36e86ac1e1f197d166d76ae7e679dfc94","modified":1740708238994},{"_id":"public/2023/11/25/machine-learning-notes/index.html","hash":"6a9c01e519e079d381b522ee15c77c7cf07765d5","modified":1740708238994},{"_id":"public/2023/07/07/algorithm-data-structure/index.html","hash":"eea3848acdd3cedb845e7d0efdfb7f53e1be195c","modified":1740708238994},{"_id":"public/2023/01/22/hello-world/index.html","hash":"ad5767bf628d2df21c6c85310bb5eabb60b5b35e","modified":1740708238994},{"_id":"public/archives/index.html","hash":"fc4ede388d2e75e92edc35f4e3256a173e9b44fe","modified":1740708238994},{"_id":"public/archives/2023/07/index.html","hash":"dfb7a09afa9d758bfd20d1b8c9569c5f75ebba47","modified":1740708238994},{"_id":"public/archives/2023/index.html","hash":"9742d3cc71132c48827bfa82a1a5870b31508f70","modified":1740708238994},{"_id":"public/archives/2023/01/index.html","hash":"6845e007453971ee8820832d24a8dd1fb0404d47","modified":1740708238994},{"_id":"public/archives/2023/11/index.html","hash":"13389ef9a9dac9126cd7b1814677ab7f19a18904","modified":1740708238994},{"_id":"public/archives/2025/index.html","hash":"4eda41c84d0542323178c6535002c23c832b1e67","modified":1740708238994},{"_id":"public/archives/2025/01/index.html","hash":"540e6cf453127d439d129c939d453394d9a5faf1","modified":1740708238994},{"_id":"public/archives/2025/02/index.html","hash":"0f60f375b3fddf05302c4d4c698d91de30606406","modified":1740708238994},{"_id":"public/js/danmu/close.png","hash":"2c3ed4345f91dc1b74a57b6dcd1e1efa9e279dbb","modified":1740708238994},{"_id":"public/medias/cursor/Horizontal.cur","hash":"c3c5e8485a67b7ab16079a96b53aff7ff52de756","modified":1740708238994},{"_id":"public/favicon.ico","hash":"801ff7b3f358b77a813787a97ef59148eec93fd8","modified":1740708238994},{"_id":"public/medias/logo.png","hash":"d08165f945567a08bd74d36b1241a0b8f1618536","modified":1740708238994},{"_id":"public/js/shareJs/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1740708238994},{"_id":"public/img/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1740708238994},{"_id":"public/js/shareJs/fonts/iconfont.svg","hash":"337b4f156f6d8f4beb32c32a3db46fef361cff74","modified":1740708238994},{"_id":"public/js/shareJs/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1740708238994},{"_id":"public/js/shareJs/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1740708238994},{"_id":"public/img/favicon.ico","hash":"a3a1b2c8f5bcd3faf1871d57d7923d6b45f35d34","modified":1740708238994},{"_id":"public/img/ai-mianjing/pic-3.png","hash":"f0343f80af2588a206bc3e38195a0f78df6284c8","modified":1740708238994},{"_id":"public/img/hello-world/Kaz.jpg","hash":"aa39b601ea6065577374034fd7a112d022ee3980","modified":1740708238994},{"_id":"public/img/ai-mianjing/pic-4.png","hash":"969c96fe6aa1ef57bb8ee346e0e816deabdac40b","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-16.png","hash":"5b3c81945f400550dfc7140e20b1a53d27a196c5","modified":1740708238994},{"_id":"public/img/ai-mianjing/pic-5.png","hash":"15e53c748fb7d4bab9e358e7373db08bb37622ef","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-27.png","hash":"d36f66e7a82d7f1b17fa05b12be35f45d1046928","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-3.png","hash":"7387a4682f742363b8c58677373d7549a9b9c1cf","modified":1740708238994},{"_id":"public/css/style.css","hash":"9d93cfbfc4026b96d8dd9115f24b44dec3643281","modified":1740708238994},{"_id":"public/js/activate-power-mode.js","hash":"3d02584da9dd820d1d9a454c5a93a2c37a8e4e42","modified":1740708238994},{"_id":"public/js/goTop.js","hash":"5bc7779f0d672c503a68b1e091fb3195df7e9815","modified":1740708238994},{"_id":"public/css/animate.min.css","hash":"8411c1c0418521c96d07bcca0d9dbce7e832ccc9","modified":1740708238994},{"_id":"public/js/app.js","hash":"38e8d7ce69449ee7fc28db92f6be88ae26e708b2","modified":1740708238994},{"_id":"public/js/local_search.js","hash":"475dc0727cb85c22f15f86701dd93c4bf449a438","modified":1740708238994},{"_id":"public/js/ribbon.min.js","hash":"3c8e4d717ca107f3723def1795c8ed62a5f1a8d0","modified":1740708238994},{"_id":"public/js/wrapImage.js","hash":"d29b7b5f24b1cbf342187096ee47ec29b5146e7c","modified":1740708238994},{"_id":"public/js/bubble/bubble.js","hash":"40cbc57f98407216ba6dc412e2b75e18c036240f","modified":1740708238994},{"_id":"public/js/aplayer/APlayer@1.10.1.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1740708238994},{"_id":"public/js/bubble/homeBubble.js","hash":"a8635136621c8c54c04462932192a94f314942cb","modified":1740708238994},{"_id":"public/js/clipboard/clipboard.min.js","hash":"6371ec0a8e242395c7d4d008d2b98e472c9dcc52","modified":1740708238994},{"_id":"public/js/jquery3.5.1.js","hash":"d2cc8d43ce1c854b1172e42b1209502ad563db83","modified":1740708238994},{"_id":"public/js/cursor/clicklove.js","hash":"9e8e79d69ad8338761272f86fe5cad0ad5e503cc","modified":1740708238994},{"_id":"public/js/cursor/explosion.min.js","hash":"ed2d0a5ad306a2745b7c8180b69e36b78d4b0698","modified":1740708238994},{"_id":"public/js/cursor/fireworks.js","hash":"6e1e9206549a6a1a4f5a8672a2dc5044a8f691bd","modified":1740708238994},{"_id":"public/js/danmu/barrager.css","hash":"3691efec6dd3d554b4a3dd20ef04836459f151a8","modified":1740708238994},{"_id":"public/js/cursor/text.js","hash":"a015017310e601f1e544cbc4b08c35b8e547c939","modified":1740708238994},{"_id":"public/js/danmu/jquery.barrager.js","hash":"305d6e93f3de102b5e1e9b1373821c849d8f54cb","modified":1740708238994},{"_id":"public/js/getPhotoOnline/index.js","hash":"f513605485600561123ffae1a70a0eb35cd5c675","modified":1740708238994},{"_id":"public/js/getSiteOnline/index.js","hash":"8b93e96331bbdcbee0deb33c9aeca6b2dceacb4b","modified":1740708238994},{"_id":"public/js/issues/index.js","hash":"e5f7b37f9dd8e966c7a63b8b6da27d53510eddeb","modified":1740708238994},{"_id":"public/js/falling/snow.js","hash":"6f4ef88304f874ef8bb8ea54f79b5d97f5a8f2f6","modified":1740708238994},{"_id":"public/js/getTalkOnline/index.js","hash":"58d9601cfd851c83c2eadd4803698171cd2d8b08","modified":1740708238994},{"_id":"public/js/prism/prism-coy.min.css","hash":"fe1246de39c25eaa7ad1b0c997ee530dbdd39ad8","modified":1740708238994},{"_id":"public/js/falling/sakura.js","hash":"ab41921e8f6ea1bedfcc348924574dc0caa20858","modified":1740708238994},{"_id":"public/js/prism/prism-dark.min.css","hash":"a3f604a19e9a46f83a2fde49dfb45782748957ca","modified":1740708238994},{"_id":"public/js/prism/prism-funky.min.css","hash":"0220f68ccda78c2b5d1109e58f3879674c93b587","modified":1740708238994},{"_id":"public/js/pjax@0.2.8/index.js","hash":"c9b1e349203e558dbe43665353e88c6eafc7dbcd","modified":1740708238994},{"_id":"public/js/vue2.6.11.js","hash":"e793aa33ef33150eaba3bc02b07455a231f053ad","modified":1740708238994},{"_id":"public/js/prism/prism-okaidia.min.css","hash":"50be6cc15d883ff3fa5d0885fed47241695a986c","modified":1740708238994},{"_id":"public/js/prism/prism-solarizedlight.min.css","hash":"927b757cd8030d12953b5c0fa6eed5de15dda8ad","modified":1740708238994},{"_id":"public/js/prism/prism-line-numbers.css","hash":"3b64b50b73729de943ec894c1d6f19115fa81624","modified":1740708238994},{"_id":"public/js/prism/prism-twilight.min.css","hash":"ff4a6e3c4f1cb9bb59ec061656eacb750d238c15","modified":1740708238994},{"_id":"public/js/prism/prism.min.css","hash":"aa405e2bcb571595c822a80f5482454c1536fa52","modified":1740708238994},{"_id":"public/js/swiper/swiper.animate1.0.3.min.js","hash":"0e48f180ca2f18b787e4b7b6e55ee3b0c6067691","modified":1740708238994},{"_id":"public/js/prism/prism-tomorrow.min.css","hash":"7b4247bc4d3b719afe5957779d0e5c8fb716c8ea","modified":1740708238994},{"_id":"public/js/shareJs/font.css","hash":"9d909397e4e94f696b7dd90a16481b50cf170362","modified":1740708238994},{"_id":"public/js/shareJs/share.min.css","hash":"573c7dddb465efd5f5a9337bd50a1ed3f8e82cff","modified":1740708238994},{"_id":"public/js/shareJs/social-share.min.js","hash":"efdfa6b695ac6f0dd04cd8153d3e3a1a1edd90c2","modified":1740708238994},{"_id":"public/js/tocbot/tocbot.css","hash":"45e469dffa7b9ebc03f99fd09fb97274cdc5e9b4","modified":1740708238994},{"_id":"public/js/swiper/swiper@5.4.1.min.css","hash":"de2263f82e7bf0778f31fd05c53000799f60701a","modified":1740708238994},{"_id":"public/js/utils/index.js","hash":"54c66b0a396cc3743884cdb979e5c400218613ce","modified":1740708238994},{"_id":"public/js/swiper/vue-awesome-swiper.js","hash":"b7a1ab21dfc58272009bfb5cb7ab87b79f5df573","modified":1740708238994},{"_id":"public/js/tocbot/tocbot.min.js","hash":"bc45d3586a21f7e364cd6efe58844932c00cf11c","modified":1740708238994},{"_id":"public/js/vue-typed-js/index.css","hash":"36a1d2f61d11ab328e349d6a523dd9dea2ec7ee1","modified":1740708238994},{"_id":"public/js/vue-seamless-scroll/index.js","hash":"f2aaf3f9b1ab7362f7cc158e5360cb1d62a57172","modified":1740708238994},{"_id":"public/js/vue-typed-js/index.js","hash":"0d80f25135de943ccdfdebec23275bd82712fae1","modified":1740708238994},{"_id":"public/js/swiper/swiper.min.js","hash":"a2fe3c0df9196597c283b2f6ffecc1d4d8702245","modified":1740708238994},{"_id":"public/js/valine/index.js","hash":"8809117760e0a7ce8dcc3f14b6421a4d415284a6","modified":1740708238994},{"_id":"public/js/waline/waline.min.js","hash":"94f70e622e2a1ab05adb205033a9ddf371c61534","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-35.png","hash":"ccc02508f63a657179c124d7637397ff9851d6d4","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-4.png","hash":"838d94327bb5086b731c649fdfb2d0ce109c71b0","modified":1740708238994},{"_id":"public/img/ai-mianjing/pic-1.png","hash":"8fc9a4161172a62c30fece73e605d4cf7efccd56","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-17.png","hash":"28493d22eede9861a75fe2a1d46390b78aa41ae5","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-20.png","hash":"724d8995131fbaedbf2f05b313862866b53c4e87","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-22.png","hash":"132aaa4f3e413125735dfd913e775837146ebde4","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-26.png","hash":"2409d57d65cb3b6583023569b8a5cfda1146ce59","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-36.png","hash":"b02febe362dc4d704c4a266fea335d83107f0bdf","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-37.png","hash":"037b77bcf91352c8ca863cc7da5fd8930850d7f8","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-6.png","hash":"3d1c7fe6c3783781ac279f471bd8db7f4a96f396","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-13.png","hash":"59de2b8eb3b6db9d4c564febdf7e3709130787a7","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-15.png","hash":"9f85209eae05833c0d6376d401f45e051023c4d2","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-18.png","hash":"1ad628440b4b5349cc6667348a33c9c97a012eff","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-19.png","hash":"7598a0992232dac91613e65e9dc4fec58dd1c56c","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-21.png","hash":"a8a3382060eab1be1c5a95e90fe7752721dbe801","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-24.png","hash":"40e00390e74ff9cf3aba708c1e9bec5724c396ca","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-29.png","hash":"b5bab6fddb0ad1c79c6d42362ac8a3a0319e6351","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-31.png","hash":"a15f390cb899248c679527434ec08a5e4b06d6f2","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-33.png","hash":"278a77be72fa2c34cad1ae3ac6d0a492665c78a1","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-34.png","hash":"d25dcd2ed03f72135c38a4901e961d5ddda92778","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-7.jpg","hash":"53a93542b958da7e52fe83aba886eb722dd26155","modified":1740708238994},{"_id":"public/img/ai-mianjing/pic-2.png","hash":"ffadc84e916d5a876b13de026623a52858242d1a","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-11.png","hash":"79e3c86dc4e6558bb5449ef6da54ed932bdf820a","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-2.png","hash":"51fbf8685ee1e3285b779d04995ba391196612bd","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-28.png","hash":"a1ac56f65a0f24d92dfac28adb18004ae483f468","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-30.png","hash":"ed3b3c733868949e51401c26cc8c5777ed3d3abf","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-23.png","hash":"5271104e246a1efa46822cf4b653a6dff3406865","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-5.png","hash":"f26891470930c14cd618f4fd5d1a702aabe5c9cb","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-10.png","hash":"3518538e91cc7fe18a169e78cbc072f68919e9d4","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-12.png","hash":"8a3f3196091822d6483ddee45b713833baa66540","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-14.png","hash":"c2edd1d1a4dcacf66c43b53b8e232b7ce944af4d","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-25.png","hash":"10b8539b73478749b91175ccff0001faafb34182","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-38.png","hash":"556b9b932485b4937b1c0dc77ee6d9a24fb25e07","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-8.png","hash":"e4d47e8c39f6ff05839c194ea682b2cbee16ae2f","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-32.png","hash":"a7f06d079e104b650620d7e152bcb9074d1066e2","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-9.png","hash":"6a99b1212556fc6513864610da7f9a379eb45b28","modified":1740708238994},{"_id":"public/img/bg.jpg","hash":"91cb967d8e17bb6304049f800b236e59ae4cc752","modified":1740708238994},{"_id":"public/img/machine-learning-notes/pic-1.png","hash":"c70db3a157a0fe618458426e13e6bd587f7f8397","modified":1740708238994}],"Category":[],"Data":[],"Page":[{"_content":"hi","source":"index.md","raw":"hi","date":"2025-02-27T15:44:29.264Z","updated":"2025-02-27T15:43:22.749Z","path":"index.html","title":"","comments":1,"layout":"page","_id":"cm7o4qpnv00002o99d3t9d2qz","content":"<p>hi</p>\n","excerpt":"","more":"<p>hi</p>\n"},{"title":"关于","type":"about","_content":"\n## 基础信息\n\n[[Email](mailto:1981270473@qq.com)] | [[Gitee](https://gitee.com/huoyu233)] | [[Github](https://github.com/HuoYu233)] | [[Bilibili](https://space.bilibili.com/82505737)]\n\n[福州大学](https://www.fzu.edu.cn/)环境工程本科在读\n\n- Basic Skills：CET-6、Java、C/C++、Python、JavaScript\n- Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis\n\n## 项目经历\n\n- [2023.09-2024.04] 面向智慧城市的群智感知平台[[Link](https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/)]\n- [2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[[Link](https://gitee.com/huoyu233/minyipin)]\n\n## 校园经历\n\n- [2023.07-2024.04] [福州大学人机共融智能课题组](https://www.fzu-urbansensing.com/)开发组成员\n- [2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员\n\n## 荣誉奖项\n\n**[Undergraduate]**\n\n- [2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛**一等奖**\n- [2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(**50%**)\n- [2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛**一等奖**\n- [2024.5] COMAP’s 2024 MCM/ICM Contest **Honorable Mention**\n- [2024.4] 第十五届蓝桥杯软件赛C/C++ **A组省三等奖**\n- [2024.4] 福州大学第八届网络信息安全竞赛**三等奖**\n- [2023.10] 第六届传智杯全国IT技能大赛优秀志愿者\n- [2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖\n\n**[High School]**\n\n- [2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖\n- [2019.10] 第十届全国青少年科学影像节三等奖\n- [2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖\n","source":"about/index.md","raw":"---\ntitle: 关于\ntype: about\n---\n\n## 基础信息\n\n[[Email](mailto:1981270473@qq.com)] | [[Gitee](https://gitee.com/huoyu233)] | [[Github](https://github.com/HuoYu233)] | [[Bilibili](https://space.bilibili.com/82505737)]\n\n[福州大学](https://www.fzu.edu.cn/)环境工程本科在读\n\n- Basic Skills：CET-6、Java、C/C++、Python、JavaScript\n- Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis\n\n## 项目经历\n\n- [2023.09-2024.04] 面向智慧城市的群智感知平台[[Link](https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/)]\n- [2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[[Link](https://gitee.com/huoyu233/minyipin)]\n\n## 校园经历\n\n- [2023.07-2024.04] [福州大学人机共融智能课题组](https://www.fzu-urbansensing.com/)开发组成员\n- [2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员\n\n## 荣誉奖项\n\n**[Undergraduate]**\n\n- [2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛**一等奖**\n- [2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(**50%**)\n- [2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛**一等奖**\n- [2024.5] COMAP’s 2024 MCM/ICM Contest **Honorable Mention**\n- [2024.4] 第十五届蓝桥杯软件赛C/C++ **A组省三等奖**\n- [2024.4] 福州大学第八届网络信息安全竞赛**三等奖**\n- [2023.10] 第六届传智杯全国IT技能大赛优秀志愿者\n- [2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖\n\n**[High School]**\n\n- [2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖\n- [2019.10] 第十届全国青少年科学影像节三等奖\n- [2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖\n","date":"2025-02-27T16:15:19.066Z","updated":"2025-02-27T16:15:19.066Z","path":"about/index.html","comments":1,"layout":"page","_id":"cm7o4qpnz00022o99dytxc7yc","content":"<h2 id=\"基础信息\"><a href=\"#基础信息\" class=\"headerlink\" title=\"基础信息\"></a>基础信息</h2><p>[<a href=\"mailto:1981270473@qq.com\">Email</a>] | [<a href=\"https://gitee.com/huoyu233\">Gitee</a>] | [<a href=\"https://github.com/HuoYu233\">Github</a>] | [<a href=\"https://space.bilibili.com/82505737\">Bilibili</a>]</p>\n<p><a href=\"https://www.fzu.edu.cn/\">福州大学</a>环境工程本科在读</p>\n<ul>\n<li>Basic Skills：CET-6、Java、C&#x2F;C++、Python、JavaScript</li>\n<li>Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis</li>\n</ul>\n<h2 id=\"项目经历\"><a href=\"#项目经历\" class=\"headerlink\" title=\"项目经历\"></a>项目经历</h2><ul>\n<li>[2023.09-2024.04] 面向智慧城市的群智感知平台[<a href=\"https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/\">Link</a>]</li>\n<li>[2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[<a href=\"https://gitee.com/huoyu233/minyipin\">Link</a>]</li>\n</ul>\n<h2 id=\"校园经历\"><a href=\"#校园经历\" class=\"headerlink\" title=\"校园经历\"></a>校园经历</h2><ul>\n<li>[2023.07-2024.04] <a href=\"https://www.fzu-urbansensing.com/\">福州大学人机共融智能课题组</a>开发组成员</li>\n<li>[2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员</li>\n</ul>\n<h2 id=\"荣誉奖项\"><a href=\"#荣誉奖项\" class=\"headerlink\" title=\"荣誉奖项\"></a>荣誉奖项</h2><p><strong>[Undergraduate]</strong></p>\n<ul>\n<li>[2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛<strong>一等奖</strong></li>\n<li>[2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(<strong>50%</strong>)</li>\n<li>[2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛<strong>一等奖</strong></li>\n<li>[2024.5] COMAP’s 2024 MCM&#x2F;ICM Contest <strong>Honorable Mention</strong></li>\n<li>[2024.4] 第十五届蓝桥杯软件赛C&#x2F;C++ <strong>A组省三等奖</strong></li>\n<li>[2024.4] 福州大学第八届网络信息安全竞赛<strong>三等奖</strong></li>\n<li>[2023.10] 第六届传智杯全国IT技能大赛优秀志愿者</li>\n<li>[2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖</li>\n</ul>\n<p><strong>[High School]</strong></p>\n<ul>\n<li>[2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖</li>\n<li>[2019.10] 第十届全国青少年科学影像节三等奖</li>\n<li>[2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖</li>\n</ul>\n","excerpt":"","more":"<h2 id=\"基础信息\"><a href=\"#基础信息\" class=\"headerlink\" title=\"基础信息\"></a>基础信息</h2><p>[<a href=\"mailto:1981270473@qq.com\">Email</a>] | [<a href=\"https://gitee.com/huoyu233\">Gitee</a>] | [<a href=\"https://github.com/HuoYu233\">Github</a>] | [<a href=\"https://space.bilibili.com/82505737\">Bilibili</a>]</p>\n<p><a href=\"https://www.fzu.edu.cn/\">福州大学</a>环境工程本科在读</p>\n<ul>\n<li>Basic Skills：CET-6、Java、C&#x2F;C++、Python、JavaScript</li>\n<li>Frameworks：Pytorch、Mybatis、Springboot、Vue、Uniapp、Redis</li>\n</ul>\n<h2 id=\"项目经历\"><a href=\"#项目经历\" class=\"headerlink\" title=\"项目经历\"></a>项目经历</h2><ul>\n<li>[2023.09-2024.04] 面向智慧城市的群智感知平台[<a href=\"https://www.fzu-urbansensing.com/Platforms-Applications/Crowdsensing_Platform/\">Link</a>]</li>\n<li>[2023.04-2023.06] 福建乡村本地招聘信息工具——闽易聘[<a href=\"https://gitee.com/huoyu233/minyipin\">Link</a>]</li>\n</ul>\n<h2 id=\"校园经历\"><a href=\"#校园经历\" class=\"headerlink\" title=\"校园经历\"></a>校园经历</h2><ul>\n<li>[2023.07-2024.04] <a href=\"https://www.fzu-urbansensing.com/\">福州大学人机共融智能课题组</a>开发组成员</li>\n<li>[2021.09-2022.06] 福州大学环境与安全工程学院网络宣传部门成员</li>\n</ul>\n<h2 id=\"荣誉奖项\"><a href=\"#荣誉奖项\" class=\"headerlink\" title=\"荣誉奖项\"></a>荣誉奖项</h2><p><strong>[Undergraduate]</strong></p>\n<ul>\n<li>[2024.12] 第六届全国高校计算机能力挑战赛C++程序设计国赛<strong>一等奖</strong></li>\n<li>[2024.12] 福州大学环安学院2023~2024学年下学期学习进步奖(<strong>50%</strong>)</li>\n<li>[2024.7] 第十六届“中国电机工程学会杯”全国大学生电工数学建模竞赛<strong>一等奖</strong></li>\n<li>[2024.5] COMAP’s 2024 MCM&#x2F;ICM Contest <strong>Honorable Mention</strong></li>\n<li>[2024.4] 第十五届蓝桥杯软件赛C&#x2F;C++ <strong>A组省三等奖</strong></li>\n<li>[2024.4] 福州大学第八届网络信息安全竞赛<strong>三等奖</strong></li>\n<li>[2023.10] 第六届传智杯全国IT技能大赛优秀志愿者</li>\n<li>[2023.4] 福州大学环安学院2022~2023学年上学期精神文明建设奖</li>\n</ul>\n<p><strong>[High School]</strong></p>\n<ul>\n<li>[2021.4] 龙岩市学生信息素养提升实践活动数字创作项目一等奖</li>\n<li>[2019.10] 第十届全国青少年科学影像节三等奖</li>\n<li>[2019.6] “外研社杯”全国中学生外语素养大赛福建赛区二等奖</li>\n</ul>\n"},{"title":"日志","type":"log","_content":"- 2025.2.28 使用`netlify`加速网站\n- 2025.1.24 主题切换为`bamboo`\n- 2025.1.22 数据误删重建 主题采用`coder`\n- 2024.1.22 绑定域名 `kazovo.cn`\n- 2024.1.22 删除目录`tools`\n- 2023.11.10 修改主题为`mashiro`\n- 2023.10.12 修改大量文章内容\n- 2023.3.16 更新日志移动至新页面`log`下\n- 2023.3.15 修改主题为 `Kaze`\n- 2023.2.17 新增`tools`页面\n- 2023.2.17 同步多篇本地文章\n- 2023.2.17 绑定域名 `hawyior.top`\n- 2023.2.17 博客基于`Hexo`搭建框架完毕，主题采用`cactus`\n","source":"log/index.md","raw":"---\ntitle: 日志\ntype: log\n---\n- 2025.2.28 使用`netlify`加速网站\n- 2025.1.24 主题切换为`bamboo`\n- 2025.1.22 数据误删重建 主题采用`coder`\n- 2024.1.22 绑定域名 `kazovo.cn`\n- 2024.1.22 删除目录`tools`\n- 2023.11.10 修改主题为`mashiro`\n- 2023.10.12 修改大量文章内容\n- 2023.3.16 更新日志移动至新页面`log`下\n- 2023.3.15 修改主题为 `Kaze`\n- 2023.2.17 新增`tools`页面\n- 2023.2.17 同步多篇本地文章\n- 2023.2.17 绑定域名 `hawyior.top`\n- 2023.2.17 博客基于`Hexo`搭建框架完毕，主题采用`cactus`\n","date":"2025-02-27T16:09:11.623Z","updated":"2025-02-27T16:09:11.623Z","path":"log/index.html","comments":1,"layout":"page","_id":"cm7o4qpo000042o99d68xfhqu","content":"<ul>\n<li>2025.2.28 使用<code>netlify</code>加速网站</li>\n<li>2025.1.24 主题切换为<code>bamboo</code></li>\n<li>2025.1.22 数据误删重建 主题采用<code>coder</code></li>\n<li>2024.1.22 绑定域名 <code>kazovo.cn</code></li>\n<li>2024.1.22 删除目录<code>tools</code></li>\n<li>2023.11.10 修改主题为<code>mashiro</code></li>\n<li>2023.10.12 修改大量文章内容</li>\n<li>2023.3.16 更新日志移动至新页面<code>log</code>下</li>\n<li>2023.3.15 修改主题为 <code>Kaze</code></li>\n<li>2023.2.17 新增<code>tools</code>页面</li>\n<li>2023.2.17 同步多篇本地文章</li>\n<li>2023.2.17 绑定域名 <code>hawyior.top</code></li>\n<li>2023.2.17 博客基于<code>Hexo</code>搭建框架完毕，主题采用<code>cactus</code></li>\n</ul>\n","excerpt":"","more":"<ul>\n<li>2025.2.28 使用<code>netlify</code>加速网站</li>\n<li>2025.1.24 主题切换为<code>bamboo</code></li>\n<li>2025.1.22 数据误删重建 主题采用<code>coder</code></li>\n<li>2024.1.22 绑定域名 <code>kazovo.cn</code></li>\n<li>2024.1.22 删除目录<code>tools</code></li>\n<li>2023.11.10 修改主题为<code>mashiro</code></li>\n<li>2023.10.12 修改大量文章内容</li>\n<li>2023.3.16 更新日志移动至新页面<code>log</code>下</li>\n<li>2023.3.15 修改主题为 <code>Kaze</code></li>\n<li>2023.2.17 新增<code>tools</code>页面</li>\n<li>2023.2.17 同步多篇本地文章</li>\n<li>2023.2.17 绑定域名 <code>hawyior.top</code></li>\n<li>2023.2.17 博客基于<code>Hexo</code>搭建框架完毕，主题采用<code>cactus</code></li>\n</ul>\n"}],"Post":[{"title":"AI面经","mathjax":true,"date":"2025-01-26T12:46:25.000Z","img":"https://img1.baidu.com/it/u=4282277671,2501328998&fm=253&fmt=auto&app=138&f=JPEG?w=449&h=252","excerpt":"人工智能经典面试问题","_content":"\n### 1 机器学习、深度学习、与人工智能对比\n\n- **机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术**\n\n> **人工智能**（人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器）\n>\n> **分类**： 弱人工智能（当前）、强人工智能、超人工智能\n>\n> **机器学习**（MachineLearning）简称ML：机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。\n>\n> **深度学习**（DeepLearning）简称DL。**最初**的深度学习是**利用深度神经网络**来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于**建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本**。\n\n#### 1.1 人工智能\n\n> **人工智能（Artificial intelligence）简称AI**。人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n>\n> - 人工智能目前分为弱人工智能和强人工智能和超人工智能：\n> - **弱人工智能**：弱人工智能（Artificial Narrow Intelligence /ANI),只专注于完成某个特定的任务，例如语音识别、图象识别和翻译等，是擅长于单个方面的人工智能。它们只是用于解决特定的具体类的任务问题而存在，大都是统计数据，以此从中归纳出模型。由于弱人工智能智能处理较为单一的问题，且发展程度并没有达到模拟人脑思维的程度，所以弱人工智能仍然属于“工具”的范畴，与传统的“产品”在本质上并无区别。\n>\n> - **强人工智能**：强人工智能（Artificial General lnteligence /AGI),属于人类级别的人工智能，在各方面都能和人类比肩，它能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作，并且和人类一样得心应手。\n>\n> - **超人工智能**：超人工智能（Artificial Superintelligence/ASI），在几乎所有领域都比最聪明的人类大脑都聪明许多，包括科学创新、通识和社交技能。在超人工智能阶段，人工智能已经跨过“奇点”，其计算和思维能力已经远超人脑。此时的人工智能已经不是人类可以理解和想象。人工智能将打破人脑受到的维度限制，其所观察和思考的内容，人脑已经无法理解，人工智能将形成一个新的社会。\n>\n> **目前我们仍处于弱人工智能阶段**。\n\n#### 1.2 机器学习\n\n> **机器学习（Machine Learning）简称ML**。机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。\n\n#### 1.3 深度学习\n\n> **深度学习（Deep Learning）简称DL**。最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本。\n>\n> 注意：你可能在接触深度学习的时候也听到过**监督学习**、**非监督学习**、**半监督学习**等概念，下面就顺便对这三个名词解析下：\n>\n> 1. **监督学习：**用一部分已知分类、有标记的样本来训练机器后，让它用学到的特征，对没有还分类、无标记的样本进行分类、贴标签。多用于分类。\n> 2. **非监督学习：**所有的数据没有标记，类别未知，让它自己学习样本之间的相似性来进行分类。多用于聚类。\n> 3. **半监督学习：**有两个样本集，一个有标记，一个没有标记。综合利用有类标的样本（ labeled sample）和没有类标的样本（ unlabeled sample），来生成合适的分类。\n\n#### 1.4 区别与联系\n\n![img](\\img\\ai-mianjing\\pic-1.png)\n\n- 人工智能的研究领域包括 **专家系统**(Expert Systems)、**机器学习**(Machine Learning)、**模式识别**、**进化计算**(Evolutionary Computation)、**模糊逻辑**(Fussy Logic)、**计算机视觉**(Computer Vision)、**自然语言处理**(NLP)、**推荐系统**(Recommender Systems)\n- 深度学习，一种**实现机器学习**的技术。是机器学习中的一个方法或者分支。\n- **深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术**。\n- 它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和[自然语言处理](https://so.csdn.net/so/search?q=自然语言处理&spm=1001.2101.3001.7020)(NLP)领域。显然，**“深度学习”是与机器学习中的“神经网络”是强相关**，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。\n- **总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU**\n\n![img](\\img\\ai-mianjing\\pic-2.png)\n\n### 2 AI的分类\n\n- **模式识别**：\n\n  - **模式识别是指用计算机对物体进行识别。**\n\n    **物体指文字、符号、图形、图像、语音、声音等实体对象；**\n\n    •**不包括概念、思想、意识等虚拟对象，它们属于认知和哲学研究范畴。**\n\n  - **模式识别应用：**\n\n    • **文字识别，车牌识别，人脸识别，视网膜识别，指纹识别，掌纹识别等**\n\n- **自然语言处理**：用自然语言同计算机进行通讯的一种技术\n\n  - 机器翻译\n  - 语音识别（**可以单列**）\n  - 文本分类/预测\n\n- **机器学习**（重点是深度学习）：机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法\n\n- **计算机视觉**：用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像\n\n- **智能机器人**：给机器人装上“大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信\n\n  息的综合处理等方面将会前进一大步\n\n- **自动程序设计**：自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序\n\n- **数据挖掘**：从大量的数据中搜索隐藏在其中信息的过程\n\n- **专家系统**等\n\n### 3 一些深度学习概念\n\n#### **监督学习基础定义**：\n\n**有监督学习 (Supervised Learning)**: 使用**已经标注的数据**训练模型，目的是让模型学会如何将输入映射到期望的输出。例如，使用一组已知类别的图片来训练一个图像**分类模型**，模型学习如何根据输入的图片预测正确的类别 。\n\n**无监督学习 (Unsupervised Learning)**: **不使用任何标注数据**，让模型自己发现数据中的结构和模式。例如，**聚类**算法可以将数据分为几个不同的组，而不事先告诉模型应该有多少组，或者各组的特征 。\n\n**半监督学习 (Semi-Supervised Learning)**: 结合**有监督和无监督学习**的方法，使用**少量标注数据和大量未标注数据**共同训练模型。这种方法在标注数据难以获得但未标注数据丰富的场景下特别有用 。 例如：**医学影像**，大部分没有标签，人工打标较为昂贵。可以用\n\n- 使用已标注数据训练一个分类器。\n- 用该分类器对未标注数据进行预测，生成伪标签。\n- 选择置信度最高的伪标签来扩充训练集。\n- 结合原始的标注数据和带有伪标签的数据重新训练分类器\n\n**自监督学习 (Self-Supervised Learning)**: 一种特殊的无监督学习方法，通过**自动生成伪标签**来训练模型。这种方法通过创建预测任务（如预测未来的帧、填充缺失的部分、一个自监督学习模型可以通过尝试预测句子中缺失的单词来学习语言的语法和词汇等），使模型能够从输入数据本身学习有用的表示 。 通常是**预训练+下游任务**的方式。\n\n**对比**：\n\n> - **有监督学习**通常需要大量的标注数据，适用于标注数据充足的任务。\n> - **无监督学习**不需要标签，适用于探索数据内在结构和关系的任务。\n> - **半监督学习**结合了有监督和无监督的优点，适用于标注数据有限但无标签数据多的情况。\n> - **自监督学习**则是一种新兴的方法，它能够从未标注的数据中学习到有用的特征表示，适用于标注数据难以获得但需要丰富特征表示的任务。\n\n**半监督学习举例**\n\n- 使用已标注数据训练一个分类器。\n- 用该分类器对未标注数据进行预测，生成伪标签。\n- 选择置信度最高的伪标签来扩充训练集。\n- 结合原始的标注数据和带有伪标签的数据重新训练分类器\n\n#### **对比预训练**：\n\n> 对比预训练是一种自监督学习方法，其核心目标是通过学习**区分不同数据之间的相似性和差异性来提升模型的特征表示能力**。这种方法不依赖于人工标注的数据，而是通过**定义一种或多种对比任务**（比如区分不同图像、文本或声音等），使得模型能够自动学习到数据的**内在特征和结构**。对比预训练通过比较正样本对（相似）和负样本对（不相似）来优化模型，使模型能够捕捉到数据中有用的信息和模式，从而在没有大量标注数据的情况下也能有效提升模型性能 。\n>\n> **在NLP和CV等领域，对比学习已经成为一种重要的自监督学习方法**，**通过自监督预训练模式**，模型可以从数据本身的先验知识分布中吸取图像或文本的特征，得到一个能够更好地适应不同任务和领域的预训练模 。这种方法的优势在于**无需大量标注数据**，同时可以**显著提高模型的泛化性能和学习能力** 。\n\n#### **平行语料库**：\n\n> **平行语料库是收录了某一源语言文本及其对应的目标语文本的语料库**。具体来说，平行语料库包括以下内容：\n>\n> - **源语言和目标语言的对应文本**：这些文本在语义上是相同的，只是用不同的语言表达，如英文原版和中文翻译版。\n> - **对齐的层面**：平行对齐指的是源语文本和目标语文本之间的具体单位的对应关系或翻译关系，可以细分为词汇、语句和段落等层面的对齐。\n> - **类型**：根据所涉及的语种数量和方向，平行语料库可以分为单向平行语料库、双向平行语料库和多向平行语料库。\n> - **应用**：平行语料库对于语言对比、双语词典编纂、机器翻译、翻译策略与规范研究等都具有很高的应用价值。\n>\n> 总的来说，平行语料库不仅为机器翻译提供了必要的数据支持，也是推动相关领域发展和进步的重要驱动力。随着科技的不断进步和全球化的深入发展，平行语料库将在未来发挥更加重要的作用\n\n#### **联合学习joint study**：\n\n> **联合学习（Joint Learning）是一种机器学习范式，它涉及同时训练多个模型或任务，以便它们可以共享知识或参数，从而提高整体的学习效率和泛化能力**。\n>\n> 在联合学习中，不同的模型或任务之间存在一定的关联性，这种关联性可以是因为它们处理相似的数据类型，或者因为它们解决的是相关的子问题。通过联合学习，这些模型可以相互促进，提高各自的性能。以下是一些关键点：\n>\n> - **模型组合**：在联合学习中，一个大模型由多个小模型组成，这些小模型可以独立训练，也可以与其他模型一起联合训练。\n> - **多任务学习**：联合学习与多任务学习（Multi-Task Learning）有相似之处，多任务学习也是让一个模型同时学习多个任务，并且在学习过程中共享参数。这样做的好处是可以利用任务之间的相关性来提升模型的泛化能力。\n> - **应用领域**：联合学习在自然语言处理（NLP）领域有广泛的应用，例如，可以通过联合模型来同时处理语法分析和语义理解等任务。\n> - **端到端学习**：在某些情况下，联合学习还涉及到端到端的训练方法，这意味着从输入到输出的整个流程都被整合在一个学习过程中，以此来优化整体性能。\n>\n> 总的来说，联合学习是一种强大的学习方法，它通过整合多个相关任务或模型的学习过程，能够提高学习效率和模型的泛化能力。这种方法在处理复杂问题时特别有用，因为它可以有效地利用不同任务之间的共同信息。\n\n### 4 衡量模型好坏的一些指标\n\n#### F1 score 精确率precision 召回率re-call\n\n![img](\\img\\ai-mianjing\\pic-3.png)\n\n![img](\\img\\ai-mianjing\\pic-4.png)\n\n#### IOU mIOU\n\n![img](\\img\\ai-mianjing\\pic-5.png)\n\n### 机器学习20道\n\n1. 网络配置时batchsize的大小怎样设置？过小和过大分别有什么特点？ **小的不稳定 大的速度快**\n\n   > - 大的batchsize **减少训练时间，提高稳定性**。 这是肯定的，同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以**减少训练时间**。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。\n   > - 过大的batchsize 泛化能力下降。而**过大的批量大小会导致收敛速度变慢**，并且可能需要更多的内存 在一定范围内，**增加batchsize有助于收敛的稳定性**，但是随着batchsize的增加，**模型的性能会下降**。\n   > - 同样是通过对训练步数的影响，小的batch_size使模型迭代次数增多，**提前到达拟合点**，但是epoch没结束，继续学习训练数据，容易导致过拟合于原始数据\n\n2. 设置学习率衰减的原因？\n\n   > 学习率衰减是为了让学习率随着训练过程逐渐减小，有助于模型**在接近全局最优解时减小步长**，避免在优化过程中跳过最佳点或产生不必要的波动\n\n3. 有哪些分类算法？\n\n   > 朴素贝叶斯、逻辑回归、K 最近邻 （KNN）、支持向量机 （SVM）、决策树、随机森林和神经网络\n\n4. 分类和回归的区别？\n\n   > 分类预测**离散标签**，将数据分类为两个或多个类别。回归预测**连续量**，估计变量之间的关系\n\n5. 请描述下k-means聚类的过程？\n\n   > k-means聚类是一种**迭代算法**，首先随机选取k个初始中心点，然后**将每个点分配到最近的中心点形成的簇**中，接着重新计算每个簇的中心点，重复此过程直到中心点不再变化或达到预设的迭代次数。\n\n6. 训练集、测试集、验证集的作用？\n\n   > 数据集的作用：**训练集用于训练模型**，**验证集用于调整超参数**并在训练过程中提供无偏评估，**测试集**用于评估模型的**泛化性能**\n   >\n   > 1. **评估偏差**: 如果我们**直接根据测试集调整超参数，那么测试集就不再是独立的评估数据集**。这意味着我们无法准确地估计模型在新数据上的性能，因为测试集已经被用来调整模型了。\n\n7. 请讲解一下k折交叉验证？\n\n   > 在 k 折叠交叉验证中，数据被划分为 k 个子集。每次，将 k 个子集中的一个用作测试集，并将其他 k-1 子集放在一起形成一个训练集。**该过程重复 k 次，每个 k 个子集恰好用作测试集一次**。\n\n8. 分类和聚类的区别？\n\n   > 分类是**监督学习任务**，需要**标注数据**来训练模型；聚类是**无监督学习**任务，不需要标注数据，目的是发现数据**内在的结构或模式。**\n\n9. 描述一下梯度的概念？\n\n   > 梯度是导数的多变量推广，表示**函数最快增长的方向和速率**。在优化中，它用于查找函数减小最快的方向。\n   >\n   > eg:\n   >\n   > 例如，考虑二元函数 f(x, y) = x^2 + y^2。我们可以计算它的梯度：\n   >\n   > 1. 计算偏导数：∂f/∂x = 2x，∂f/∂y = 2y\n   > 2. 在点 (x0, y0) = (1, 1) 处，梯度向量为：∇f(1, 1) = [2*1, 2*1] = [2, 2]\n   >\n   > 因此，在点 (1, 1) 处，梯度向量指向 [2, 2] 方向，且函数在该方向上的增长率最大，为 √(2^2 + 2^2) = √8 ≈ 2.83。这意味着在点 (1, 1) 处，函数 f(x, y) = x^2 + y^2 沿着 [2, 2] 方向增长最快，且每单位距离增长 2.83。\n\n10. 有监督学习、无监督学习和半监督学习的区别？\n\n    > 见 1.1 节\n\n11. 带核的SVM为什么能分类非线性问题？\n\n    > 非线性问题的 SVM：SVM 可以通过使用内核技巧**将输入空间转换为更高维的空间**来对非线性问题进行分类，从而更容易线性分离数据。\n\n12. 请描述常见的梯度下降方法？\n\n    > 常用的方法包括批量梯度下降、随机梯度下降 （SGD） 和小批量梯度下降，用于计算损失函数梯度的数据量不同。\n    >\n    > 批量梯度下降、随机梯度下降（SGD）和小批量梯度下降是梯度下降优化算法的三种不同实现方式，它们在机器学习和深度学习中被广泛使用来优化模 型的参数。**具体介绍如下**：\n    >\n    > - **批量梯度下降**：它在每次迭代时**使用所有的训练样本来计算梯度**并更新模型参数。这种方法可以确保每次迭代都沿着全局最优方向更新，因此收敛曲线通常比较平滑。然而，当数据集非常大时，每次迭代的计算成本会非常高，导致训练速度变慢。\n    > - **随机梯度下降**：它在**每次迭代时只使用一个训练样本来计算梯度并更新模型参数**。这种方法的优点是计算速度快，可以快速进行模型更新，特别适用于大数据集。但是，由于每次更新只基于一个样本，可能会导致收敛过程波动较大，不一定能收敛到全局最优解。\n    > - **小批量梯度下降**：它**介于批量梯度下降和随机梯度下降之间**，每次迭代**使用一小部分训练样本**（即小批量）来计算梯度并更新模型参数。这种方法既保留了批量梯度下降的稳定收敛特性，又具有随机梯度下降的快速计算优势。因此，小批量梯度下降在实践中被广泛应用，尤其是在深度学习模型的训练中。\n    >\n    > 总的来说，这三种方法都是梯度下降的变体，目的是通过迭代更新模型参数以最小化损失函数。批量梯度下降使用所有数据，收敛稳定但计算成本高；随机梯度下降使用单个样本，计算快但可能波动大；小批量梯度下降折中了前两者的特点，使用部分数据，既稳定又相对高效。\n\n13. Adam、RMSprop、Adagrad、Momentum优化算法？\n\n    > **Adam** 将 **RMSprop 和 Stochastic Gradient Descent 的思想与动量相结合**。它动态调整每个参数的学习率。RMSprop 在训练期间调整学习率，而 Adagrad 通过将学习率缩放为与梯度的所有过去平方值的平方根成反比来调整学习率。动量有助于加速新元朝相关方向发展。\n    >\n    > 此方法是在**梯度下降的基础上加入了“惯性”的概念**，即考虑了历史梯度对当前梯度的影响，使得更新过程具有一定的连续性，并能够加快学习速度，减少振荡。Momentum 通过使用动量项来对网络参数进行平滑处理，有助于使梯度的摆动幅度变小，从而加快收敛速度。\n\n14. 什么是过拟合？怎么解决过拟合问题？\n\n    > 过拟合是指模型在训练数据上表现良好但在新数据上泛化能力差的现象。\n    >\n    > **解决过拟合的方法**包括增加**数据量**、减少模型**复杂度**、使用**正则化**技术和集成学习方法等。\n\n15. 怎样解决梯度消失/爆炸问题？\n\n    > **含义**：\n    >\n    > 1. **梯度消失**：在深度神经网络的反向传播过程中，梯度可能会因为多层连续的乘法操作而变得非常小，以至于权重几乎不会被更新，导致训练过程提前停止。这种现象通常发生在激活函数选择不当（如使用sigmoid或tanh函数）且网络层次较深的情况下。\n    > 2. **梯度爆炸**：与梯度消失相反，梯度爆炸是指梯度值变得非常大，以至于导致模型参数更新过于频繁和巨大，从而使得模型无法收敛到一个稳定的解。梯度爆炸通常是由于梯度值在反向传播过程中连续乘以较大的数而累积起来的。\n    >\n    > 如何解决？\n    >\n    > - **选择合适的激活函数**：**避免**使用**容易饱和**的激活函数，如sigmoid或tanh，转而使用ReLU（Rectified Linear Units）及其变种Leaky ReLU、Parametric ReLU等，它们在输入值较大时不会饱和，有助于缓解梯度消失问题。\n    > - **批量归一化（Batch Normalization）**：通过对每一层的输入进行归一化处理，保持输入数据的均值为0、方差为1，有助于稳定梯度变化，防止梯度消失和爆炸。\n    > - **残差连接（Residual Connections）**：在深度网络中使用残差连接，即让前一层的输出直接加到后续某层的输入上，可以保证梯度能够绕过一些层直接传回，从而缓解梯度消失问题。\n    > - **梯度裁剪（Gradient Clipping）**：通过设置一个阈值来限制梯度的最大值，可以有效防止梯度爆炸。\n    > - **优化器选择**：使用具有自适应学习率调整能力的优化器，如Adam、RMSprop等，可以在不同情况下自动调整学习率，减少梯度消失或爆炸的风险。\n    > - **短程记忆结构**：对于循环神经网络（RNN），可以**使用长短时记忆（LSTM）**或门控循环单元（GRU）这类结构来代替传统的RNN结构，因为它们能够更好地捕捉长距离依赖关系，从而缓解梯度消失问题。\n    > - **正则化技术**：如L1/L2正则化，也可以帮助控制模型参数的规模，间接地减小梯度爆炸的可能性。\n\n16. 讲解下神经网络反向传播算法？\n\n    > **反向传播算法**：反向传播是人工神经网络中使用的一种方法，用于计算单个输入输出示例的损失函数相对于网络权重的梯度，从而使用梯度下降更新权重。\n\n17. 激活函数的作用是什么？有哪些激活函数？它们的表达式分别是？\n\n    > 激活函数作用：引入**非线性**因素，使得神经网络可以学习和模拟任何复杂的函数和数据分布，这样网络就能处理非线性问题。如果**没有激活函数，无论神经网络有多少层，输出都只是输入的线性组合**，这大大限制了网络的表达能力和复杂度。\n    >\n    > 常见的激活函数包括 Sigmoid、Tanh、ReLU：（需要去了解一下基本原理）\n\n18. 请讲解一下正则化的概念？L1正则化是什么？L2正则化是什么？\n\n    > 正则化是机器学习和统计学中用于**防止模型过拟合**的一种技术。过拟合是指模型在训练数据上表现得很好，但在未见过的新数据上表现不佳的现象，通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是潜在的数据生成规律。**正则化通过在模型训练的损失函数中添加一个额外的惩罚项来解决这个问题，该惩罚项会惩罚模型的复杂度**\n    >\n    > - **L1正则化**（也称为Lasso正则化）通过向损失函数添加参数的绝对值之和作为惩罚项来工作。L1正则化倾向于产生稀疏参数，即许多参数会变成零，这可以用于特征选择。\n    > - **L2正则化**（也称为Ridge正则化）通过向损失函数添加参数的平方和作为惩罚项来工作。与L1正则化不同，L2正则化倾向于使参数非常小但不完全为零，从而保持所有特征但减少其影响。\n    > - 在某些情况下，人们可能会**同时使用L1和L2正则化**，这种方法被称为**弹性网络**正则化。\n    > - 正则化技术不仅限于线性模型，它们也广泛应用于神经网络、决策树和其他类型的机器学习模型中。在神经网络中，除了L1和L2正则化外，还有如**Dropout**正则化等特殊形式的正则化技术，它通过在训练过程中随机丢弃一部分神经元来防止过拟合。\n\n19. 除了BatchNormalization还有其他什么方法来加速模型训练吗？\n\n    > 1. 使用**半字训练（半精度浮点数）**\n    > 2. **合理的超参数设计**：选择合适的batch size、epoch数量和学习率策略对于加速模型训练至关重要。较小的batch size可以加快单次迭代的速度，而适当的epoch数量和学习率调整可以帮助模型更快地收敛。\n    > 3. **增大batch size**：虽然增大batch size可以提高训练速度，但过大的batch size可能会影响模型的泛化性能和收敛速度。因此，需要根据具体情况选择合适的batch size大小。\n    > 4. **多GPU分布式训练**：通过使用多个GPU进行模型并行或数据并行训练，可以显著提高模型的训练速度。模型并行是将模型的不同部分分配给不同的计算设备，而数据并行则是将数据集分割成多个子集，每个子集在不同的计算设备上进行处理。\n    > 5. **模型压缩与加速技术**：包括参数剪枝、参数量化、紧凑网络设计、知识蒸馏、低秩分解、参数共享等方法。这些技术旨在减少模型的大小和复杂性，从而提高训练和推理的速度。\n    > 6. **使用预训练模型**：在相似任务上使用预训练模型可以显著减少训练时间，因为预训练模型已经学习了大量的特征，只需要对顶层进行微调即可。\n\n20. 机器学习中，为何要经常对数据进行预处理？\n\n    > 处理噪声、缺失值、降维、结构化非结构化处理\n    >\n    > 1. **处理缺失值与噪声**：数据中可能存在缺失值，预处理可以包括填补或删除这些缺失值，以确保数据的完整性。\n    > 2. **无量纲化**：不同特征可能具有不同的量纲和规模，无量纲化处理可以消除这种差异，使得模型能够更公平地评估每个特征的重要性。\n    > 3. **数据规范化**：通过最值归一化或Z-Score规范化等方法，将数据缩放到特定的范围内，有助于加快模型的收敛速度。\n    > 4. **特征工程**：数据预处理是特征工程的一部分，它包括数据清洗、特征提取、特征选择和特征构造等子问题。通过这些步骤，可以创建出能够使机器学习算法达到最佳性能的特征。\n    > 5. **提高模型性能**：干净且经过恰当处理的数据可以帮助算法更准确地学习模式和关系，从而提高模型的性能。\n    > 6. **增强模型泛化能力**：通过正则化等预处理方法，可以防止模型过拟合，提高其在新数据上的泛化能力。","source":"_posts/ai-mianjing.md","raw":"---\ntitle: AI面经\nmathjax: true\ndate: 2025/01/26 20:46:25\nimg: https://img1.baidu.com/it/u=4282277671,2501328998&fm=253&fmt=auto&app=138&f=JPEG?w=449&h=252\nexcerpt: 人工智能经典面试问题\n---\n\n### 1 机器学习、深度学习、与人工智能对比\n\n- **机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术**\n\n> **人工智能**（人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器）\n>\n> **分类**： 弱人工智能（当前）、强人工智能、超人工智能\n>\n> **机器学习**（MachineLearning）简称ML：机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。\n>\n> **深度学习**（DeepLearning）简称DL。**最初**的深度学习是**利用深度神经网络**来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于**建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本**。\n\n#### 1.1 人工智能\n\n> **人工智能（Artificial intelligence）简称AI**。人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。\n>\n> - 人工智能目前分为弱人工智能和强人工智能和超人工智能：\n> - **弱人工智能**：弱人工智能（Artificial Narrow Intelligence /ANI),只专注于完成某个特定的任务，例如语音识别、图象识别和翻译等，是擅长于单个方面的人工智能。它们只是用于解决特定的具体类的任务问题而存在，大都是统计数据，以此从中归纳出模型。由于弱人工智能智能处理较为单一的问题，且发展程度并没有达到模拟人脑思维的程度，所以弱人工智能仍然属于“工具”的范畴，与传统的“产品”在本质上并无区别。\n>\n> - **强人工智能**：强人工智能（Artificial General lnteligence /AGI),属于人类级别的人工智能，在各方面都能和人类比肩，它能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作，并且和人类一样得心应手。\n>\n> - **超人工智能**：超人工智能（Artificial Superintelligence/ASI），在几乎所有领域都比最聪明的人类大脑都聪明许多，包括科学创新、通识和社交技能。在超人工智能阶段，人工智能已经跨过“奇点”，其计算和思维能力已经远超人脑。此时的人工智能已经不是人类可以理解和想象。人工智能将打破人脑受到的维度限制，其所观察和思考的内容，人脑已经无法理解，人工智能将形成一个新的社会。\n>\n> **目前我们仍处于弱人工智能阶段**。\n\n#### 1.2 机器学习\n\n> **机器学习（Machine Learning）简称ML**。机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。\n\n#### 1.3 深度学习\n\n> **深度学习（Deep Learning）简称DL**。最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本。\n>\n> 注意：你可能在接触深度学习的时候也听到过**监督学习**、**非监督学习**、**半监督学习**等概念，下面就顺便对这三个名词解析下：\n>\n> 1. **监督学习：**用一部分已知分类、有标记的样本来训练机器后，让它用学到的特征，对没有还分类、无标记的样本进行分类、贴标签。多用于分类。\n> 2. **非监督学习：**所有的数据没有标记，类别未知，让它自己学习样本之间的相似性来进行分类。多用于聚类。\n> 3. **半监督学习：**有两个样本集，一个有标记，一个没有标记。综合利用有类标的样本（ labeled sample）和没有类标的样本（ unlabeled sample），来生成合适的分类。\n\n#### 1.4 区别与联系\n\n![img](\\img\\ai-mianjing\\pic-1.png)\n\n- 人工智能的研究领域包括 **专家系统**(Expert Systems)、**机器学习**(Machine Learning)、**模式识别**、**进化计算**(Evolutionary Computation)、**模糊逻辑**(Fussy Logic)、**计算机视觉**(Computer Vision)、**自然语言处理**(NLP)、**推荐系统**(Recommender Systems)\n- 深度学习，一种**实现机器学习**的技术。是机器学习中的一个方法或者分支。\n- **深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术**。\n- 它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和[自然语言处理](https://so.csdn.net/so/search?q=自然语言处理&spm=1001.2101.3001.7020)(NLP)领域。显然，**“深度学习”是与机器学习中的“神经网络”是强相关**，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。\n- **总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU**\n\n![img](\\img\\ai-mianjing\\pic-2.png)\n\n### 2 AI的分类\n\n- **模式识别**：\n\n  - **模式识别是指用计算机对物体进行识别。**\n\n    **物体指文字、符号、图形、图像、语音、声音等实体对象；**\n\n    •**不包括概念、思想、意识等虚拟对象，它们属于认知和哲学研究范畴。**\n\n  - **模式识别应用：**\n\n    • **文字识别，车牌识别，人脸识别，视网膜识别，指纹识别，掌纹识别等**\n\n- **自然语言处理**：用自然语言同计算机进行通讯的一种技术\n\n  - 机器翻译\n  - 语音识别（**可以单列**）\n  - 文本分类/预测\n\n- **机器学习**（重点是深度学习）：机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法\n\n- **计算机视觉**：用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像\n\n- **智能机器人**：给机器人装上“大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信\n\n  息的综合处理等方面将会前进一大步\n\n- **自动程序设计**：自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序\n\n- **数据挖掘**：从大量的数据中搜索隐藏在其中信息的过程\n\n- **专家系统**等\n\n### 3 一些深度学习概念\n\n#### **监督学习基础定义**：\n\n**有监督学习 (Supervised Learning)**: 使用**已经标注的数据**训练模型，目的是让模型学会如何将输入映射到期望的输出。例如，使用一组已知类别的图片来训练一个图像**分类模型**，模型学习如何根据输入的图片预测正确的类别 。\n\n**无监督学习 (Unsupervised Learning)**: **不使用任何标注数据**，让模型自己发现数据中的结构和模式。例如，**聚类**算法可以将数据分为几个不同的组，而不事先告诉模型应该有多少组，或者各组的特征 。\n\n**半监督学习 (Semi-Supervised Learning)**: 结合**有监督和无监督学习**的方法，使用**少量标注数据和大量未标注数据**共同训练模型。这种方法在标注数据难以获得但未标注数据丰富的场景下特别有用 。 例如：**医学影像**，大部分没有标签，人工打标较为昂贵。可以用\n\n- 使用已标注数据训练一个分类器。\n- 用该分类器对未标注数据进行预测，生成伪标签。\n- 选择置信度最高的伪标签来扩充训练集。\n- 结合原始的标注数据和带有伪标签的数据重新训练分类器\n\n**自监督学习 (Self-Supervised Learning)**: 一种特殊的无监督学习方法，通过**自动生成伪标签**来训练模型。这种方法通过创建预测任务（如预测未来的帧、填充缺失的部分、一个自监督学习模型可以通过尝试预测句子中缺失的单词来学习语言的语法和词汇等），使模型能够从输入数据本身学习有用的表示 。 通常是**预训练+下游任务**的方式。\n\n**对比**：\n\n> - **有监督学习**通常需要大量的标注数据，适用于标注数据充足的任务。\n> - **无监督学习**不需要标签，适用于探索数据内在结构和关系的任务。\n> - **半监督学习**结合了有监督和无监督的优点，适用于标注数据有限但无标签数据多的情况。\n> - **自监督学习**则是一种新兴的方法，它能够从未标注的数据中学习到有用的特征表示，适用于标注数据难以获得但需要丰富特征表示的任务。\n\n**半监督学习举例**\n\n- 使用已标注数据训练一个分类器。\n- 用该分类器对未标注数据进行预测，生成伪标签。\n- 选择置信度最高的伪标签来扩充训练集。\n- 结合原始的标注数据和带有伪标签的数据重新训练分类器\n\n#### **对比预训练**：\n\n> 对比预训练是一种自监督学习方法，其核心目标是通过学习**区分不同数据之间的相似性和差异性来提升模型的特征表示能力**。这种方法不依赖于人工标注的数据，而是通过**定义一种或多种对比任务**（比如区分不同图像、文本或声音等），使得模型能够自动学习到数据的**内在特征和结构**。对比预训练通过比较正样本对（相似）和负样本对（不相似）来优化模型，使模型能够捕捉到数据中有用的信息和模式，从而在没有大量标注数据的情况下也能有效提升模型性能 。\n>\n> **在NLP和CV等领域，对比学习已经成为一种重要的自监督学习方法**，**通过自监督预训练模式**，模型可以从数据本身的先验知识分布中吸取图像或文本的特征，得到一个能够更好地适应不同任务和领域的预训练模 。这种方法的优势在于**无需大量标注数据**，同时可以**显著提高模型的泛化性能和学习能力** 。\n\n#### **平行语料库**：\n\n> **平行语料库是收录了某一源语言文本及其对应的目标语文本的语料库**。具体来说，平行语料库包括以下内容：\n>\n> - **源语言和目标语言的对应文本**：这些文本在语义上是相同的，只是用不同的语言表达，如英文原版和中文翻译版。\n> - **对齐的层面**：平行对齐指的是源语文本和目标语文本之间的具体单位的对应关系或翻译关系，可以细分为词汇、语句和段落等层面的对齐。\n> - **类型**：根据所涉及的语种数量和方向，平行语料库可以分为单向平行语料库、双向平行语料库和多向平行语料库。\n> - **应用**：平行语料库对于语言对比、双语词典编纂、机器翻译、翻译策略与规范研究等都具有很高的应用价值。\n>\n> 总的来说，平行语料库不仅为机器翻译提供了必要的数据支持，也是推动相关领域发展和进步的重要驱动力。随着科技的不断进步和全球化的深入发展，平行语料库将在未来发挥更加重要的作用\n\n#### **联合学习joint study**：\n\n> **联合学习（Joint Learning）是一种机器学习范式，它涉及同时训练多个模型或任务，以便它们可以共享知识或参数，从而提高整体的学习效率和泛化能力**。\n>\n> 在联合学习中，不同的模型或任务之间存在一定的关联性，这种关联性可以是因为它们处理相似的数据类型，或者因为它们解决的是相关的子问题。通过联合学习，这些模型可以相互促进，提高各自的性能。以下是一些关键点：\n>\n> - **模型组合**：在联合学习中，一个大模型由多个小模型组成，这些小模型可以独立训练，也可以与其他模型一起联合训练。\n> - **多任务学习**：联合学习与多任务学习（Multi-Task Learning）有相似之处，多任务学习也是让一个模型同时学习多个任务，并且在学习过程中共享参数。这样做的好处是可以利用任务之间的相关性来提升模型的泛化能力。\n> - **应用领域**：联合学习在自然语言处理（NLP）领域有广泛的应用，例如，可以通过联合模型来同时处理语法分析和语义理解等任务。\n> - **端到端学习**：在某些情况下，联合学习还涉及到端到端的训练方法，这意味着从输入到输出的整个流程都被整合在一个学习过程中，以此来优化整体性能。\n>\n> 总的来说，联合学习是一种强大的学习方法，它通过整合多个相关任务或模型的学习过程，能够提高学习效率和模型的泛化能力。这种方法在处理复杂问题时特别有用，因为它可以有效地利用不同任务之间的共同信息。\n\n### 4 衡量模型好坏的一些指标\n\n#### F1 score 精确率precision 召回率re-call\n\n![img](\\img\\ai-mianjing\\pic-3.png)\n\n![img](\\img\\ai-mianjing\\pic-4.png)\n\n#### IOU mIOU\n\n![img](\\img\\ai-mianjing\\pic-5.png)\n\n### 机器学习20道\n\n1. 网络配置时batchsize的大小怎样设置？过小和过大分别有什么特点？ **小的不稳定 大的速度快**\n\n   > - 大的batchsize **减少训练时间，提高稳定性**。 这是肯定的，同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以**减少训练时间**。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。\n   > - 过大的batchsize 泛化能力下降。而**过大的批量大小会导致收敛速度变慢**，并且可能需要更多的内存 在一定范围内，**增加batchsize有助于收敛的稳定性**，但是随着batchsize的增加，**模型的性能会下降**。\n   > - 同样是通过对训练步数的影响，小的batch_size使模型迭代次数增多，**提前到达拟合点**，但是epoch没结束，继续学习训练数据，容易导致过拟合于原始数据\n\n2. 设置学习率衰减的原因？\n\n   > 学习率衰减是为了让学习率随着训练过程逐渐减小，有助于模型**在接近全局最优解时减小步长**，避免在优化过程中跳过最佳点或产生不必要的波动\n\n3. 有哪些分类算法？\n\n   > 朴素贝叶斯、逻辑回归、K 最近邻 （KNN）、支持向量机 （SVM）、决策树、随机森林和神经网络\n\n4. 分类和回归的区别？\n\n   > 分类预测**离散标签**，将数据分类为两个或多个类别。回归预测**连续量**，估计变量之间的关系\n\n5. 请描述下k-means聚类的过程？\n\n   > k-means聚类是一种**迭代算法**，首先随机选取k个初始中心点，然后**将每个点分配到最近的中心点形成的簇**中，接着重新计算每个簇的中心点，重复此过程直到中心点不再变化或达到预设的迭代次数。\n\n6. 训练集、测试集、验证集的作用？\n\n   > 数据集的作用：**训练集用于训练模型**，**验证集用于调整超参数**并在训练过程中提供无偏评估，**测试集**用于评估模型的**泛化性能**\n   >\n   > 1. **评估偏差**: 如果我们**直接根据测试集调整超参数，那么测试集就不再是独立的评估数据集**。这意味着我们无法准确地估计模型在新数据上的性能，因为测试集已经被用来调整模型了。\n\n7. 请讲解一下k折交叉验证？\n\n   > 在 k 折叠交叉验证中，数据被划分为 k 个子集。每次，将 k 个子集中的一个用作测试集，并将其他 k-1 子集放在一起形成一个训练集。**该过程重复 k 次，每个 k 个子集恰好用作测试集一次**。\n\n8. 分类和聚类的区别？\n\n   > 分类是**监督学习任务**，需要**标注数据**来训练模型；聚类是**无监督学习**任务，不需要标注数据，目的是发现数据**内在的结构或模式。**\n\n9. 描述一下梯度的概念？\n\n   > 梯度是导数的多变量推广，表示**函数最快增长的方向和速率**。在优化中，它用于查找函数减小最快的方向。\n   >\n   > eg:\n   >\n   > 例如，考虑二元函数 f(x, y) = x^2 + y^2。我们可以计算它的梯度：\n   >\n   > 1. 计算偏导数：∂f/∂x = 2x，∂f/∂y = 2y\n   > 2. 在点 (x0, y0) = (1, 1) 处，梯度向量为：∇f(1, 1) = [2*1, 2*1] = [2, 2]\n   >\n   > 因此，在点 (1, 1) 处，梯度向量指向 [2, 2] 方向，且函数在该方向上的增长率最大，为 √(2^2 + 2^2) = √8 ≈ 2.83。这意味着在点 (1, 1) 处，函数 f(x, y) = x^2 + y^2 沿着 [2, 2] 方向增长最快，且每单位距离增长 2.83。\n\n10. 有监督学习、无监督学习和半监督学习的区别？\n\n    > 见 1.1 节\n\n11. 带核的SVM为什么能分类非线性问题？\n\n    > 非线性问题的 SVM：SVM 可以通过使用内核技巧**将输入空间转换为更高维的空间**来对非线性问题进行分类，从而更容易线性分离数据。\n\n12. 请描述常见的梯度下降方法？\n\n    > 常用的方法包括批量梯度下降、随机梯度下降 （SGD） 和小批量梯度下降，用于计算损失函数梯度的数据量不同。\n    >\n    > 批量梯度下降、随机梯度下降（SGD）和小批量梯度下降是梯度下降优化算法的三种不同实现方式，它们在机器学习和深度学习中被广泛使用来优化模 型的参数。**具体介绍如下**：\n    >\n    > - **批量梯度下降**：它在每次迭代时**使用所有的训练样本来计算梯度**并更新模型参数。这种方法可以确保每次迭代都沿着全局最优方向更新，因此收敛曲线通常比较平滑。然而，当数据集非常大时，每次迭代的计算成本会非常高，导致训练速度变慢。\n    > - **随机梯度下降**：它在**每次迭代时只使用一个训练样本来计算梯度并更新模型参数**。这种方法的优点是计算速度快，可以快速进行模型更新，特别适用于大数据集。但是，由于每次更新只基于一个样本，可能会导致收敛过程波动较大，不一定能收敛到全局最优解。\n    > - **小批量梯度下降**：它**介于批量梯度下降和随机梯度下降之间**，每次迭代**使用一小部分训练样本**（即小批量）来计算梯度并更新模型参数。这种方法既保留了批量梯度下降的稳定收敛特性，又具有随机梯度下降的快速计算优势。因此，小批量梯度下降在实践中被广泛应用，尤其是在深度学习模型的训练中。\n    >\n    > 总的来说，这三种方法都是梯度下降的变体，目的是通过迭代更新模型参数以最小化损失函数。批量梯度下降使用所有数据，收敛稳定但计算成本高；随机梯度下降使用单个样本，计算快但可能波动大；小批量梯度下降折中了前两者的特点，使用部分数据，既稳定又相对高效。\n\n13. Adam、RMSprop、Adagrad、Momentum优化算法？\n\n    > **Adam** 将 **RMSprop 和 Stochastic Gradient Descent 的思想与动量相结合**。它动态调整每个参数的学习率。RMSprop 在训练期间调整学习率，而 Adagrad 通过将学习率缩放为与梯度的所有过去平方值的平方根成反比来调整学习率。动量有助于加速新元朝相关方向发展。\n    >\n    > 此方法是在**梯度下降的基础上加入了“惯性”的概念**，即考虑了历史梯度对当前梯度的影响，使得更新过程具有一定的连续性，并能够加快学习速度，减少振荡。Momentum 通过使用动量项来对网络参数进行平滑处理，有助于使梯度的摆动幅度变小，从而加快收敛速度。\n\n14. 什么是过拟合？怎么解决过拟合问题？\n\n    > 过拟合是指模型在训练数据上表现良好但在新数据上泛化能力差的现象。\n    >\n    > **解决过拟合的方法**包括增加**数据量**、减少模型**复杂度**、使用**正则化**技术和集成学习方法等。\n\n15. 怎样解决梯度消失/爆炸问题？\n\n    > **含义**：\n    >\n    > 1. **梯度消失**：在深度神经网络的反向传播过程中，梯度可能会因为多层连续的乘法操作而变得非常小，以至于权重几乎不会被更新，导致训练过程提前停止。这种现象通常发生在激活函数选择不当（如使用sigmoid或tanh函数）且网络层次较深的情况下。\n    > 2. **梯度爆炸**：与梯度消失相反，梯度爆炸是指梯度值变得非常大，以至于导致模型参数更新过于频繁和巨大，从而使得模型无法收敛到一个稳定的解。梯度爆炸通常是由于梯度值在反向传播过程中连续乘以较大的数而累积起来的。\n    >\n    > 如何解决？\n    >\n    > - **选择合适的激活函数**：**避免**使用**容易饱和**的激活函数，如sigmoid或tanh，转而使用ReLU（Rectified Linear Units）及其变种Leaky ReLU、Parametric ReLU等，它们在输入值较大时不会饱和，有助于缓解梯度消失问题。\n    > - **批量归一化（Batch Normalization）**：通过对每一层的输入进行归一化处理，保持输入数据的均值为0、方差为1，有助于稳定梯度变化，防止梯度消失和爆炸。\n    > - **残差连接（Residual Connections）**：在深度网络中使用残差连接，即让前一层的输出直接加到后续某层的输入上，可以保证梯度能够绕过一些层直接传回，从而缓解梯度消失问题。\n    > - **梯度裁剪（Gradient Clipping）**：通过设置一个阈值来限制梯度的最大值，可以有效防止梯度爆炸。\n    > - **优化器选择**：使用具有自适应学习率调整能力的优化器，如Adam、RMSprop等，可以在不同情况下自动调整学习率，减少梯度消失或爆炸的风险。\n    > - **短程记忆结构**：对于循环神经网络（RNN），可以**使用长短时记忆（LSTM）**或门控循环单元（GRU）这类结构来代替传统的RNN结构，因为它们能够更好地捕捉长距离依赖关系，从而缓解梯度消失问题。\n    > - **正则化技术**：如L1/L2正则化，也可以帮助控制模型参数的规模，间接地减小梯度爆炸的可能性。\n\n16. 讲解下神经网络反向传播算法？\n\n    > **反向传播算法**：反向传播是人工神经网络中使用的一种方法，用于计算单个输入输出示例的损失函数相对于网络权重的梯度，从而使用梯度下降更新权重。\n\n17. 激活函数的作用是什么？有哪些激活函数？它们的表达式分别是？\n\n    > 激活函数作用：引入**非线性**因素，使得神经网络可以学习和模拟任何复杂的函数和数据分布，这样网络就能处理非线性问题。如果**没有激活函数，无论神经网络有多少层，输出都只是输入的线性组合**，这大大限制了网络的表达能力和复杂度。\n    >\n    > 常见的激活函数包括 Sigmoid、Tanh、ReLU：（需要去了解一下基本原理）\n\n18. 请讲解一下正则化的概念？L1正则化是什么？L2正则化是什么？\n\n    > 正则化是机器学习和统计学中用于**防止模型过拟合**的一种技术。过拟合是指模型在训练数据上表现得很好，但在未见过的新数据上表现不佳的现象，通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是潜在的数据生成规律。**正则化通过在模型训练的损失函数中添加一个额外的惩罚项来解决这个问题，该惩罚项会惩罚模型的复杂度**\n    >\n    > - **L1正则化**（也称为Lasso正则化）通过向损失函数添加参数的绝对值之和作为惩罚项来工作。L1正则化倾向于产生稀疏参数，即许多参数会变成零，这可以用于特征选择。\n    > - **L2正则化**（也称为Ridge正则化）通过向损失函数添加参数的平方和作为惩罚项来工作。与L1正则化不同，L2正则化倾向于使参数非常小但不完全为零，从而保持所有特征但减少其影响。\n    > - 在某些情况下，人们可能会**同时使用L1和L2正则化**，这种方法被称为**弹性网络**正则化。\n    > - 正则化技术不仅限于线性模型，它们也广泛应用于神经网络、决策树和其他类型的机器学习模型中。在神经网络中，除了L1和L2正则化外，还有如**Dropout**正则化等特殊形式的正则化技术，它通过在训练过程中随机丢弃一部分神经元来防止过拟合。\n\n19. 除了BatchNormalization还有其他什么方法来加速模型训练吗？\n\n    > 1. 使用**半字训练（半精度浮点数）**\n    > 2. **合理的超参数设计**：选择合适的batch size、epoch数量和学习率策略对于加速模型训练至关重要。较小的batch size可以加快单次迭代的速度，而适当的epoch数量和学习率调整可以帮助模型更快地收敛。\n    > 3. **增大batch size**：虽然增大batch size可以提高训练速度，但过大的batch size可能会影响模型的泛化性能和收敛速度。因此，需要根据具体情况选择合适的batch size大小。\n    > 4. **多GPU分布式训练**：通过使用多个GPU进行模型并行或数据并行训练，可以显著提高模型的训练速度。模型并行是将模型的不同部分分配给不同的计算设备，而数据并行则是将数据集分割成多个子集，每个子集在不同的计算设备上进行处理。\n    > 5. **模型压缩与加速技术**：包括参数剪枝、参数量化、紧凑网络设计、知识蒸馏、低秩分解、参数共享等方法。这些技术旨在减少模型的大小和复杂性，从而提高训练和推理的速度。\n    > 6. **使用预训练模型**：在相似任务上使用预训练模型可以显著减少训练时间，因为预训练模型已经学习了大量的特征，只需要对顶层进行微调即可。\n\n20. 机器学习中，为何要经常对数据进行预处理？\n\n    > 处理噪声、缺失值、降维、结构化非结构化处理\n    >\n    > 1. **处理缺失值与噪声**：数据中可能存在缺失值，预处理可以包括填补或删除这些缺失值，以确保数据的完整性。\n    > 2. **无量纲化**：不同特征可能具有不同的量纲和规模，无量纲化处理可以消除这种差异，使得模型能够更公平地评估每个特征的重要性。\n    > 3. **数据规范化**：通过最值归一化或Z-Score规范化等方法，将数据缩放到特定的范围内，有助于加快模型的收敛速度。\n    > 4. **特征工程**：数据预处理是特征工程的一部分，它包括数据清洗、特征提取、特征选择和特征构造等子问题。通过这些步骤，可以创建出能够使机器学习算法达到最佳性能的特征。\n    > 5. **提高模型性能**：干净且经过恰当处理的数据可以帮助算法更准确地学习模式和关系，从而提高模型的性能。\n    > 6. **增强模型泛化能力**：通过正则化等预处理方法，可以防止模型过拟合，提高其在新数据上的泛化能力。","slug":"ai-mianjing","published":1,"updated":"2025-02-17T12:28:29.610Z","comments":1,"layout":"post","photos":[],"_id":"cm7o4qpnw00012o999tg65g1b","content":"<h3 id=\"1-机器学习、深度学习、与人工智能对比\"><a href=\"#1-机器学习、深度学习、与人工智能对比\" class=\"headerlink\" title=\"1 机器学习、深度学习、与人工智能对比\"></a>1 机器学习、深度学习、与人工智能对比</h3><ul>\n<li><strong>机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术</strong></li>\n</ul>\n<blockquote>\n<p><strong>人工智能</strong>（人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器）</p>\n<p><strong>分类</strong>： 弱人工智能（当前）、强人工智能、超人工智能</p>\n<p><strong>机器学习</strong>（MachineLearning）简称ML：机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。</p>\n<p><strong>深度学习</strong>（DeepLearning）简称DL。<strong>最初</strong>的深度学习是<strong>利用深度神经网络</strong>来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于<strong>建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本</strong>。</p>\n</blockquote>\n<h4 id=\"1-1-人工智能\"><a href=\"#1-1-人工智能\" class=\"headerlink\" title=\"1.1 人工智能\"></a>1.1 人工智能</h4><blockquote>\n<p><strong>人工智能（Artificial intelligence）简称AI</strong>。人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。</p>\n<ul>\n<li><p>人工智能目前分为弱人工智能和强人工智能和超人工智能：</p>\n</li>\n<li><p><strong>弱人工智能</strong>：弱人工智能（Artificial Narrow Intelligence &#x2F;ANI),只专注于完成某个特定的任务，例如语音识别、图象识别和翻译等，是擅长于单个方面的人工智能。它们只是用于解决特定的具体类的任务问题而存在，大都是统计数据，以此从中归纳出模型。由于弱人工智能智能处理较为单一的问题，且发展程度并没有达到模拟人脑思维的程度，所以弱人工智能仍然属于“工具”的范畴，与传统的“产品”在本质上并无区别。</p>\n</li>\n<li><p><strong>强人工智能</strong>：强人工智能（Artificial General lnteligence &#x2F;AGI),属于人类级别的人工智能，在各方面都能和人类比肩，它能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作，并且和人类一样得心应手。</p>\n</li>\n<li><p><strong>超人工智能</strong>：超人工智能（Artificial Superintelligence&#x2F;ASI），在几乎所有领域都比最聪明的人类大脑都聪明许多，包括科学创新、通识和社交技能。在超人工智能阶段，人工智能已经跨过“奇点”，其计算和思维能力已经远超人脑。此时的人工智能已经不是人类可以理解和想象。人工智能将打破人脑受到的维度限制，其所观察和思考的内容，人脑已经无法理解，人工智能将形成一个新的社会。</p>\n</li>\n</ul>\n<p><strong>目前我们仍处于弱人工智能阶段</strong>。</p>\n</blockquote>\n<h4 id=\"1-2-机器学习\"><a href=\"#1-2-机器学习\" class=\"headerlink\" title=\"1.2 机器学习\"></a>1.2 机器学习</h4><blockquote>\n<p><strong>机器学习（Machine Learning）简称ML</strong>。机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。</p>\n</blockquote>\n<h4 id=\"1-3-深度学习\"><a href=\"#1-3-深度学习\" class=\"headerlink\" title=\"1.3 深度学习\"></a>1.3 深度学习</h4><blockquote>\n<p><strong>深度学习（Deep Learning）简称DL</strong>。最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本。</p>\n<p>注意：你可能在接触深度学习的时候也听到过<strong>监督学习</strong>、<strong>非监督学习</strong>、<strong>半监督学习</strong>等概念，下面就顺便对这三个名词解析下：</p>\n<ol>\n<li><strong>监督学习：</strong>用一部分已知分类、有标记的样本来训练机器后，让它用学到的特征，对没有还分类、无标记的样本进行分类、贴标签。多用于分类。</li>\n<li><strong>非监督学习：</strong>所有的数据没有标记，类别未知，让它自己学习样本之间的相似性来进行分类。多用于聚类。</li>\n<li><strong>半监督学习：</strong>有两个样本集，一个有标记，一个没有标记。综合利用有类标的样本（ labeled sample）和没有类标的样本（ unlabeled sample），来生成合适的分类。</li>\n</ol>\n</blockquote>\n<h4 id=\"1-4-区别与联系\"><a href=\"#1-4-区别与联系\" class=\"headerlink\" title=\"1.4 区别与联系\"></a>1.4 区别与联系</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-1.png\" class=\"lazyload placeholder\" data-srcset=\"/%5Cimg%5Cai-mianjing%5Cpic-1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<ul>\n<li>人工智能的研究领域包括 <strong>专家系统</strong>(Expert Systems)、<strong>机器学习</strong>(Machine Learning)、<strong>模式识别</strong>、<strong>进化计算</strong>(Evolutionary Computation)、<strong>模糊逻辑</strong>(Fussy Logic)、<strong>计算机视觉</strong>(Computer Vision)、<strong>自然语言处理</strong>(NLP)、<strong>推荐系统</strong>(Recommender Systems)</li>\n<li>深度学习，一种<strong>实现机器学习</strong>的技术。是机器学习中的一个方法或者分支。</li>\n<li><strong>深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术</strong>。</li>\n<li>它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和<a href=\"https://so.csdn.net/so/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&spm=1001.2101.3001.7020\">自然语言处理</a>(NLP)领域。显然，<strong>“深度学习”是与机器学习中的“神经网络”是强相关</strong>，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。</li>\n<li><strong>总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU</strong></li>\n</ul>\n<p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-2.png\" class=\"lazyload placeholder\" data-srcset=\"/%5Cimg%5Cai-mianjing%5Cpic-2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"2-AI的分类\"><a href=\"#2-AI的分类\" class=\"headerlink\" title=\"2 AI的分类\"></a>2 AI的分类</h3><ul>\n<li><p><strong>模式识别</strong>：</p>\n<ul>\n<li><p><strong>模式识别是指用计算机对物体进行识别。</strong></p>\n<p><strong>物体指文字、符号、图形、图像、语音、声音等实体对象；</strong></p>\n<p>•<strong>不包括概念、思想、意识等虚拟对象，它们属于认知和哲学研究范畴。</strong></p>\n</li>\n<li><p><strong>模式识别应用：</strong></p>\n<p>• <strong>文字识别，车牌识别，人脸识别，视网膜识别，指纹识别，掌纹识别等</strong></p>\n</li>\n</ul>\n</li>\n<li><p><strong>自然语言处理</strong>：用自然语言同计算机进行通讯的一种技术</p>\n<ul>\n<li>机器翻译</li>\n<li>语音识别（<strong>可以单列</strong>）</li>\n<li>文本分类&#x2F;预测</li>\n</ul>\n</li>\n<li><p><strong>机器学习</strong>（重点是深度学习）：机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法</p>\n</li>\n<li><p><strong>计算机视觉</strong>：用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像</p>\n</li>\n<li><p><strong>智能机器人</strong>：给机器人装上“大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信</p>\n<p>息的综合处理等方面将会前进一大步</p>\n</li>\n<li><p><strong>自动程序设计</strong>：自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序</p>\n</li>\n<li><p><strong>数据挖掘</strong>：从大量的数据中搜索隐藏在其中信息的过程</p>\n</li>\n<li><p><strong>专家系统</strong>等</p>\n</li>\n</ul>\n<h3 id=\"3-一些深度学习概念\"><a href=\"#3-一些深度学习概念\" class=\"headerlink\" title=\"3 一些深度学习概念\"></a>3 一些深度学习概念</h3><h4 id=\"监督学习基础定义：\"><a href=\"#监督学习基础定义：\" class=\"headerlink\" title=\"监督学习基础定义：\"></a><strong>监督学习基础定义</strong>：</h4><p><strong>有监督学习 (Supervised Learning)</strong>: 使用<strong>已经标注的数据</strong>训练模型，目的是让模型学会如何将输入映射到期望的输出。例如，使用一组已知类别的图片来训练一个图像<strong>分类模型</strong>，模型学习如何根据输入的图片预测正确的类别 。</p>\n<p><strong>无监督学习 (Unsupervised Learning)</strong>: <strong>不使用任何标注数据</strong>，让模型自己发现数据中的结构和模式。例如，<strong>聚类</strong>算法可以将数据分为几个不同的组，而不事先告诉模型应该有多少组，或者各组的特征 。</p>\n<p><strong>半监督学习 (Semi-Supervised Learning)</strong>: 结合<strong>有监督和无监督学习</strong>的方法，使用<strong>少量标注数据和大量未标注数据</strong>共同训练模型。这种方法在标注数据难以获得但未标注数据丰富的场景下特别有用 。 例如：<strong>医学影像</strong>，大部分没有标签，人工打标较为昂贵。可以用</p>\n<ul>\n<li>使用已标注数据训练一个分类器。</li>\n<li>用该分类器对未标注数据进行预测，生成伪标签。</li>\n<li>选择置信度最高的伪标签来扩充训练集。</li>\n<li>结合原始的标注数据和带有伪标签的数据重新训练分类器</li>\n</ul>\n<p><strong>自监督学习 (Self-Supervised Learning)</strong>: 一种特殊的无监督学习方法，通过<strong>自动生成伪标签</strong>来训练模型。这种方法通过创建预测任务（如预测未来的帧、填充缺失的部分、一个自监督学习模型可以通过尝试预测句子中缺失的单词来学习语言的语法和词汇等），使模型能够从输入数据本身学习有用的表示 。 通常是<strong>预训练+下游任务</strong>的方式。</p>\n<p><strong>对比</strong>：</p>\n<blockquote>\n<ul>\n<li><strong>有监督学习</strong>通常需要大量的标注数据，适用于标注数据充足的任务。</li>\n<li><strong>无监督学习</strong>不需要标签，适用于探索数据内在结构和关系的任务。</li>\n<li><strong>半监督学习</strong>结合了有监督和无监督的优点，适用于标注数据有限但无标签数据多的情况。</li>\n<li><strong>自监督学习</strong>则是一种新兴的方法，它能够从未标注的数据中学习到有用的特征表示，适用于标注数据难以获得但需要丰富特征表示的任务。</li>\n</ul>\n</blockquote>\n<p><strong>半监督学习举例</strong></p>\n<ul>\n<li>使用已标注数据训练一个分类器。</li>\n<li>用该分类器对未标注数据进行预测，生成伪标签。</li>\n<li>选择置信度最高的伪标签来扩充训练集。</li>\n<li>结合原始的标注数据和带有伪标签的数据重新训练分类器</li>\n</ul>\n<h4 id=\"对比预训练：\"><a href=\"#对比预训练：\" class=\"headerlink\" title=\"对比预训练：\"></a><strong>对比预训练</strong>：</h4><blockquote>\n<p>对比预训练是一种自监督学习方法，其核心目标是通过学习<strong>区分不同数据之间的相似性和差异性来提升模型的特征表示能力</strong>。这种方法不依赖于人工标注的数据，而是通过<strong>定义一种或多种对比任务</strong>（比如区分不同图像、文本或声音等），使得模型能够自动学习到数据的<strong>内在特征和结构</strong>。对比预训练通过比较正样本对（相似）和负样本对（不相似）来优化模型，使模型能够捕捉到数据中有用的信息和模式，从而在没有大量标注数据的情况下也能有效提升模型性能 。</p>\n<p><strong>在NLP和CV等领域，对比学习已经成为一种重要的自监督学习方法</strong>，<strong>通过自监督预训练模式</strong>，模型可以从数据本身的先验知识分布中吸取图像或文本的特征，得到一个能够更好地适应不同任务和领域的预训练模 。这种方法的优势在于<strong>无需大量标注数据</strong>，同时可以<strong>显著提高模型的泛化性能和学习能力</strong> 。</p>\n</blockquote>\n<h4 id=\"平行语料库：\"><a href=\"#平行语料库：\" class=\"headerlink\" title=\"平行语料库：\"></a><strong>平行语料库</strong>：</h4><blockquote>\n<p><strong>平行语料库是收录了某一源语言文本及其对应的目标语文本的语料库</strong>。具体来说，平行语料库包括以下内容：</p>\n<ul>\n<li><strong>源语言和目标语言的对应文本</strong>：这些文本在语义上是相同的，只是用不同的语言表达，如英文原版和中文翻译版。</li>\n<li><strong>对齐的层面</strong>：平行对齐指的是源语文本和目标语文本之间的具体单位的对应关系或翻译关系，可以细分为词汇、语句和段落等层面的对齐。</li>\n<li><strong>类型</strong>：根据所涉及的语种数量和方向，平行语料库可以分为单向平行语料库、双向平行语料库和多向平行语料库。</li>\n<li><strong>应用</strong>：平行语料库对于语言对比、双语词典编纂、机器翻译、翻译策略与规范研究等都具有很高的应用价值。</li>\n</ul>\n<p>总的来说，平行语料库不仅为机器翻译提供了必要的数据支持，也是推动相关领域发展和进步的重要驱动力。随着科技的不断进步和全球化的深入发展，平行语料库将在未来发挥更加重要的作用</p>\n</blockquote>\n<h4 id=\"联合学习joint-study：\"><a href=\"#联合学习joint-study：\" class=\"headerlink\" title=\"联合学习joint study：\"></a><strong>联合学习joint study</strong>：</h4><blockquote>\n<p><strong>联合学习（Joint Learning）是一种机器学习范式，它涉及同时训练多个模型或任务，以便它们可以共享知识或参数，从而提高整体的学习效率和泛化能力</strong>。</p>\n<p>在联合学习中，不同的模型或任务之间存在一定的关联性，这种关联性可以是因为它们处理相似的数据类型，或者因为它们解决的是相关的子问题。通过联合学习，这些模型可以相互促进，提高各自的性能。以下是一些关键点：</p>\n<ul>\n<li><strong>模型组合</strong>：在联合学习中，一个大模型由多个小模型组成，这些小模型可以独立训练，也可以与其他模型一起联合训练。</li>\n<li><strong>多任务学习</strong>：联合学习与多任务学习（Multi-Task Learning）有相似之处，多任务学习也是让一个模型同时学习多个任务，并且在学习过程中共享参数。这样做的好处是可以利用任务之间的相关性来提升模型的泛化能力。</li>\n<li><strong>应用领域</strong>：联合学习在自然语言处理（NLP）领域有广泛的应用，例如，可以通过联合模型来同时处理语法分析和语义理解等任务。</li>\n<li><strong>端到端学习</strong>：在某些情况下，联合学习还涉及到端到端的训练方法，这意味着从输入到输出的整个流程都被整合在一个学习过程中，以此来优化整体性能。</li>\n</ul>\n<p>总的来说，联合学习是一种强大的学习方法，它通过整合多个相关任务或模型的学习过程，能够提高学习效率和模型的泛化能力。这种方法在处理复杂问题时特别有用，因为它可以有效地利用不同任务之间的共同信息。</p>\n</blockquote>\n<h3 id=\"4-衡量模型好坏的一些指标\"><a href=\"#4-衡量模型好坏的一些指标\" class=\"headerlink\" title=\"4 衡量模型好坏的一些指标\"></a>4 衡量模型好坏的一些指标</h3><h4 id=\"F1-score-精确率precision-召回率re-call\"><a href=\"#F1-score-精确率precision-召回率re-call\" class=\"headerlink\" title=\"F1 score 精确率precision 召回率re-call\"></a>F1 score 精确率precision 召回率re-call</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-3.png\" class=\"lazyload placeholder\" data-srcset=\"/%5Cimg%5Cai-mianjing%5Cpic-3.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-4.png\" class=\"lazyload placeholder\" data-srcset=\"/%5Cimg%5Cai-mianjing%5Cpic-4.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h4 id=\"IOU-mIOU\"><a href=\"#IOU-mIOU\" class=\"headerlink\" title=\"IOU mIOU\"></a>IOU mIOU</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-5.png\" class=\"lazyload placeholder\" data-srcset=\"/%5Cimg%5Cai-mianjing%5Cpic-5.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"机器学习20道\"><a href=\"#机器学习20道\" class=\"headerlink\" title=\"机器学习20道\"></a>机器学习20道</h3><ol>\n<li><p>网络配置时batchsize的大小怎样设置？过小和过大分别有什么特点？ <strong>小的不稳定 大的速度快</strong></p>\n<blockquote>\n<ul>\n<li>大的batchsize <strong>减少训练时间，提高稳定性</strong>。 这是肯定的，同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以<strong>减少训练时间</strong>。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。</li>\n<li>过大的batchsize 泛化能力下降。而<strong>过大的批量大小会导致收敛速度变慢</strong>，并且可能需要更多的内存 在一定范围内，<strong>增加batchsize有助于收敛的稳定性</strong>，但是随着batchsize的增加，<strong>模型的性能会下降</strong>。</li>\n<li>同样是通过对训练步数的影响，小的batch_size使模型迭代次数增多，<strong>提前到达拟合点</strong>，但是epoch没结束，继续学习训练数据，容易导致过拟合于原始数据</li>\n</ul>\n</blockquote>\n</li>\n<li><p>设置学习率衰减的原因？</p>\n<blockquote>\n<p>学习率衰减是为了让学习率随着训练过程逐渐减小，有助于模型<strong>在接近全局最优解时减小步长</strong>，避免在优化过程中跳过最佳点或产生不必要的波动</p>\n</blockquote>\n</li>\n<li><p>有哪些分类算法？</p>\n<blockquote>\n<p>朴素贝叶斯、逻辑回归、K 最近邻 （KNN）、支持向量机 （SVM）、决策树、随机森林和神经网络</p>\n</blockquote>\n</li>\n<li><p>分类和回归的区别？</p>\n<blockquote>\n<p>分类预测<strong>离散标签</strong>，将数据分类为两个或多个类别。回归预测<strong>连续量</strong>，估计变量之间的关系</p>\n</blockquote>\n</li>\n<li><p>请描述下k-means聚类的过程？</p>\n<blockquote>\n<p>k-means聚类是一种<strong>迭代算法</strong>，首先随机选取k个初始中心点，然后<strong>将每个点分配到最近的中心点形成的簇</strong>中，接着重新计算每个簇的中心点，重复此过程直到中心点不再变化或达到预设的迭代次数。</p>\n</blockquote>\n</li>\n<li><p>训练集、测试集、验证集的作用？</p>\n<blockquote>\n<p>数据集的作用：<strong>训练集用于训练模型</strong>，<strong>验证集用于调整超参数</strong>并在训练过程中提供无偏评估，<strong>测试集</strong>用于评估模型的<strong>泛化性能</strong></p>\n<ol>\n<li><strong>评估偏差</strong>: 如果我们<strong>直接根据测试集调整超参数，那么测试集就不再是独立的评估数据集</strong>。这意味着我们无法准确地估计模型在新数据上的性能，因为测试集已经被用来调整模型了。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>请讲解一下k折交叉验证？</p>\n<blockquote>\n<p>在 k 折叠交叉验证中，数据被划分为 k 个子集。每次，将 k 个子集中的一个用作测试集，并将其他 k-1 子集放在一起形成一个训练集。<strong>该过程重复 k 次，每个 k 个子集恰好用作测试集一次</strong>。</p>\n</blockquote>\n</li>\n<li><p>分类和聚类的区别？</p>\n<blockquote>\n<p>分类是<strong>监督学习任务</strong>，需要<strong>标注数据</strong>来训练模型；聚类是<strong>无监督学习</strong>任务，不需要标注数据，目的是发现数据<strong>内在的结构或模式。</strong></p>\n</blockquote>\n</li>\n<li><p>描述一下梯度的概念？</p>\n<blockquote>\n<p>梯度是导数的多变量推广，表示<strong>函数最快增长的方向和速率</strong>。在优化中，它用于查找函数减小最快的方向。</p>\n<p>eg:</p>\n<p>例如，考虑二元函数 f(x, y) &#x3D; x^2 + y^2。我们可以计算它的梯度：</p>\n<ol>\n<li>计算偏导数：∂f&#x2F;∂x &#x3D; 2x，∂f&#x2F;∂y &#x3D; 2y</li>\n<li>在点 (x0, y0) &#x3D; (1, 1) 处，梯度向量为：∇f(1, 1) &#x3D; [2<em>1, 2</em>1] &#x3D; [2, 2]</li>\n</ol>\n<p>因此，在点 (1, 1) 处，梯度向量指向 [2, 2] 方向，且函数在该方向上的增长率最大，为 √(2^2 + 2^2) &#x3D; √8 ≈ 2.83。这意味着在点 (1, 1) 处，函数 f(x, y) &#x3D; x^2 + y^2 沿着 [2, 2] 方向增长最快，且每单位距离增长 2.83。</p>\n</blockquote>\n</li>\n<li><p>有监督学习、无监督学习和半监督学习的区别？</p>\n<blockquote>\n<p>见 1.1 节</p>\n</blockquote>\n</li>\n<li><p>带核的SVM为什么能分类非线性问题？</p>\n<blockquote>\n<p>非线性问题的 SVM：SVM 可以通过使用内核技巧<strong>将输入空间转换为更高维的空间</strong>来对非线性问题进行分类，从而更容易线性分离数据。</p>\n</blockquote>\n</li>\n<li><p>请描述常见的梯度下降方法？</p>\n<blockquote>\n<p>常用的方法包括批量梯度下降、随机梯度下降 （SGD） 和小批量梯度下降，用于计算损失函数梯度的数据量不同。</p>\n<p>批量梯度下降、随机梯度下降（SGD）和小批量梯度下降是梯度下降优化算法的三种不同实现方式，它们在机器学习和深度学习中被广泛使用来优化模 型的参数。<strong>具体介绍如下</strong>：</p>\n<ul>\n<li><strong>批量梯度下降</strong>：它在每次迭代时<strong>使用所有的训练样本来计算梯度</strong>并更新模型参数。这种方法可以确保每次迭代都沿着全局最优方向更新，因此收敛曲线通常比较平滑。然而，当数据集非常大时，每次迭代的计算成本会非常高，导致训练速度变慢。</li>\n<li><strong>随机梯度下降</strong>：它在<strong>每次迭代时只使用一个训练样本来计算梯度并更新模型参数</strong>。这种方法的优点是计算速度快，可以快速进行模型更新，特别适用于大数据集。但是，由于每次更新只基于一个样本，可能会导致收敛过程波动较大，不一定能收敛到全局最优解。</li>\n<li><strong>小批量梯度下降</strong>：它<strong>介于批量梯度下降和随机梯度下降之间</strong>，每次迭代<strong>使用一小部分训练样本</strong>（即小批量）来计算梯度并更新模型参数。这种方法既保留了批量梯度下降的稳定收敛特性，又具有随机梯度下降的快速计算优势。因此，小批量梯度下降在实践中被广泛应用，尤其是在深度学习模型的训练中。</li>\n</ul>\n<p>总的来说，这三种方法都是梯度下降的变体，目的是通过迭代更新模型参数以最小化损失函数。批量梯度下降使用所有数据，收敛稳定但计算成本高；随机梯度下降使用单个样本，计算快但可能波动大；小批量梯度下降折中了前两者的特点，使用部分数据，既稳定又相对高效。</p>\n</blockquote>\n</li>\n<li><p>Adam、RMSprop、Adagrad、Momentum优化算法？</p>\n<blockquote>\n<p><strong>Adam</strong> 将 <strong>RMSprop 和 Stochastic Gradient Descent 的思想与动量相结合</strong>。它动态调整每个参数的学习率。RMSprop 在训练期间调整学习率，而 Adagrad 通过将学习率缩放为与梯度的所有过去平方值的平方根成反比来调整学习率。动量有助于加速新元朝相关方向发展。</p>\n<p>此方法是在<strong>梯度下降的基础上加入了“惯性”的概念</strong>，即考虑了历史梯度对当前梯度的影响，使得更新过程具有一定的连续性，并能够加快学习速度，减少振荡。Momentum 通过使用动量项来对网络参数进行平滑处理，有助于使梯度的摆动幅度变小，从而加快收敛速度。</p>\n</blockquote>\n</li>\n<li><p>什么是过拟合？怎么解决过拟合问题？</p>\n<blockquote>\n<p>过拟合是指模型在训练数据上表现良好但在新数据上泛化能力差的现象。</p>\n<p><strong>解决过拟合的方法</strong>包括增加<strong>数据量</strong>、减少模型<strong>复杂度</strong>、使用<strong>正则化</strong>技术和集成学习方法等。</p>\n</blockquote>\n</li>\n<li><p>怎样解决梯度消失&#x2F;爆炸问题？</p>\n<blockquote>\n<p><strong>含义</strong>：</p>\n<ol>\n<li><strong>梯度消失</strong>：在深度神经网络的反向传播过程中，梯度可能会因为多层连续的乘法操作而变得非常小，以至于权重几乎不会被更新，导致训练过程提前停止。这种现象通常发生在激活函数选择不当（如使用sigmoid或tanh函数）且网络层次较深的情况下。</li>\n<li><strong>梯度爆炸</strong>：与梯度消失相反，梯度爆炸是指梯度值变得非常大，以至于导致模型参数更新过于频繁和巨大，从而使得模型无法收敛到一个稳定的解。梯度爆炸通常是由于梯度值在反向传播过程中连续乘以较大的数而累积起来的。</li>\n</ol>\n<p>如何解决？</p>\n<ul>\n<li><strong>选择合适的激活函数</strong>：<strong>避免</strong>使用<strong>容易饱和</strong>的激活函数，如sigmoid或tanh，转而使用ReLU（Rectified Linear Units）及其变种Leaky ReLU、Parametric ReLU等，它们在输入值较大时不会饱和，有助于缓解梯度消失问题。</li>\n<li><strong>批量归一化（Batch Normalization）</strong>：通过对每一层的输入进行归一化处理，保持输入数据的均值为0、方差为1，有助于稳定梯度变化，防止梯度消失和爆炸。</li>\n<li><strong>残差连接（Residual Connections）</strong>：在深度网络中使用残差连接，即让前一层的输出直接加到后续某层的输入上，可以保证梯度能够绕过一些层直接传回，从而缓解梯度消失问题。</li>\n<li><strong>梯度裁剪（Gradient Clipping）</strong>：通过设置一个阈值来限制梯度的最大值，可以有效防止梯度爆炸。</li>\n<li><strong>优化器选择</strong>：使用具有自适应学习率调整能力的优化器，如Adam、RMSprop等，可以在不同情况下自动调整学习率，减少梯度消失或爆炸的风险。</li>\n<li><strong>短程记忆结构</strong>：对于循环神经网络（RNN），可以<strong>使用长短时记忆（LSTM）</strong>或门控循环单元（GRU）这类结构来代替传统的RNN结构，因为它们能够更好地捕捉长距离依赖关系，从而缓解梯度消失问题。</li>\n<li><strong>正则化技术</strong>：如L1&#x2F;L2正则化，也可以帮助控制模型参数的规模，间接地减小梯度爆炸的可能性。</li>\n</ul>\n</blockquote>\n</li>\n<li><p>讲解下神经网络反向传播算法？</p>\n<blockquote>\n<p><strong>反向传播算法</strong>：反向传播是人工神经网络中使用的一种方法，用于计算单个输入输出示例的损失函数相对于网络权重的梯度，从而使用梯度下降更新权重。</p>\n</blockquote>\n</li>\n<li><p>激活函数的作用是什么？有哪些激活函数？它们的表达式分别是？</p>\n<blockquote>\n<p>激活函数作用：引入<strong>非线性</strong>因素，使得神经网络可以学习和模拟任何复杂的函数和数据分布，这样网络就能处理非线性问题。如果<strong>没有激活函数，无论神经网络有多少层，输出都只是输入的线性组合</strong>，这大大限制了网络的表达能力和复杂度。</p>\n<p>常见的激活函数包括 Sigmoid、Tanh、ReLU：（需要去了解一下基本原理）</p>\n</blockquote>\n</li>\n<li><p>请讲解一下正则化的概念？L1正则化是什么？L2正则化是什么？</p>\n<blockquote>\n<p>正则化是机器学习和统计学中用于<strong>防止模型过拟合</strong>的一种技术。过拟合是指模型在训练数据上表现得很好，但在未见过的新数据上表现不佳的现象，通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是潜在的数据生成规律。<strong>正则化通过在模型训练的损失函数中添加一个额外的惩罚项来解决这个问题，该惩罚项会惩罚模型的复杂度</strong></p>\n<ul>\n<li><strong>L1正则化</strong>（也称为Lasso正则化）通过向损失函数添加参数的绝对值之和作为惩罚项来工作。L1正则化倾向于产生稀疏参数，即许多参数会变成零，这可以用于特征选择。</li>\n<li><strong>L2正则化</strong>（也称为Ridge正则化）通过向损失函数添加参数的平方和作为惩罚项来工作。与L1正则化不同，L2正则化倾向于使参数非常小但不完全为零，从而保持所有特征但减少其影响。</li>\n<li>在某些情况下，人们可能会<strong>同时使用L1和L2正则化</strong>，这种方法被称为<strong>弹性网络</strong>正则化。</li>\n<li>正则化技术不仅限于线性模型，它们也广泛应用于神经网络、决策树和其他类型的机器学习模型中。在神经网络中，除了L1和L2正则化外，还有如<strong>Dropout</strong>正则化等特殊形式的正则化技术，它通过在训练过程中随机丢弃一部分神经元来防止过拟合。</li>\n</ul>\n</blockquote>\n</li>\n<li><p>除了BatchNormalization还有其他什么方法来加速模型训练吗？</p>\n<blockquote>\n<ol>\n<li>使用<strong>半字训练（半精度浮点数）</strong></li>\n<li><strong>合理的超参数设计</strong>：选择合适的batch size、epoch数量和学习率策略对于加速模型训练至关重要。较小的batch size可以加快单次迭代的速度，而适当的epoch数量和学习率调整可以帮助模型更快地收敛。</li>\n<li><strong>增大batch size</strong>：虽然增大batch size可以提高训练速度，但过大的batch size可能会影响模型的泛化性能和收敛速度。因此，需要根据具体情况选择合适的batch size大小。</li>\n<li><strong>多GPU分布式训练</strong>：通过使用多个GPU进行模型并行或数据并行训练，可以显著提高模型的训练速度。模型并行是将模型的不同部分分配给不同的计算设备，而数据并行则是将数据集分割成多个子集，每个子集在不同的计算设备上进行处理。</li>\n<li><strong>模型压缩与加速技术</strong>：包括参数剪枝、参数量化、紧凑网络设计、知识蒸馏、低秩分解、参数共享等方法。这些技术旨在减少模型的大小和复杂性，从而提高训练和推理的速度。</li>\n<li><strong>使用预训练模型</strong>：在相似任务上使用预训练模型可以显著减少训练时间，因为预训练模型已经学习了大量的特征，只需要对顶层进行微调即可。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>机器学习中，为何要经常对数据进行预处理？</p>\n<blockquote>\n<p>处理噪声、缺失值、降维、结构化非结构化处理</p>\n<ol>\n<li><strong>处理缺失值与噪声</strong>：数据中可能存在缺失值，预处理可以包括填补或删除这些缺失值，以确保数据的完整性。</li>\n<li><strong>无量纲化</strong>：不同特征可能具有不同的量纲和规模，无量纲化处理可以消除这种差异，使得模型能够更公平地评估每个特征的重要性。</li>\n<li><strong>数据规范化</strong>：通过最值归一化或Z-Score规范化等方法，将数据缩放到特定的范围内，有助于加快模型的收敛速度。</li>\n<li><strong>特征工程</strong>：数据预处理是特征工程的一部分，它包括数据清洗、特征提取、特征选择和特征构造等子问题。通过这些步骤，可以创建出能够使机器学习算法达到最佳性能的特征。</li>\n<li><strong>提高模型性能</strong>：干净且经过恰当处理的数据可以帮助算法更准确地学习模式和关系，从而提高模型的性能。</li>\n<li><strong>增强模型泛化能力</strong>：通过正则化等预处理方法，可以防止模型过拟合，提高其在新数据上的泛化能力。</li>\n</ol>\n</blockquote>\n</li>\n</ol>\n","more":"<h3 id=\"1-机器学习、深度学习、与人工智能对比\"><a href=\"#1-机器学习、深度学习、与人工智能对比\" class=\"headerlink\" title=\"1 机器学习、深度学习、与人工智能对比\"></a>1 机器学习、深度学习、与人工智能对比</h3><ul>\n<li><strong>机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术</strong></li>\n</ul>\n<blockquote>\n<p><strong>人工智能</strong>（人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器）</p>\n<p><strong>分类</strong>： 弱人工智能（当前）、强人工智能、超人工智能</p>\n<p><strong>机器学习</strong>（MachineLearning）简称ML：机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。</p>\n<p><strong>深度学习</strong>（DeepLearning）简称DL。<strong>最初</strong>的深度学习是<strong>利用深度神经网络</strong>来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于<strong>建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本</strong>。</p>\n</blockquote>\n<h4 id=\"1-1-人工智能\"><a href=\"#1-1-人工智能\" class=\"headerlink\" title=\"1.1 人工智能\"></a>1.1 人工智能</h4><blockquote>\n<p><strong>人工智能（Artificial intelligence）简称AI</strong>。人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。</p>\n<ul>\n<li><p>人工智能目前分为弱人工智能和强人工智能和超人工智能：</p>\n</li>\n<li><p><strong>弱人工智能</strong>：弱人工智能（Artificial Narrow Intelligence &#x2F;ANI),只专注于完成某个特定的任务，例如语音识别、图象识别和翻译等，是擅长于单个方面的人工智能。它们只是用于解决特定的具体类的任务问题而存在，大都是统计数据，以此从中归纳出模型。由于弱人工智能智能处理较为单一的问题，且发展程度并没有达到模拟人脑思维的程度，所以弱人工智能仍然属于“工具”的范畴，与传统的“产品”在本质上并无区别。</p>\n</li>\n<li><p><strong>强人工智能</strong>：强人工智能（Artificial General lnteligence &#x2F;AGI),属于人类级别的人工智能，在各方面都能和人类比肩，它能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作，并且和人类一样得心应手。</p>\n</li>\n<li><p><strong>超人工智能</strong>：超人工智能（Artificial Superintelligence&#x2F;ASI），在几乎所有领域都比最聪明的人类大脑都聪明许多，包括科学创新、通识和社交技能。在超人工智能阶段，人工智能已经跨过“奇点”，其计算和思维能力已经远超人脑。此时的人工智能已经不是人类可以理解和想象。人工智能将打破人脑受到的维度限制，其所观察和思考的内容，人脑已经无法理解，人工智能将形成一个新的社会。</p>\n</li>\n</ul>\n<p><strong>目前我们仍处于弱人工智能阶段</strong>。</p>\n</blockquote>\n<h4 id=\"1-2-机器学习\"><a href=\"#1-2-机器学习\" class=\"headerlink\" title=\"1.2 机器学习\"></a>1.2 机器学习</h4><blockquote>\n<p><strong>机器学习（Machine Learning）简称ML</strong>。机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。</p>\n</blockquote>\n<h4 id=\"1-3-深度学习\"><a href=\"#1-3-深度学习\" class=\"headerlink\" title=\"1.3 深度学习\"></a>1.3 深度学习</h4><blockquote>\n<p><strong>深度学习（Deep Learning）简称DL</strong>。最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本。</p>\n<p>注意：你可能在接触深度学习的时候也听到过<strong>监督学习</strong>、<strong>非监督学习</strong>、<strong>半监督学习</strong>等概念，下面就顺便对这三个名词解析下：</p>\n<ol>\n<li><strong>监督学习：</strong>用一部分已知分类、有标记的样本来训练机器后，让它用学到的特征，对没有还分类、无标记的样本进行分类、贴标签。多用于分类。</li>\n<li><strong>非监督学习：</strong>所有的数据没有标记，类别未知，让它自己学习样本之间的相似性来进行分类。多用于聚类。</li>\n<li><strong>半监督学习：</strong>有两个样本集，一个有标记，一个没有标记。综合利用有类标的样本（ labeled sample）和没有类标的样本（ unlabeled sample），来生成合适的分类。</li>\n</ol>\n</blockquote>\n<h4 id=\"1-4-区别与联系\"><a href=\"#1-4-区别与联系\" class=\"headerlink\" title=\"1.4 区别与联系\"></a>1.4 区别与联系</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-1.png\" alt=\"img\"></p>\n<ul>\n<li>人工智能的研究领域包括 <strong>专家系统</strong>(Expert Systems)、<strong>机器学习</strong>(Machine Learning)、<strong>模式识别</strong>、<strong>进化计算</strong>(Evolutionary Computation)、<strong>模糊逻辑</strong>(Fussy Logic)、<strong>计算机视觉</strong>(Computer Vision)、<strong>自然语言处理</strong>(NLP)、<strong>推荐系统</strong>(Recommender Systems)</li>\n<li>深度学习，一种<strong>实现机器学习</strong>的技术。是机器学习中的一个方法或者分支。</li>\n<li><strong>深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术</strong>。</li>\n<li>它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和<a href=\"https://so.csdn.net/so/search?q=%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86&spm=1001.2101.3001.7020\">自然语言处理</a>(NLP)领域。显然，<strong>“深度学习”是与机器学习中的“神经网络”是强相关</strong>，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。</li>\n<li><strong>总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU</strong></li>\n</ul>\n<p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-2.png\" alt=\"img\"></p>\n<h3 id=\"2-AI的分类\"><a href=\"#2-AI的分类\" class=\"headerlink\" title=\"2 AI的分类\"></a>2 AI的分类</h3><ul>\n<li><p><strong>模式识别</strong>：</p>\n<ul>\n<li><p><strong>模式识别是指用计算机对物体进行识别。</strong></p>\n<p><strong>物体指文字、符号、图形、图像、语音、声音等实体对象；</strong></p>\n<p>•<strong>不包括概念、思想、意识等虚拟对象，它们属于认知和哲学研究范畴。</strong></p>\n</li>\n<li><p><strong>模式识别应用：</strong></p>\n<p>• <strong>文字识别，车牌识别，人脸识别，视网膜识别，指纹识别，掌纹识别等</strong></p>\n</li>\n</ul>\n</li>\n<li><p><strong>自然语言处理</strong>：用自然语言同计算机进行通讯的一种技术</p>\n<ul>\n<li>机器翻译</li>\n<li>语音识别（<strong>可以单列</strong>）</li>\n<li>文本分类&#x2F;预测</li>\n</ul>\n</li>\n<li><p><strong>机器学习</strong>（重点是深度学习）：机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法</p>\n</li>\n<li><p><strong>计算机视觉</strong>：用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像</p>\n</li>\n<li><p><strong>智能机器人</strong>：给机器人装上“大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信</p>\n<p>息的综合处理等方面将会前进一大步</p>\n</li>\n<li><p><strong>自动程序设计</strong>：自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序</p>\n</li>\n<li><p><strong>数据挖掘</strong>：从大量的数据中搜索隐藏在其中信息的过程</p>\n</li>\n<li><p><strong>专家系统</strong>等</p>\n</li>\n</ul>\n<h3 id=\"3-一些深度学习概念\"><a href=\"#3-一些深度学习概念\" class=\"headerlink\" title=\"3 一些深度学习概念\"></a>3 一些深度学习概念</h3><h4 id=\"监督学习基础定义：\"><a href=\"#监督学习基础定义：\" class=\"headerlink\" title=\"监督学习基础定义：\"></a><strong>监督学习基础定义</strong>：</h4><p><strong>有监督学习 (Supervised Learning)</strong>: 使用<strong>已经标注的数据</strong>训练模型，目的是让模型学会如何将输入映射到期望的输出。例如，使用一组已知类别的图片来训练一个图像<strong>分类模型</strong>，模型学习如何根据输入的图片预测正确的类别 。</p>\n<p><strong>无监督学习 (Unsupervised Learning)</strong>: <strong>不使用任何标注数据</strong>，让模型自己发现数据中的结构和模式。例如，<strong>聚类</strong>算法可以将数据分为几个不同的组，而不事先告诉模型应该有多少组，或者各组的特征 。</p>\n<p><strong>半监督学习 (Semi-Supervised Learning)</strong>: 结合<strong>有监督和无监督学习</strong>的方法，使用<strong>少量标注数据和大量未标注数据</strong>共同训练模型。这种方法在标注数据难以获得但未标注数据丰富的场景下特别有用 。 例如：<strong>医学影像</strong>，大部分没有标签，人工打标较为昂贵。可以用</p>\n<ul>\n<li>使用已标注数据训练一个分类器。</li>\n<li>用该分类器对未标注数据进行预测，生成伪标签。</li>\n<li>选择置信度最高的伪标签来扩充训练集。</li>\n<li>结合原始的标注数据和带有伪标签的数据重新训练分类器</li>\n</ul>\n<p><strong>自监督学习 (Self-Supervised Learning)</strong>: 一种特殊的无监督学习方法，通过<strong>自动生成伪标签</strong>来训练模型。这种方法通过创建预测任务（如预测未来的帧、填充缺失的部分、一个自监督学习模型可以通过尝试预测句子中缺失的单词来学习语言的语法和词汇等），使模型能够从输入数据本身学习有用的表示 。 通常是<strong>预训练+下游任务</strong>的方式。</p>\n<p><strong>对比</strong>：</p>\n<blockquote>\n<ul>\n<li><strong>有监督学习</strong>通常需要大量的标注数据，适用于标注数据充足的任务。</li>\n<li><strong>无监督学习</strong>不需要标签，适用于探索数据内在结构和关系的任务。</li>\n<li><strong>半监督学习</strong>结合了有监督和无监督的优点，适用于标注数据有限但无标签数据多的情况。</li>\n<li><strong>自监督学习</strong>则是一种新兴的方法，它能够从未标注的数据中学习到有用的特征表示，适用于标注数据难以获得但需要丰富特征表示的任务。</li>\n</ul>\n</blockquote>\n<p><strong>半监督学习举例</strong></p>\n<ul>\n<li>使用已标注数据训练一个分类器。</li>\n<li>用该分类器对未标注数据进行预测，生成伪标签。</li>\n<li>选择置信度最高的伪标签来扩充训练集。</li>\n<li>结合原始的标注数据和带有伪标签的数据重新训练分类器</li>\n</ul>\n<h4 id=\"对比预训练：\"><a href=\"#对比预训练：\" class=\"headerlink\" title=\"对比预训练：\"></a><strong>对比预训练</strong>：</h4><blockquote>\n<p>对比预训练是一种自监督学习方法，其核心目标是通过学习<strong>区分不同数据之间的相似性和差异性来提升模型的特征表示能力</strong>。这种方法不依赖于人工标注的数据，而是通过<strong>定义一种或多种对比任务</strong>（比如区分不同图像、文本或声音等），使得模型能够自动学习到数据的<strong>内在特征和结构</strong>。对比预训练通过比较正样本对（相似）和负样本对（不相似）来优化模型，使模型能够捕捉到数据中有用的信息和模式，从而在没有大量标注数据的情况下也能有效提升模型性能 。</p>\n<p><strong>在NLP和CV等领域，对比学习已经成为一种重要的自监督学习方法</strong>，<strong>通过自监督预训练模式</strong>，模型可以从数据本身的先验知识分布中吸取图像或文本的特征，得到一个能够更好地适应不同任务和领域的预训练模 。这种方法的优势在于<strong>无需大量标注数据</strong>，同时可以<strong>显著提高模型的泛化性能和学习能力</strong> 。</p>\n</blockquote>\n<h4 id=\"平行语料库：\"><a href=\"#平行语料库：\" class=\"headerlink\" title=\"平行语料库：\"></a><strong>平行语料库</strong>：</h4><blockquote>\n<p><strong>平行语料库是收录了某一源语言文本及其对应的目标语文本的语料库</strong>。具体来说，平行语料库包括以下内容：</p>\n<ul>\n<li><strong>源语言和目标语言的对应文本</strong>：这些文本在语义上是相同的，只是用不同的语言表达，如英文原版和中文翻译版。</li>\n<li><strong>对齐的层面</strong>：平行对齐指的是源语文本和目标语文本之间的具体单位的对应关系或翻译关系，可以细分为词汇、语句和段落等层面的对齐。</li>\n<li><strong>类型</strong>：根据所涉及的语种数量和方向，平行语料库可以分为单向平行语料库、双向平行语料库和多向平行语料库。</li>\n<li><strong>应用</strong>：平行语料库对于语言对比、双语词典编纂、机器翻译、翻译策略与规范研究等都具有很高的应用价值。</li>\n</ul>\n<p>总的来说，平行语料库不仅为机器翻译提供了必要的数据支持，也是推动相关领域发展和进步的重要驱动力。随着科技的不断进步和全球化的深入发展，平行语料库将在未来发挥更加重要的作用</p>\n</blockquote>\n<h4 id=\"联合学习joint-study：\"><a href=\"#联合学习joint-study：\" class=\"headerlink\" title=\"联合学习joint study：\"></a><strong>联合学习joint study</strong>：</h4><blockquote>\n<p><strong>联合学习（Joint Learning）是一种机器学习范式，它涉及同时训练多个模型或任务，以便它们可以共享知识或参数，从而提高整体的学习效率和泛化能力</strong>。</p>\n<p>在联合学习中，不同的模型或任务之间存在一定的关联性，这种关联性可以是因为它们处理相似的数据类型，或者因为它们解决的是相关的子问题。通过联合学习，这些模型可以相互促进，提高各自的性能。以下是一些关键点：</p>\n<ul>\n<li><strong>模型组合</strong>：在联合学习中，一个大模型由多个小模型组成，这些小模型可以独立训练，也可以与其他模型一起联合训练。</li>\n<li><strong>多任务学习</strong>：联合学习与多任务学习（Multi-Task Learning）有相似之处，多任务学习也是让一个模型同时学习多个任务，并且在学习过程中共享参数。这样做的好处是可以利用任务之间的相关性来提升模型的泛化能力。</li>\n<li><strong>应用领域</strong>：联合学习在自然语言处理（NLP）领域有广泛的应用，例如，可以通过联合模型来同时处理语法分析和语义理解等任务。</li>\n<li><strong>端到端学习</strong>：在某些情况下，联合学习还涉及到端到端的训练方法，这意味着从输入到输出的整个流程都被整合在一个学习过程中，以此来优化整体性能。</li>\n</ul>\n<p>总的来说，联合学习是一种强大的学习方法，它通过整合多个相关任务或模型的学习过程，能够提高学习效率和模型的泛化能力。这种方法在处理复杂问题时特别有用，因为它可以有效地利用不同任务之间的共同信息。</p>\n</blockquote>\n<h3 id=\"4-衡量模型好坏的一些指标\"><a href=\"#4-衡量模型好坏的一些指标\" class=\"headerlink\" title=\"4 衡量模型好坏的一些指标\"></a>4 衡量模型好坏的一些指标</h3><h4 id=\"F1-score-精确率precision-召回率re-call\"><a href=\"#F1-score-精确率precision-召回率re-call\" class=\"headerlink\" title=\"F1 score 精确率precision 召回率re-call\"></a>F1 score 精确率precision 召回率re-call</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-3.png\" alt=\"img\"></p>\n<p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-4.png\" alt=\"img\"></p>\n<h4 id=\"IOU-mIOU\"><a href=\"#IOU-mIOU\" class=\"headerlink\" title=\"IOU mIOU\"></a>IOU mIOU</h4><p><img src=\"/%5Cimg%5Cai-mianjing%5Cpic-5.png\" alt=\"img\"></p>\n<h3 id=\"机器学习20道\"><a href=\"#机器学习20道\" class=\"headerlink\" title=\"机器学习20道\"></a>机器学习20道</h3><ol>\n<li><p>网络配置时batchsize的大小怎样设置？过小和过大分别有什么特点？ <strong>小的不稳定 大的速度快</strong></p>\n<blockquote>\n<ul>\n<li>大的batchsize <strong>减少训练时间，提高稳定性</strong>。 这是肯定的，同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以<strong>减少训练时间</strong>。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。</li>\n<li>过大的batchsize 泛化能力下降。而<strong>过大的批量大小会导致收敛速度变慢</strong>，并且可能需要更多的内存 在一定范围内，<strong>增加batchsize有助于收敛的稳定性</strong>，但是随着batchsize的增加，<strong>模型的性能会下降</strong>。</li>\n<li>同样是通过对训练步数的影响，小的batch_size使模型迭代次数增多，<strong>提前到达拟合点</strong>，但是epoch没结束，继续学习训练数据，容易导致过拟合于原始数据</li>\n</ul>\n</blockquote>\n</li>\n<li><p>设置学习率衰减的原因？</p>\n<blockquote>\n<p>学习率衰减是为了让学习率随着训练过程逐渐减小，有助于模型<strong>在接近全局最优解时减小步长</strong>，避免在优化过程中跳过最佳点或产生不必要的波动</p>\n</blockquote>\n</li>\n<li><p>有哪些分类算法？</p>\n<blockquote>\n<p>朴素贝叶斯、逻辑回归、K 最近邻 （KNN）、支持向量机 （SVM）、决策树、随机森林和神经网络</p>\n</blockquote>\n</li>\n<li><p>分类和回归的区别？</p>\n<blockquote>\n<p>分类预测<strong>离散标签</strong>，将数据分类为两个或多个类别。回归预测<strong>连续量</strong>，估计变量之间的关系</p>\n</blockquote>\n</li>\n<li><p>请描述下k-means聚类的过程？</p>\n<blockquote>\n<p>k-means聚类是一种<strong>迭代算法</strong>，首先随机选取k个初始中心点，然后<strong>将每个点分配到最近的中心点形成的簇</strong>中，接着重新计算每个簇的中心点，重复此过程直到中心点不再变化或达到预设的迭代次数。</p>\n</blockquote>\n</li>\n<li><p>训练集、测试集、验证集的作用？</p>\n<blockquote>\n<p>数据集的作用：<strong>训练集用于训练模型</strong>，<strong>验证集用于调整超参数</strong>并在训练过程中提供无偏评估，<strong>测试集</strong>用于评估模型的<strong>泛化性能</strong></p>\n<ol>\n<li><strong>评估偏差</strong>: 如果我们<strong>直接根据测试集调整超参数，那么测试集就不再是独立的评估数据集</strong>。这意味着我们无法准确地估计模型在新数据上的性能，因为测试集已经被用来调整模型了。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>请讲解一下k折交叉验证？</p>\n<blockquote>\n<p>在 k 折叠交叉验证中，数据被划分为 k 个子集。每次，将 k 个子集中的一个用作测试集，并将其他 k-1 子集放在一起形成一个训练集。<strong>该过程重复 k 次，每个 k 个子集恰好用作测试集一次</strong>。</p>\n</blockquote>\n</li>\n<li><p>分类和聚类的区别？</p>\n<blockquote>\n<p>分类是<strong>监督学习任务</strong>，需要<strong>标注数据</strong>来训练模型；聚类是<strong>无监督学习</strong>任务，不需要标注数据，目的是发现数据<strong>内在的结构或模式。</strong></p>\n</blockquote>\n</li>\n<li><p>描述一下梯度的概念？</p>\n<blockquote>\n<p>梯度是导数的多变量推广，表示<strong>函数最快增长的方向和速率</strong>。在优化中，它用于查找函数减小最快的方向。</p>\n<p>eg:</p>\n<p>例如，考虑二元函数 f(x, y) &#x3D; x^2 + y^2。我们可以计算它的梯度：</p>\n<ol>\n<li>计算偏导数：∂f&#x2F;∂x &#x3D; 2x，∂f&#x2F;∂y &#x3D; 2y</li>\n<li>在点 (x0, y0) &#x3D; (1, 1) 处，梯度向量为：∇f(1, 1) &#x3D; [2<em>1, 2</em>1] &#x3D; [2, 2]</li>\n</ol>\n<p>因此，在点 (1, 1) 处，梯度向量指向 [2, 2] 方向，且函数在该方向上的增长率最大，为 √(2^2 + 2^2) &#x3D; √8 ≈ 2.83。这意味着在点 (1, 1) 处，函数 f(x, y) &#x3D; x^2 + y^2 沿着 [2, 2] 方向增长最快，且每单位距离增长 2.83。</p>\n</blockquote>\n</li>\n<li><p>有监督学习、无监督学习和半监督学习的区别？</p>\n<blockquote>\n<p>见 1.1 节</p>\n</blockquote>\n</li>\n<li><p>带核的SVM为什么能分类非线性问题？</p>\n<blockquote>\n<p>非线性问题的 SVM：SVM 可以通过使用内核技巧<strong>将输入空间转换为更高维的空间</strong>来对非线性问题进行分类，从而更容易线性分离数据。</p>\n</blockquote>\n</li>\n<li><p>请描述常见的梯度下降方法？</p>\n<blockquote>\n<p>常用的方法包括批量梯度下降、随机梯度下降 （SGD） 和小批量梯度下降，用于计算损失函数梯度的数据量不同。</p>\n<p>批量梯度下降、随机梯度下降（SGD）和小批量梯度下降是梯度下降优化算法的三种不同实现方式，它们在机器学习和深度学习中被广泛使用来优化模 型的参数。<strong>具体介绍如下</strong>：</p>\n<ul>\n<li><strong>批量梯度下降</strong>：它在每次迭代时<strong>使用所有的训练样本来计算梯度</strong>并更新模型参数。这种方法可以确保每次迭代都沿着全局最优方向更新，因此收敛曲线通常比较平滑。然而，当数据集非常大时，每次迭代的计算成本会非常高，导致训练速度变慢。</li>\n<li><strong>随机梯度下降</strong>：它在<strong>每次迭代时只使用一个训练样本来计算梯度并更新模型参数</strong>。这种方法的优点是计算速度快，可以快速进行模型更新，特别适用于大数据集。但是，由于每次更新只基于一个样本，可能会导致收敛过程波动较大，不一定能收敛到全局最优解。</li>\n<li><strong>小批量梯度下降</strong>：它<strong>介于批量梯度下降和随机梯度下降之间</strong>，每次迭代<strong>使用一小部分训练样本</strong>（即小批量）来计算梯度并更新模型参数。这种方法既保留了批量梯度下降的稳定收敛特性，又具有随机梯度下降的快速计算优势。因此，小批量梯度下降在实践中被广泛应用，尤其是在深度学习模型的训练中。</li>\n</ul>\n<p>总的来说，这三种方法都是梯度下降的变体，目的是通过迭代更新模型参数以最小化损失函数。批量梯度下降使用所有数据，收敛稳定但计算成本高；随机梯度下降使用单个样本，计算快但可能波动大；小批量梯度下降折中了前两者的特点，使用部分数据，既稳定又相对高效。</p>\n</blockquote>\n</li>\n<li><p>Adam、RMSprop、Adagrad、Momentum优化算法？</p>\n<blockquote>\n<p><strong>Adam</strong> 将 <strong>RMSprop 和 Stochastic Gradient Descent 的思想与动量相结合</strong>。它动态调整每个参数的学习率。RMSprop 在训练期间调整学习率，而 Adagrad 通过将学习率缩放为与梯度的所有过去平方值的平方根成反比来调整学习率。动量有助于加速新元朝相关方向发展。</p>\n<p>此方法是在<strong>梯度下降的基础上加入了“惯性”的概念</strong>，即考虑了历史梯度对当前梯度的影响，使得更新过程具有一定的连续性，并能够加快学习速度，减少振荡。Momentum 通过使用动量项来对网络参数进行平滑处理，有助于使梯度的摆动幅度变小，从而加快收敛速度。</p>\n</blockquote>\n</li>\n<li><p>什么是过拟合？怎么解决过拟合问题？</p>\n<blockquote>\n<p>过拟合是指模型在训练数据上表现良好但在新数据上泛化能力差的现象。</p>\n<p><strong>解决过拟合的方法</strong>包括增加<strong>数据量</strong>、减少模型<strong>复杂度</strong>、使用<strong>正则化</strong>技术和集成学习方法等。</p>\n</blockquote>\n</li>\n<li><p>怎样解决梯度消失&#x2F;爆炸问题？</p>\n<blockquote>\n<p><strong>含义</strong>：</p>\n<ol>\n<li><strong>梯度消失</strong>：在深度神经网络的反向传播过程中，梯度可能会因为多层连续的乘法操作而变得非常小，以至于权重几乎不会被更新，导致训练过程提前停止。这种现象通常发生在激活函数选择不当（如使用sigmoid或tanh函数）且网络层次较深的情况下。</li>\n<li><strong>梯度爆炸</strong>：与梯度消失相反，梯度爆炸是指梯度值变得非常大，以至于导致模型参数更新过于频繁和巨大，从而使得模型无法收敛到一个稳定的解。梯度爆炸通常是由于梯度值在反向传播过程中连续乘以较大的数而累积起来的。</li>\n</ol>\n<p>如何解决？</p>\n<ul>\n<li><strong>选择合适的激活函数</strong>：<strong>避免</strong>使用<strong>容易饱和</strong>的激活函数，如sigmoid或tanh，转而使用ReLU（Rectified Linear Units）及其变种Leaky ReLU、Parametric ReLU等，它们在输入值较大时不会饱和，有助于缓解梯度消失问题。</li>\n<li><strong>批量归一化（Batch Normalization）</strong>：通过对每一层的输入进行归一化处理，保持输入数据的均值为0、方差为1，有助于稳定梯度变化，防止梯度消失和爆炸。</li>\n<li><strong>残差连接（Residual Connections）</strong>：在深度网络中使用残差连接，即让前一层的输出直接加到后续某层的输入上，可以保证梯度能够绕过一些层直接传回，从而缓解梯度消失问题。</li>\n<li><strong>梯度裁剪（Gradient Clipping）</strong>：通过设置一个阈值来限制梯度的最大值，可以有效防止梯度爆炸。</li>\n<li><strong>优化器选择</strong>：使用具有自适应学习率调整能力的优化器，如Adam、RMSprop等，可以在不同情况下自动调整学习率，减少梯度消失或爆炸的风险。</li>\n<li><strong>短程记忆结构</strong>：对于循环神经网络（RNN），可以<strong>使用长短时记忆（LSTM）</strong>或门控循环单元（GRU）这类结构来代替传统的RNN结构，因为它们能够更好地捕捉长距离依赖关系，从而缓解梯度消失问题。</li>\n<li><strong>正则化技术</strong>：如L1&#x2F;L2正则化，也可以帮助控制模型参数的规模，间接地减小梯度爆炸的可能性。</li>\n</ul>\n</blockquote>\n</li>\n<li><p>讲解下神经网络反向传播算法？</p>\n<blockquote>\n<p><strong>反向传播算法</strong>：反向传播是人工神经网络中使用的一种方法，用于计算单个输入输出示例的损失函数相对于网络权重的梯度，从而使用梯度下降更新权重。</p>\n</blockquote>\n</li>\n<li><p>激活函数的作用是什么？有哪些激活函数？它们的表达式分别是？</p>\n<blockquote>\n<p>激活函数作用：引入<strong>非线性</strong>因素，使得神经网络可以学习和模拟任何复杂的函数和数据分布，这样网络就能处理非线性问题。如果<strong>没有激活函数，无论神经网络有多少层，输出都只是输入的线性组合</strong>，这大大限制了网络的表达能力和复杂度。</p>\n<p>常见的激活函数包括 Sigmoid、Tanh、ReLU：（需要去了解一下基本原理）</p>\n</blockquote>\n</li>\n<li><p>请讲解一下正则化的概念？L1正则化是什么？L2正则化是什么？</p>\n<blockquote>\n<p>正则化是机器学习和统计学中用于<strong>防止模型过拟合</strong>的一种技术。过拟合是指模型在训练数据上表现得很好，但在未见过的新数据上表现不佳的现象，通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是潜在的数据生成规律。<strong>正则化通过在模型训练的损失函数中添加一个额外的惩罚项来解决这个问题，该惩罚项会惩罚模型的复杂度</strong></p>\n<ul>\n<li><strong>L1正则化</strong>（也称为Lasso正则化）通过向损失函数添加参数的绝对值之和作为惩罚项来工作。L1正则化倾向于产生稀疏参数，即许多参数会变成零，这可以用于特征选择。</li>\n<li><strong>L2正则化</strong>（也称为Ridge正则化）通过向损失函数添加参数的平方和作为惩罚项来工作。与L1正则化不同，L2正则化倾向于使参数非常小但不完全为零，从而保持所有特征但减少其影响。</li>\n<li>在某些情况下，人们可能会<strong>同时使用L1和L2正则化</strong>，这种方法被称为<strong>弹性网络</strong>正则化。</li>\n<li>正则化技术不仅限于线性模型，它们也广泛应用于神经网络、决策树和其他类型的机器学习模型中。在神经网络中，除了L1和L2正则化外，还有如<strong>Dropout</strong>正则化等特殊形式的正则化技术，它通过在训练过程中随机丢弃一部分神经元来防止过拟合。</li>\n</ul>\n</blockquote>\n</li>\n<li><p>除了BatchNormalization还有其他什么方法来加速模型训练吗？</p>\n<blockquote>\n<ol>\n<li>使用<strong>半字训练（半精度浮点数）</strong></li>\n<li><strong>合理的超参数设计</strong>：选择合适的batch size、epoch数量和学习率策略对于加速模型训练至关重要。较小的batch size可以加快单次迭代的速度，而适当的epoch数量和学习率调整可以帮助模型更快地收敛。</li>\n<li><strong>增大batch size</strong>：虽然增大batch size可以提高训练速度，但过大的batch size可能会影响模型的泛化性能和收敛速度。因此，需要根据具体情况选择合适的batch size大小。</li>\n<li><strong>多GPU分布式训练</strong>：通过使用多个GPU进行模型并行或数据并行训练，可以显著提高模型的训练速度。模型并行是将模型的不同部分分配给不同的计算设备，而数据并行则是将数据集分割成多个子集，每个子集在不同的计算设备上进行处理。</li>\n<li><strong>模型压缩与加速技术</strong>：包括参数剪枝、参数量化、紧凑网络设计、知识蒸馏、低秩分解、参数共享等方法。这些技术旨在减少模型的大小和复杂性，从而提高训练和推理的速度。</li>\n<li><strong>使用预训练模型</strong>：在相似任务上使用预训练模型可以显著减少训练时间，因为预训练模型已经学习了大量的特征，只需要对顶层进行微调即可。</li>\n</ol>\n</blockquote>\n</li>\n<li><p>机器学习中，为何要经常对数据进行预处理？</p>\n<blockquote>\n<p>处理噪声、缺失值、降维、结构化非结构化处理</p>\n<ol>\n<li><strong>处理缺失值与噪声</strong>：数据中可能存在缺失值，预处理可以包括填补或删除这些缺失值，以确保数据的完整性。</li>\n<li><strong>无量纲化</strong>：不同特征可能具有不同的量纲和规模，无量纲化处理可以消除这种差异，使得模型能够更公平地评估每个特征的重要性。</li>\n<li><strong>数据规范化</strong>：通过最值归一化或Z-Score规范化等方法，将数据缩放到特定的范围内，有助于加快模型的收敛速度。</li>\n<li><strong>特征工程</strong>：数据预处理是特征工程的一部分，它包括数据清洗、特征提取、特征选择和特征构造等子问题。通过这些步骤，可以创建出能够使机器学习算法达到最佳性能的特征。</li>\n<li><strong>提高模型性能</strong>：干净且经过恰当处理的数据可以帮助算法更准确地学习模式和关系，从而提高模型的性能。</li>\n<li><strong>增强模型泛化能力</strong>：通过正则化等预处理方法，可以防止模型过拟合，提高其在新数据上的泛化能力。</li>\n</ol>\n</blockquote>\n</li>\n</ol>\n"},{"title":"基于Transformer的催化反应产率预测","mathjax":true,"date":"2025-02-25T12:46:25.000Z","img":"https://img0.baidu.com/it/u=3213989145,974537053&fm=253&fmt=auto&app=120&f=PNG?w=1023&h=362","excerpt":"RT","_content":"# 任务概述\n\n构建一个能够准确预测碳氮成键反应产率的预测模型。  \n\n通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。\n\n或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。\n\n训练集中包含19999条反应数据，测试集中包含3539条反应数据。约85%与12%。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。\n\n**评价指标**\n\n实验真实结果与预测结果$R^2$决定系数来进行评测:\n$$\nR^2(y,\\hat{y})=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n$$\n\n\n# baseline\n\n1. **导入库**：首先，代码导入了需要用到的库，包括 `pandas`（用于数据处理和分析），`scikit-learn`（机器学习库），`rdkit`（化学信息工具）。\n2. **读取数据**：代码通过使用 `pd.read_csv` 函数从文件中读取训练集和测试集数据。\n3. **使用Morgan分子指纹建模SMILES**：\n\n   \\- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。\n\n1. **使用随机森林预测结果**：\n\n   \\- 这里直接调用`sklearn`的`RandomForestRegressor`模块实例化一个随机森林模型，并对`n_estimators`等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地`'./random_forest_model.pkl'`。\n\n1. **加载模型进行预测，并将保存结果文件到本地：**\n\n   ` pkl`文件直接使用`pickle.load()`加载，然后使用`model.predict(x)`进行预测。\n\n## SMILES\n\nSMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。\n\nSMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。\n\n在SMILES中，原子由他们的化学符号表示，=表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。\n\n（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）\n\n由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。\n\n## Morgan fingerprint\n位向量（bit vector）形式的特征，即由0,1组成的向量。\n\n分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。\n\n通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。\n\n## RDKit\n\nRDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES\n\nRDkit是化学信息学中主要的工具，是开源的。网址：http://www.rdkit.org\n支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。\n\n## 结果\n\nbaseline的$R^2 = 0.0745336043830066$，约0.08\n\n# RNN建模\n\nRNN（Recurrent Neural Network）是处理序列数据的一把好手。RNN的网络每层除了会有自己的输出以外，还会输出一个隐向量到下一层。\n\n![](https://img-blog.csdnimg.cn/direct/4d59745e6a904f34afcf1713ff2cd2c3.png)\n\n其中，每一层相当于做了一次线性变换：\n\n$$h_n = \\sigma(W_{hh}h_{n-1} + W_{hx}x_n + b_n)$$\n\n每层的输出：$$ y_n = Softmax(Vh_n + c)$$\n\n通过隐向量的不断传递，序列后面的部分就通过“阅读”隐向量，获取前面序列的信息，从而提升学习能力。\n\n但是RNN也有缺点：如果序列太长，那么两个相距比较远的字符之间的联系需要通过多个隐藏向量。\n\n同时，RNN需要一层一层地传递，所以并行能力差，同时也比较容易出现梯度消失或梯度爆炸问题。\n","source":"_posts/ai4chem.md","raw":"---\ntitle: 基于Transformer的催化反应产率预测\nmathjax: true\ndate: 2025/2/25 20:46:25\nimg: https://img0.baidu.com/it/u=3213989145,974537053&fm=253&fmt=auto&app=120&f=PNG?w=1023&h=362\nexcerpt: RT\n---\n# 任务概述\n\n构建一个能够准确预测碳氮成键反应产率的预测模型。  \n\n通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。\n\n或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。\n\n训练集中包含19999条反应数据，测试集中包含3539条反应数据。约85%与12%。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。\n\n**评价指标**\n\n实验真实结果与预测结果$R^2$决定系数来进行评测:\n$$\nR^2(y,\\hat{y})=1-\\frac{\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}{\\sum_{i=1}^n(y_i-\\bar{y})^2}\n$$\n\n\n# baseline\n\n1. **导入库**：首先，代码导入了需要用到的库，包括 `pandas`（用于数据处理和分析），`scikit-learn`（机器学习库），`rdkit`（化学信息工具）。\n2. **读取数据**：代码通过使用 `pd.read_csv` 函数从文件中读取训练集和测试集数据。\n3. **使用Morgan分子指纹建模SMILES**：\n\n   \\- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。\n\n1. **使用随机森林预测结果**：\n\n   \\- 这里直接调用`sklearn`的`RandomForestRegressor`模块实例化一个随机森林模型，并对`n_estimators`等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地`'./random_forest_model.pkl'`。\n\n1. **加载模型进行预测，并将保存结果文件到本地：**\n\n   ` pkl`文件直接使用`pickle.load()`加载，然后使用`model.predict(x)`进行预测。\n\n## SMILES\n\nSMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。\n\nSMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。\n\n在SMILES中，原子由他们的化学符号表示，=表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。\n\n（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）\n\n由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。\n\n## Morgan fingerprint\n位向量（bit vector）形式的特征，即由0,1组成的向量。\n\n分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。\n\n通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。\n\n## RDKit\n\nRDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES\n\nRDkit是化学信息学中主要的工具，是开源的。网址：http://www.rdkit.org\n支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。\n\n## 结果\n\nbaseline的$R^2 = 0.0745336043830066$，约0.08\n\n# RNN建模\n\nRNN（Recurrent Neural Network）是处理序列数据的一把好手。RNN的网络每层除了会有自己的输出以外，还会输出一个隐向量到下一层。\n\n![](https://img-blog.csdnimg.cn/direct/4d59745e6a904f34afcf1713ff2cd2c3.png)\n\n其中，每一层相当于做了一次线性变换：\n\n$$h_n = \\sigma(W_{hh}h_{n-1} + W_{hx}x_n + b_n)$$\n\n每层的输出：$$ y_n = Softmax(Vh_n + c)$$\n\n通过隐向量的不断传递，序列后面的部分就通过“阅读”隐向量，获取前面序列的信息，从而提升学习能力。\n\n但是RNN也有缺点：如果序列太长，那么两个相距比较远的字符之间的联系需要通过多个隐藏向量。\n\n同时，RNN需要一层一层地传递，所以并行能力差，同时也比较容易出现梯度消失或梯度爆炸问题。\n","slug":"ai4chem","published":1,"updated":"2025-02-27T16:14:19.705Z","comments":1,"layout":"post","photos":[],"_id":"cm7o4qpnz00032o99g8xs8w4f","content":"<h1 id=\"任务概述\"><a href=\"#任务概述\" class=\"headerlink\" title=\"任务概述\"></a>任务概述</h1><p>构建一个能够准确预测碳氮成键反应产率的预测模型。  </p>\n<p>通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。</p>\n<p>或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。</p>\n<p>训练集中包含19999条反应数据，测试集中包含3539条反应数据。约85%与12%。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。</p>\n<p><strong>评价指标</strong></p>\n<p>实验真实结果与预测结果$R^2$决定系数来进行评测:<br>$$<br>R^2(y,\\hat{y})&#x3D;1-\\frac{\\sum_{i&#x3D;1}^n(y_i-\\hat{y}<em>i)^2}{\\sum</em>{i&#x3D;1}^n(y_i-\\bar{y})^2}<br>$$</p>\n<h1 id=\"baseline\"><a href=\"#baseline\" class=\"headerlink\" title=\"baseline\"></a>baseline</h1><ol>\n<li><p><strong>导入库</strong>：首先，代码导入了需要用到的库，包括 <code>pandas</code>（用于数据处理和分析），<code>scikit-learn</code>（机器学习库），<code>rdkit</code>（化学信息工具）。</p>\n</li>\n<li><p><strong>读取数据</strong>：代码通过使用 <code>pd.read_csv</code> 函数从文件中读取训练集和测试集数据。</p>\n</li>\n<li><p><strong>使用Morgan分子指纹建模SMILES</strong>：</p>\n<p>- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。</p>\n</li>\n<li><p><strong>使用随机森林预测结果</strong>：</p>\n<p>- 这里直接调用<code>sklearn</code>的<code>RandomForestRegressor</code>模块实例化一个随机森林模型，并对<code>n_estimators</code>等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地<code>&#39;./random_forest_model.pkl&#39;</code>。</p>\n</li>\n<li><p><strong>加载模型进行预测，并将保存结果文件到本地：</strong></p>\n<p><code> pkl</code>文件直接使用<code>pickle.load()</code>加载，然后使用<code>model.predict(x)</code>进行预测。</p>\n</li>\n</ol>\n<h2 id=\"SMILES\"><a href=\"#SMILES\" class=\"headerlink\" title=\"SMILES\"></a>SMILES</h2><p>SMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。</p>\n<p>SMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。</p>\n<p>在SMILES中，原子由他们的化学符号表示，&#x3D;表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。</p>\n<p>（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）</p>\n<p>由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。</p>\n<h2 id=\"Morgan-fingerprint\"><a href=\"#Morgan-fingerprint\" class=\"headerlink\" title=\"Morgan fingerprint\"></a>Morgan fingerprint</h2><p>位向量（bit vector）形式的特征，即由0,1组成的向量。</p>\n<p>分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。</p>\n<p>通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。</p>\n<h2 id=\"RDKit\"><a href=\"#RDKit\" class=\"headerlink\" title=\"RDKit\"></a>RDKit</h2><p>RDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES</p>\n<p>RDkit是化学信息学中主要的工具，是开源的。网址：<a href=\"http://www.rdkit.org/\">http://www.rdkit.org</a><br>支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。</p>\n<h2 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h2><p>baseline的$R^2 &#x3D; 0.0745336043830066$，约0.08</p>\n<h1 id=\"RNN建模\"><a href=\"#RNN建模\" class=\"headerlink\" title=\"RNN建模\"></a>RNN建模</h1><p>RNN（Recurrent Neural Network）是处理序列数据的一把好手。RNN的网络每层除了会有自己的输出以外，还会输出一个隐向量到下一层。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/direct/4d59745e6a904f34afcf1713ff2cd2c3.png\" class=\"lazyload placeholder\" data-srcset=\"https://img-blog.csdnimg.cn/direct/4d59745e6a904f34afcf1713ff2cd2c3.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\"></p>\n<p>其中，每一层相当于做了一次线性变换：</p>\n<p>$$h_n &#x3D; \\sigma(W_{hh}h_{n-1} + W_{hx}x_n + b_n)$$</p>\n<p>每层的输出：$$ y_n &#x3D; Softmax(Vh_n + c)$$</p>\n<p>通过隐向量的不断传递，序列后面的部分就通过“阅读”隐向量，获取前面序列的信息，从而提升学习能力。</p>\n<p>但是RNN也有缺点：如果序列太长，那么两个相距比较远的字符之间的联系需要通过多个隐藏向量。</p>\n<p>同时，RNN需要一层一层地传递，所以并行能力差，同时也比较容易出现梯度消失或梯度爆炸问题。</p>\n","more":"<h1 id=\"任务概述\"><a href=\"#任务概述\" class=\"headerlink\" title=\"任务概述\"></a>任务概述</h1><p>构建一个能够准确预测碳氮成键反应产率的预测模型。  </p>\n<p>通过对反应中所包含的反应底物、添加剂、溶剂以及产物进行合理的特征化，运用机器学习模型或者深度学习模型拟合预测反应的产率。</p>\n<p>或者利用训练集数据对开源大语言模型进行微调以预测反应的产率。</p>\n<p>训练集中包含19999条反应数据，测试集中包含3539条反应数据。约85%与12%。每条训练数据包含 rxnid, Reactant1, Reactant2 , Product , Additive , Solvent , Yield字段。其中 Reactant1 , Reactant2 , Product , Additive , Solvent 字段中为对应物质的SMILES字符串，Yield字段为目标字段，是经过归一化的浮点数。</p>\n<p><strong>评价指标</strong></p>\n<p>实验真实结果与预测结果$R^2$决定系数来进行评测:<br>$$<br>R^2(y,\\hat{y})&#x3D;1-\\frac{\\sum_{i&#x3D;1}^n(y_i-\\hat{y}<em>i)^2}{\\sum</em>{i&#x3D;1}^n(y_i-\\bar{y})^2}<br>$$</p>\n<h1 id=\"baseline\"><a href=\"#baseline\" class=\"headerlink\" title=\"baseline\"></a>baseline</h1><ol>\n<li><p><strong>导入库</strong>：首先，代码导入了需要用到的库，包括 <code>pandas</code>（用于数据处理和分析），<code>scikit-learn</code>（机器学习库），<code>rdkit</code>（化学信息工具）。</p>\n</li>\n<li><p><strong>读取数据</strong>：代码通过使用 <code>pd.read_csv</code> 函数从文件中读取训练集和测试集数据。</p>\n</li>\n<li><p><strong>使用Morgan分子指纹建模SMILES</strong>：</p>\n<p>- 这个过程需要调用rdkit的相关模块。然后将Reactant1,Reactant2,Product,Additive,Solvent字段的向量拼接到一起，组成一个更长的向量。</p>\n</li>\n<li><p><strong>使用随机森林预测结果</strong>：</p>\n<p>- 这里直接调用<code>sklearn</code>的<code>RandomForestRegressor</code>模块实例化一个随机森林模型，并对<code>n_estimators</code>等重要参数进行指定。最后使用model.fit(x, y)训练模型。模型保存在本地<code>&#39;./random_forest_model.pkl&#39;</code>。</p>\n</li>\n<li><p><strong>加载模型进行预测，并将保存结果文件到本地：</strong></p>\n<p><code> pkl</code>文件直接使用<code>pickle.load()</code>加载，然后使用<code>model.predict(x)</code>进行预测。</p>\n</li>\n</ol>\n<h2 id=\"SMILES\"><a href=\"#SMILES\" class=\"headerlink\" title=\"SMILES\"></a>SMILES</h2><p>SMILES,全称是Simplified Molecular Input Line Entry System，是一种将化学分子用ASCII字符表示的方法，是化学信息学领域非常重要的工具。</p>\n<p>SMILES将化学分子中涉及的原子、键、电荷等信息，用对应的ASCII字符表示；环、侧链等化学结构信息，用特定的书写规范表达。以此，几乎所有的分子都可以用特定的SMILES表示，且SMILES的表示还算比较直观。</p>\n<p>在SMILES中，原子由他们的化学符号表示，&#x3D;表示双键、#表示三键、[]里面的内容表示侧基或者特殊原子（例如[Cu+2]表示带电+2电荷的Cu离子）。通过SMLIES，就可以把分子表示为序列类型的数据了。</p>\n<p>（注：SMILES有自己的局限性：例如选择不同的起始原子，写出来的SMILES不同；它无法表示空间信息。）</p>\n<p>由于Reactant1,Reactant2,Product,Additive,Solvent都是由SMILES表示。所以，可以使用rdkit工具直接提取SMILES的分子指纹（向量），作为特征。</p>\n<h2 id=\"Morgan-fingerprint\"><a href=\"#Morgan-fingerprint\" class=\"headerlink\" title=\"Morgan fingerprint\"></a>Morgan fingerprint</h2><p>位向量（bit vector）形式的特征，即由0,1组成的向量。</p>\n<p>分子指纹是一个具有固定长度的位向量（即由0，1组成），其中，每个为1的值表示这个分子具有某些特定的化学结构。</p>\n<p>通常，分子指纹的维度都是上千的，也即记录了上千个子结构是否出现在分子中。</p>\n<h2 id=\"RDKit\"><a href=\"#RDKit\" class=\"headerlink\" title=\"RDKit\"></a>RDKit</h2><p>RDkit会将分子读取为RDkit中专属的rdkit.Chem.rdchem.Mol对象，并以Mol对象为基础，可以对分子进行转化为各种表达形式，例如SMILES</p>\n<p>RDkit是化学信息学中主要的工具，是开源的。网址：<a href=\"http://www.rdkit.org/\">http://www.rdkit.org</a><br>支持WIN\\MAC\\Linux，可以被python、Java、C调用。几乎所有的与化学信息学相关的内容都可以在上面找到。</p>\n<h2 id=\"结果\"><a href=\"#结果\" class=\"headerlink\" title=\"结果\"></a>结果</h2><p>baseline的$R^2 &#x3D; 0.0745336043830066$，约0.08</p>\n<h1 id=\"RNN建模\"><a href=\"#RNN建模\" class=\"headerlink\" title=\"RNN建模\"></a>RNN建模</h1><p>RNN（Recurrent Neural Network）是处理序列数据的一把好手。RNN的网络每层除了会有自己的输出以外，还会输出一个隐向量到下一层。</p>\n<p><img src=\"https://img-blog.csdnimg.cn/direct/4d59745e6a904f34afcf1713ff2cd2c3.png\"></p>\n<p>其中，每一层相当于做了一次线性变换：</p>\n<p>$$h_n &#x3D; \\sigma(W_{hh}h_{n-1} + W_{hx}x_n + b_n)$$</p>\n<p>每层的输出：$$ y_n &#x3D; Softmax(Vh_n + c)$$</p>\n<p>通过隐向量的不断传递，序列后面的部分就通过“阅读”隐向量，获取前面序列的信息，从而提升学习能力。</p>\n<p>但是RNN也有缺点：如果序列太长，那么两个相距比较远的字符之间的联系需要通过多个隐藏向量。</p>\n<p>同时，RNN需要一层一层地传递，所以并行能力差，同时也比较容易出现梯度消失或梯度爆炸问题。</p>\n"},{"title":"Hello World","mathjax":true,"date":"2023-01-22T12:46:25.000Z","img":"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png","excerpt":"我的halo word","_content":"勇神牛逼！！！！\n\n测试latex\n\n$sin(\\alpha + \\beta)$\n\n测试图片\n\n![img](/img/hello-world/Kaz.jpg)\n\n![Transformer](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png)\n\n测试代码块\n\n```python\nimport torch\n#attention is all u need\nprint(\"Yong shen NB\")\n```\n\n```c++\n#include <iostream>\nusing namespace std;\nint main(){\n    cout<<\"勇神牛逼\";\n    pair<int, int> p;\n    int get = [&](int l, int r){\n    \treturn l + r >> 1;  \n    };\n    return 0;\n}\n```\n\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\nmathjax: true\ndate: 2023/1/22 20:46:25\nimg: https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\nexcerpt: 我的halo word\n---\n勇神牛逼！！！！\n\n测试latex\n\n$sin(\\alpha + \\beta)$\n\n测试图片\n\n![img](/img/hello-world/Kaz.jpg)\n\n![Transformer](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png)\n\n测试代码块\n\n```python\nimport torch\n#attention is all u need\nprint(\"Yong shen NB\")\n```\n\n```c++\n#include <iostream>\nusing namespace std;\nint main(){\n    cout<<\"勇神牛逼\";\n    pair<int, int> p;\n    int get = [&](int l, int r){\n    \treturn l + r >> 1;  \n    };\n    return 0;\n}\n```\n\n","slug":"hello-world","published":1,"updated":"2025-02-17T12:39:42.971Z","comments":1,"layout":"post","photos":[],"_id":"cm7o4qpo000052o99dpe1a6kh","content":"<p>勇神牛逼！！！！</p>\n<p>测试latex</p>\n<p>$sin(\\alpha + \\beta)$</p>\n<p>测试图片</p>\n<p><img src=\"/img/hello-world/Kaz.jpg\" class=\"lazyload placeholder\" data-srcset=\"/img/hello-world/Kaz.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" class=\"lazyload placeholder\" data-srcset=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"Transformer\"></p>\n<p>测试代码块</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#attention is all u need</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Yong shen NB&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"string\">&quot;勇神牛逼&quot;</span>;</span><br><span class=\"line\">    pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; p;</span><br><span class=\"line\">    <span class=\"type\">int</span> get = [&amp;](<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">return</span> l + r &gt;&gt; <span class=\"number\">1</span>;  </span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n","more":"<p>勇神牛逼！！！！</p>\n<p>测试latex</p>\n<p>$sin(\\alpha + \\beta)$</p>\n<p>测试图片</p>\n<p><img src=\"/img/hello-world/Kaz.jpg\" alt=\"img\"></p>\n<p><img src=\"https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/23450/dashboard/1736488152304/image.png\" alt=\"Transformer\"></p>\n<p>测试代码块</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"comment\">#attention is all u need</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Yong shen NB&quot;</span>)</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight c++\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"meta\">#<span class=\"keyword\">include</span> <span class=\"string\">&lt;iostream&gt;</span></span></span><br><span class=\"line\"><span class=\"keyword\">using</span> <span class=\"keyword\">namespace</span> std;</span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">main</span><span class=\"params\">()</span></span>&#123;</span><br><span class=\"line\">    cout&lt;&lt;<span class=\"string\">&quot;勇神牛逼&quot;</span>;</span><br><span class=\"line\">    pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt; p;</span><br><span class=\"line\">    <span class=\"type\">int</span> get = [&amp;](<span class=\"type\">int</span> l, <span class=\"type\">int</span> r)&#123;</span><br><span class=\"line\">    \t<span class=\"keyword\">return</span> l + r &gt;&gt; <span class=\"number\">1</span>;  </span><br><span class=\"line\">    &#125;;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n"},{"title":"Algorithm-Data-Structure","mathjax":true,"date":"2023-07-07T12:46:25.000Z","img":"https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg","excerpt":"算法竞赛基础数据结构","_content":"\n## 链表与邻接表\n\n由于用`结构体+指针`比较慢，一般在面试题使用，在这里使用**数组**模拟链表\n\n- 单链表\n\n`e[N]`：储存链表结点的值\n\n`ne[N]`：储存结点的下一个结点下标，其中空结点下标为-1\n\n```cpp\n// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点\nint head, e[N], ne[N], idx;\n\n// 初始化\nvoid init()\n{\n    head = -1;\n    idx = 0;\n}\n\n// 在链表头插入一个数a\nvoid insert(int a)\n{\n    e[idx] = a, ne[idx] = head, head = idx ++ ;\n}\n\n//在a插到下标是k的结点后面\nvoid insert(int a,int k)\n{\n    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;\n}\n\n// 将头结点删除，需要保证头结点存在\nvoid remove()\n{\n    head = ne[head];\n}\n\n// 将下标为k的结点的后一个点删除\nvoid remove(k)\n{\n    ne[k] = ne[ne[k]];\n}\n```\n\n- 双链表\n\n作用：优化某些问题\n\n```cpp\n// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点\nint e[N], l[N], r[N], idx;\n\n// 初始化\nvoid init()\n{\n    //0是左端点，1是右端点\n    r[0] = 1, l[1] = 0;\n    idx = 2;\n}\n\n// 在节点a的右边插入一个数x\nvoid insert(int a, int x)\n{\n    e[idx] = x;\n    l[idx] = a, r[idx] = r[a];\n    l[r[a]] = idx, r[a] = idx ++ ;\n}\n\n// 删除节点a\nvoid remove(int a)\n{\n    l[r[a]] = l[a];\n    r[l[a]] = r[a];\n}\n```\n\n- 邻接表\n\nN个单链表，用于存储树和图\n\n## 栈\n\n先进后出(FILO)\n\n```cpp\n// tt表示栈顶\nint stk[N], tt = 0;\n\n// 向栈顶插入一个数\nstk[ ++ tt] = x;\n\n// 从栈顶弹出一个数\ntt -- ;\n\n// 栈顶的值\nstk[tt];\n\n// 判断栈是否为空，如果 tt > 0，则表示不为空\nif (tt > 0)\n{\n\tnot empty\n}else\n{\n    empty\n}\n```\n\n## 队列\n\n先进先出(FIFO)\n\n```cpp\n// hh 表示队头，tt表示队尾\nint q[N], hh = 0, tt = -1;\n\n// 向队尾插入一个数\nq[ ++ tt] = x;\n\n// 从队头弹出一个数\nhh ++ ;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果 hh <= tt，则表示不为空\nif (hh <= tt)\n{\n\n}\n```\n\n循环队列\n\n```cpp\n// hh 表示队头，tt表示队尾的后一个位置\nint q[N], hh = 0, tt = 0;\n\n// 向队尾插入一个数\nq[tt ++ ] = x;\nif (tt == N) tt = 0;\n\n// 从队头弹出一个数\nhh ++ ;\nif (hh == N) hh = 0;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果hh != tt，则表示不为空\nif (hh != tt)\n{\n\n}\n```\n\n## 单调栈\n\n题型：求给定序列每一个数左/右边离他最近的比他大/小的数\n\n```cpp\nint tt = 0;\nfor (int i = 1; i <= n; i ++ )\n{\n    while (tt && check(stk[tt], i)) tt -- ;\n    stk[ ++ tt] = i;\n}\n```\n\n## 单调队列\n\n题型：求滑动窗口的最大/小值\n\n```cpp\nint hh = 0, tt = -1;\nfor (int i = 0; i < n; i ++ )\n{\n    while (hh <= tt && check_out(q[hh])) hh ++ ;  // 判断队头是否滑出窗口\n    while (hh <= tt && check(q[tt], i)) tt -- ;\n    q[ ++ tt] = i;\n}\n```\n\n## KMP\n\n对于字符串$s$,判断是否包含模式串$t$\n\n```cpp\n// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度\n//求模式串的Next数组：\nfor (int i = 2, j = 0; i <= m; i ++ )\n{\n    while (j && p[i] != p[j + 1]) j = ne[j];\n    if (p[i] == p[j + 1]) j ++ ;\n    ne[i] = j;\n}\n\n// 匹配\nfor (int i = 1, j = 0; i <= n; i ++ )\n{\n    while (j && s[i] != p[j + 1]) j = ne[j];\n    if (s[i] == p[j + 1]) j ++ ;\n    if (j == m)\n    {\n        j = ne[j];\n        // 匹配成功后的逻辑\n    }\n}\n```\n\n```cpp\ns.find(t) != s.npos\n```\n\n## Trie树\n\n快速存储和查找字符串集合的数据结构\n\n```cpp\nint son[N][26], cnt[N], idx;\n// 0号点既是根节点，又是空节点\n// son[][]存储树中每个节点的子节点\n// cnt[]存储以每个节点结尾的单词数量\n\n// 插入一个字符串\nvoid insert(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) son[p][u] = ++ idx;\n        p = son[p][u];\n    }\n    cnt[p] ++ ;\n}\n\n// 查询字符串出现的次数\nint query(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) return 0;\n        p = son[p][u];\n    }\n    return cnt[p];\n}\n```\n\n[最大异或对](https://www.acwing.com/problem/content/145/)\n\n[前缀统计](https://www.acwing.com/problem/content/144/)\n\n## 并查集\n\n1. 将两个集合合并\n2. 询问两个元素是否在一个集合当中\n\n近乎$O(1)$\n\n基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点\n\n- 如何判断是树根？\n\n$p[x] = x$\n\n- 如何求$x$的集合编号\n\n```cpp\nwhile(p[x]!=x) x = p[x]\n```\n\n- 如何合并两个区间\n\n设p[x]为x集合编号，p[y]是y集合编号。p[x]=y\n\n优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根\n\n```cpp\n(1)朴素并查集：\n\n    int p[N]; //存储每个点的祖宗节点\n\n    // 返回x的祖宗节点+路径压缩\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n\n\n(2)维护size的并查集：\n\n    int p[N], size[N];\n    //p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        size[i] = 1;\n    }\n\n    // 合并a和b所在的两个集合：\n    size[find(b)] += size[find(a)];\n    p[find(a)] = find(b);\n\n\n(3)维护到祖宗节点距离的并查集：\n\n    int p[N], d[N];\n    //p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x)\n        {\n            int u = find(p[x]);\n            d[x] += d[p[x]];\n            p[x] = u;\n        }\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        d[i] = 0;\n    }\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n    d[find(a)] = distance; // 根据具体问题，初始化find(a)的偏移量\n```\n\n## C++ STL\n\n```cpp\nvector, 变长数组，倍增的思想\n    size()  返回元素个数\n    empty()  返回是否为空\n    clear()  清空\n    front()/back()\n    push_back()/pop_back()\n    begin()/end()\n    []\n    支持比较运算，按字典序\n\npair<int, int>\n    first, 第一个元素\n    second, 第二个元素\n    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）\n\nstring，字符串\n    size()/length()  返回字符串长度\n    empty()\n    clear()\n    substr(起始下标，(子串长度))  返回子串\n    c_str()  返回字符串所在字符数组的起始地址\n\nqueue, 队列\n    size()\n    empty()\n    push()  向队尾插入一个元素\n    front()  返回队头元素\n    back()  返回队尾元素\n    pop()  弹出队头元素\n\npriority_queue, 优先队列，默认是大根堆\n    size()\n    empty()\n    push()  插入一个元素\n    top()  返回堆顶元素\n    pop()  弹出堆顶元素\n    定义成小根堆的方式：priority_queue<int, vector<int>, greater<int>> q;\n\nstack, 栈\n    size()\n    empty()\n    push()  向栈顶插入一个元素\n    top()  返回栈顶元素\n    pop()  弹出栈顶元素\n\ndeque, 双端队列\n    size()\n    empty()\n    clear()\n    front()/back()\n    push_back()/pop_back()\n    push_front()/pop_front()\n    begin()/end()\n    []\n\nset, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列\n    size()\n    empty()\n    clear()\n    begin()/end()\n    ++, -- 返回前驱和后继，时间复杂度 O(logn)\n\n    set/multiset\n        insert()  插入一个数\n        find()  查找一个数\n        count()  返回某一个数的个数\n        erase()\n            (1) 输入是一个数x，删除所有x   O(k + logn)\n            (2) 输入一个迭代器，删除这个迭代器\n        lower_bound()/upper_bound()\n            lower_bound(x)  返回大于等于x的最小的数的迭代器\n            upper_bound(x)  返回大于x的最小的数的迭代器\n    map/multimap\n        insert()  插入的数是一个pair\n        erase()  输入的参数是pair或者迭代器\n        find()\n        []  注意multimap不支持此操作。 时间复杂度是 O(logn)\n        lower_bound()/upper_bound()\n\nunordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表\n    和上面类似，增删改查的时间复杂度是 O(1)\n    不支持 lower_bound()/upper_bound()， 迭代器的++，--\n\nbitset, 圧位\n    bitset<10000> s;\n    ~, &, |, ^\n    >>, <<\n    ==, !=\n    []\n\n    count()  返回有多少个1\n\n    any()  判断是否至少有一个1\n    none()  判断是否全为0\n\n    set()  把所有位置成1\n    set(k, v)  将第k位变成v\n    reset()  把所有位变成0\n    flip()  等价于~\n    flip(k) 把第k位取反\n```","source":"_posts/algorithm-data-structure.md","raw":"---\ntitle: Algorithm-Data-Structure\nmathjax: true\ndate: 2023/07/07 20:46:25\nimg: https://cdn.acwing.com/media/activity/surface/QQ%E5%9B%BE%E7%89%8720231022233411.jpg\nexcerpt: 算法竞赛基础数据结构\n---\n\n## 链表与邻接表\n\n由于用`结构体+指针`比较慢，一般在面试题使用，在这里使用**数组**模拟链表\n\n- 单链表\n\n`e[N]`：储存链表结点的值\n\n`ne[N]`：储存结点的下一个结点下标，其中空结点下标为-1\n\n```cpp\n// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点\nint head, e[N], ne[N], idx;\n\n// 初始化\nvoid init()\n{\n    head = -1;\n    idx = 0;\n}\n\n// 在链表头插入一个数a\nvoid insert(int a)\n{\n    e[idx] = a, ne[idx] = head, head = idx ++ ;\n}\n\n//在a插到下标是k的结点后面\nvoid insert(int a,int k)\n{\n    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;\n}\n\n// 将头结点删除，需要保证头结点存在\nvoid remove()\n{\n    head = ne[head];\n}\n\n// 将下标为k的结点的后一个点删除\nvoid remove(k)\n{\n    ne[k] = ne[ne[k]];\n}\n```\n\n- 双链表\n\n作用：优化某些问题\n\n```cpp\n// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点\nint e[N], l[N], r[N], idx;\n\n// 初始化\nvoid init()\n{\n    //0是左端点，1是右端点\n    r[0] = 1, l[1] = 0;\n    idx = 2;\n}\n\n// 在节点a的右边插入一个数x\nvoid insert(int a, int x)\n{\n    e[idx] = x;\n    l[idx] = a, r[idx] = r[a];\n    l[r[a]] = idx, r[a] = idx ++ ;\n}\n\n// 删除节点a\nvoid remove(int a)\n{\n    l[r[a]] = l[a];\n    r[l[a]] = r[a];\n}\n```\n\n- 邻接表\n\nN个单链表，用于存储树和图\n\n## 栈\n\n先进后出(FILO)\n\n```cpp\n// tt表示栈顶\nint stk[N], tt = 0;\n\n// 向栈顶插入一个数\nstk[ ++ tt] = x;\n\n// 从栈顶弹出一个数\ntt -- ;\n\n// 栈顶的值\nstk[tt];\n\n// 判断栈是否为空，如果 tt > 0，则表示不为空\nif (tt > 0)\n{\n\tnot empty\n}else\n{\n    empty\n}\n```\n\n## 队列\n\n先进先出(FIFO)\n\n```cpp\n// hh 表示队头，tt表示队尾\nint q[N], hh = 0, tt = -1;\n\n// 向队尾插入一个数\nq[ ++ tt] = x;\n\n// 从队头弹出一个数\nhh ++ ;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果 hh <= tt，则表示不为空\nif (hh <= tt)\n{\n\n}\n```\n\n循环队列\n\n```cpp\n// hh 表示队头，tt表示队尾的后一个位置\nint q[N], hh = 0, tt = 0;\n\n// 向队尾插入一个数\nq[tt ++ ] = x;\nif (tt == N) tt = 0;\n\n// 从队头弹出一个数\nhh ++ ;\nif (hh == N) hh = 0;\n\n// 队头的值\nq[hh];\n\n// 判断队列是否为空，如果hh != tt，则表示不为空\nif (hh != tt)\n{\n\n}\n```\n\n## 单调栈\n\n题型：求给定序列每一个数左/右边离他最近的比他大/小的数\n\n```cpp\nint tt = 0;\nfor (int i = 1; i <= n; i ++ )\n{\n    while (tt && check(stk[tt], i)) tt -- ;\n    stk[ ++ tt] = i;\n}\n```\n\n## 单调队列\n\n题型：求滑动窗口的最大/小值\n\n```cpp\nint hh = 0, tt = -1;\nfor (int i = 0; i < n; i ++ )\n{\n    while (hh <= tt && check_out(q[hh])) hh ++ ;  // 判断队头是否滑出窗口\n    while (hh <= tt && check(q[tt], i)) tt -- ;\n    q[ ++ tt] = i;\n}\n```\n\n## KMP\n\n对于字符串$s$,判断是否包含模式串$t$\n\n```cpp\n// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度\n//求模式串的Next数组：\nfor (int i = 2, j = 0; i <= m; i ++ )\n{\n    while (j && p[i] != p[j + 1]) j = ne[j];\n    if (p[i] == p[j + 1]) j ++ ;\n    ne[i] = j;\n}\n\n// 匹配\nfor (int i = 1, j = 0; i <= n; i ++ )\n{\n    while (j && s[i] != p[j + 1]) j = ne[j];\n    if (s[i] == p[j + 1]) j ++ ;\n    if (j == m)\n    {\n        j = ne[j];\n        // 匹配成功后的逻辑\n    }\n}\n```\n\n```cpp\ns.find(t) != s.npos\n```\n\n## Trie树\n\n快速存储和查找字符串集合的数据结构\n\n```cpp\nint son[N][26], cnt[N], idx;\n// 0号点既是根节点，又是空节点\n// son[][]存储树中每个节点的子节点\n// cnt[]存储以每个节点结尾的单词数量\n\n// 插入一个字符串\nvoid insert(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) son[p][u] = ++ idx;\n        p = son[p][u];\n    }\n    cnt[p] ++ ;\n}\n\n// 查询字符串出现的次数\nint query(char *str)\n{\n    int p = 0;\n    for (int i = 0; str[i]; i ++ )\n    {\n        int u = str[i] - 'a';\n        if (!son[p][u]) return 0;\n        p = son[p][u];\n    }\n    return cnt[p];\n}\n```\n\n[最大异或对](https://www.acwing.com/problem/content/145/)\n\n[前缀统计](https://www.acwing.com/problem/content/144/)\n\n## 并查集\n\n1. 将两个集合合并\n2. 询问两个元素是否在一个集合当中\n\n近乎$O(1)$\n\n基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点\n\n- 如何判断是树根？\n\n$p[x] = x$\n\n- 如何求$x$的集合编号\n\n```cpp\nwhile(p[x]!=x) x = p[x]\n```\n\n- 如何合并两个区间\n\n设p[x]为x集合编号，p[y]是y集合编号。p[x]=y\n\n优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根\n\n```cpp\n(1)朴素并查集：\n\n    int p[N]; //存储每个点的祖宗节点\n\n    // 返回x的祖宗节点+路径压缩\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ ) p[i] = i;\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n\n\n(2)维护size的并查集：\n\n    int p[N], size[N];\n    //p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x) p[x] = find(p[x]);\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        size[i] = 1;\n    }\n\n    // 合并a和b所在的两个集合：\n    size[find(b)] += size[find(a)];\n    p[find(a)] = find(b);\n\n\n(3)维护到祖宗节点距离的并查集：\n\n    int p[N], d[N];\n    //p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离\n\n    // 返回x的祖宗节点\n    int find(int x)\n    {\n        if (p[x] != x)\n        {\n            int u = find(p[x]);\n            d[x] += d[p[x]];\n            p[x] = u;\n        }\n        return p[x];\n    }\n\n    // 初始化，假定节点编号是1~n\n    for (int i = 1; i <= n; i ++ )\n    {\n        p[i] = i;\n        d[i] = 0;\n    }\n\n    // 合并a和b所在的两个集合：\n    p[find(a)] = find(b);\n    d[find(a)] = distance; // 根据具体问题，初始化find(a)的偏移量\n```\n\n## C++ STL\n\n```cpp\nvector, 变长数组，倍增的思想\n    size()  返回元素个数\n    empty()  返回是否为空\n    clear()  清空\n    front()/back()\n    push_back()/pop_back()\n    begin()/end()\n    []\n    支持比较运算，按字典序\n\npair<int, int>\n    first, 第一个元素\n    second, 第二个元素\n    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）\n\nstring，字符串\n    size()/length()  返回字符串长度\n    empty()\n    clear()\n    substr(起始下标，(子串长度))  返回子串\n    c_str()  返回字符串所在字符数组的起始地址\n\nqueue, 队列\n    size()\n    empty()\n    push()  向队尾插入一个元素\n    front()  返回队头元素\n    back()  返回队尾元素\n    pop()  弹出队头元素\n\npriority_queue, 优先队列，默认是大根堆\n    size()\n    empty()\n    push()  插入一个元素\n    top()  返回堆顶元素\n    pop()  弹出堆顶元素\n    定义成小根堆的方式：priority_queue<int, vector<int>, greater<int>> q;\n\nstack, 栈\n    size()\n    empty()\n    push()  向栈顶插入一个元素\n    top()  返回栈顶元素\n    pop()  弹出栈顶元素\n\ndeque, 双端队列\n    size()\n    empty()\n    clear()\n    front()/back()\n    push_back()/pop_back()\n    push_front()/pop_front()\n    begin()/end()\n    []\n\nset, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列\n    size()\n    empty()\n    clear()\n    begin()/end()\n    ++, -- 返回前驱和后继，时间复杂度 O(logn)\n\n    set/multiset\n        insert()  插入一个数\n        find()  查找一个数\n        count()  返回某一个数的个数\n        erase()\n            (1) 输入是一个数x，删除所有x   O(k + logn)\n            (2) 输入一个迭代器，删除这个迭代器\n        lower_bound()/upper_bound()\n            lower_bound(x)  返回大于等于x的最小的数的迭代器\n            upper_bound(x)  返回大于x的最小的数的迭代器\n    map/multimap\n        insert()  插入的数是一个pair\n        erase()  输入的参数是pair或者迭代器\n        find()\n        []  注意multimap不支持此操作。 时间复杂度是 O(logn)\n        lower_bound()/upper_bound()\n\nunordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表\n    和上面类似，增删改查的时间复杂度是 O(1)\n    不支持 lower_bound()/upper_bound()， 迭代器的++，--\n\nbitset, 圧位\n    bitset<10000> s;\n    ~, &, |, ^\n    >>, <<\n    ==, !=\n    []\n\n    count()  返回有多少个1\n\n    any()  判断是否至少有一个1\n    none()  判断是否全为0\n\n    set()  把所有位置成1\n    set(k, v)  将第k位变成v\n    reset()  把所有位变成0\n    flip()  等价于~\n    flip(k) 把第k位取反\n```","slug":"algorithm-data-structure","published":1,"updated":"2025-02-20T06:45:40.656Z","comments":1,"layout":"post","photos":[],"_id":"cm7o4qpo100062o991r2mgkzc","content":"<h2 id=\"链表与邻接表\"><a href=\"#链表与邻接表\" class=\"headerlink\" title=\"链表与邻接表\"></a>链表与邻接表</h2><p>由于用<code>结构体+指针</code>比较慢，一般在面试题使用，在这里使用<strong>数组</strong>模拟链表</p>\n<ul>\n<li>单链表</li>\n</ul>\n<p><code>e[N]</code>：储存链表结点的值</p>\n<p><code>ne[N]</code>：储存结点的下一个结点下标，其中空结点下标为-1</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> head, e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = <span class=\"number\">-1</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在链表头插入一个数a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = head, head = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//在a插到下标是k的结点后面</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将头结点删除，需要保证头结点存在</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = ne[head];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将下标为k的结点的后一个点删除</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ne[k] = ne[ne[k]];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双链表</li>\n</ul>\n<p>作用：优化某些问题</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> e[N], l[N], r[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//0是左端点，1是右端点</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">1</span>, l[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在节点a的右边插入一个数x</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = x;</span><br><span class=\"line\">    l[idx] = a, r[idx] = r[a];</span><br><span class=\"line\">    l[r[a]] = idx, r[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 删除节点a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    l[r[a]] = l[a];</span><br><span class=\"line\">    r[l[a]] = r[a];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>邻接表</li>\n</ul>\n<p>N个单链表，用于存储树和图</p>\n<h2 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h2><p>先进后出(FILO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tt表示栈顶</span></span><br><span class=\"line\"><span class=\"type\">int</span> stk[N], tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向栈顶插入一个数</span></span><br><span class=\"line\">stk[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从栈顶弹出一个数</span></span><br><span class=\"line\">tt -- ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 栈顶的值</span></span><br><span class=\"line\">stk[tt];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断栈是否为空，如果 tt &gt; 0，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">not</span> empty</span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    empty</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"队列\"><a href=\"#队列\" class=\"headerlink\" title=\"队列\"></a>队列</h2><p>先进先出(FIFO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果 hh &lt;= tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh &lt;= tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>循环队列</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾的后一个位置</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[tt ++ ] = x;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt == N) tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh == N) hh = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果hh != tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh != tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调栈\"><a href=\"#单调栈\" class=\"headerlink\" title=\"单调栈\"></a>单调栈</h2><p>题型：求给定序列每一个数左&#x2F;右边离他最近的比他大&#x2F;小的数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (tt &amp;&amp; <span class=\"built_in\">check</span>(stk[tt], i)) tt -- ;</span><br><span class=\"line\">    stk[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调队列\"><a href=\"#单调队列\" class=\"headerlink\" title=\"单调队列\"></a>单调队列</h2><p>题型：求滑动窗口的最大&#x2F;小值</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check_out</span>(q[hh])) hh ++ ;  <span class=\"comment\">// 判断队头是否滑出窗口</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check</span>(q[tt], i)) tt -- ;</span><br><span class=\"line\">    q[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><p>对于字符串$s$,判断是否包含模式串$t$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度</span></span><br><span class=\"line\"><span class=\"comment\">//求模式串的Next数组：</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>, j = <span class=\"number\">0</span>; i &lt;= m; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; p[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    ne[i] = j;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 匹配</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>, j = <span class=\"number\">0</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; s[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (j == m)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        j = ne[j];</span><br><span class=\"line\">        <span class=\"comment\">// 匹配成功后的逻辑</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">s.<span class=\"built_in\">find</span>(t) != s.npos</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Trie树\"><a href=\"#Trie树\" class=\"headerlink\" title=\"Trie树\"></a>Trie树</h2><p>快速存储和查找字符串集合的数据结构</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> son[N][<span class=\"number\">26</span>], cnt[N], idx;</span><br><span class=\"line\"><span class=\"comment\">// 0号点既是根节点，又是空节点</span></span><br><span class=\"line\"><span class=\"comment\">// son[][]存储树中每个节点的子节点</span></span><br><span class=\"line\"><span class=\"comment\">// cnt[]存储以每个节点结尾的单词数量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 插入一个字符串</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) son[p][u] = ++ idx;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cnt[p] ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 查询字符串出现的次数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">query</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cnt[p];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/145/\">最大异或对</a></p>\n<p><a href=\"https://www.acwing.com/problem/content/144/\">前缀统计</a></p>\n<h2 id=\"并查集\"><a href=\"#并查集\" class=\"headerlink\" title=\"并查集\"></a>并查集</h2><ol>\n<li>将两个集合合并</li>\n<li>询问两个元素是否在一个集合当中</li>\n</ol>\n<p>近乎$O(1)$</p>\n<p>基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点</p>\n<ul>\n<li>如何判断是树根？</li>\n</ul>\n<p>$p[x] &#x3D; x$</p>\n<ul>\n<li>如何求$x$的集合编号</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(p[x]!=x) x = p[x]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何合并两个区间</li>\n</ul>\n<p>设p[x]为x集合编号，p[y]是y集合编号。p[x]&#x3D;y</p>\n<p>优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">1</span>)朴素并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N]; <span class=\"comment\">//存储每个点的祖宗节点</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点+路径压缩</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">2</span>)维护size的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], size[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        size[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    size[<span class=\"built_in\">find</span>(b)] += size[<span class=\"built_in\">find</span>(a)];</span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">3</span>)维护到祖宗节点距离的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], d[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> u = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">            d[x] += d[p[x]];</span><br><span class=\"line\">            p[x] = u;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        d[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">    d[<span class=\"built_in\">find</span>(a)] = distance; <span class=\"comment\">// 根据具体问题，初始化find(a)的偏移量</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"C-STL\"><a href=\"#C-STL\" class=\"headerlink\" title=\"C++ STL\"></a>C++ STL</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vector, 变长数组，倍增的思想</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()  返回元素个数</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()  返回是否为空</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()  清空</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\">    支持比较运算，按字典序</span><br><span class=\"line\"></span><br><span class=\"line\">pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt;</span><br><span class=\"line\">    first, 第一个元素</span><br><span class=\"line\">    second, 第二个元素</span><br><span class=\"line\">    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）</span><br><span class=\"line\"></span><br><span class=\"line\">string，字符串</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()/<span class=\"built_in\">length</span>()  返回字符串长度</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">substr</span>(起始下标，(子串长度))  返回子串</span><br><span class=\"line\">    <span class=\"built_in\">c_str</span>()  返回字符串所在字符数组的起始地址</span><br><span class=\"line\"></span><br><span class=\"line\">queue, 队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向队尾插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()  返回队头元素</span><br><span class=\"line\">    <span class=\"built_in\">back</span>()  返回队尾元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出队头元素</span><br><span class=\"line\"></span><br><span class=\"line\">priority_queue, 优先队列，默认是大根堆</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回堆顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出堆顶元素</span><br><span class=\"line\">    定义成小根堆的方式：priority_queue&lt;<span class=\"type\">int</span>, vector&lt;<span class=\"type\">int</span>&gt;, greater&lt;<span class=\"type\">int</span>&gt;&gt; q;</span><br><span class=\"line\"></span><br><span class=\"line\">stack, 栈</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向栈顶插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回栈顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出栈顶元素</span><br><span class=\"line\"></span><br><span class=\"line\">deque, 双端队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_front</span>()/<span class=\"built_in\">pop_front</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    ++, -- 返回前驱和后继，时间复杂度 <span class=\"built_in\">O</span>(logn)</span><br><span class=\"line\"></span><br><span class=\"line\">    set/<span class=\"function\">multiset</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span>  查找一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">count</span><span class=\"params\">()</span>  返回某一个数的个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">1</span>)</span> 输入是一个数x，删除所有x   <span class=\"title\">O</span><span class=\"params\">(k + logn)</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">2</span>)</span> 输入一个迭代器，删除这个迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">lower_bound</span><span class=\"params\">(x)</span>  返回大于等于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">upper_bound</span><span class=\"params\">(x)</span>  返回大于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">    map/multimap</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入的数是一个pair</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span>  输入的参数是pair或者迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        []  注意multimap不支持此操作。 时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(logn)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">unordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表</span></span><br><span class=\"line\"><span class=\"function\">    和上面类似，增删改查的时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(<span class=\"number\">1</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    不支持 <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span>， 迭代器的++，--</span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">bitset, 圧位</span></span><br><span class=\"line\"><span class=\"function\">    bitset&lt;10000&gt; s</span>;</span><br><span class=\"line\">    ~, &amp;, |, ^</span><br><span class=\"line\">    &gt;&gt;, &lt;&lt;</span><br><span class=\"line\">    ==, !=</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">count</span>()  返回有多少个<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">any</span>()  判断是否至少有一个<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">none</span>()  判断是否全为<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">set</span>()  把所有位置成<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">set</span>(k, v)  将第k位变成v</span><br><span class=\"line\">    <span class=\"built_in\">reset</span>()  把所有位变成<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"built_in\">flip</span>()  等价于~</span><br><span class=\"line\">    <span class=\"built_in\">flip</span>(k) 把第k位取反</span><br></pre></td></tr></table></figure>","more":"<h2 id=\"链表与邻接表\"><a href=\"#链表与邻接表\" class=\"headerlink\" title=\"链表与邻接表\"></a>链表与邻接表</h2><p>由于用<code>结构体+指针</code>比较慢，一般在面试题使用，在这里使用<strong>数组</strong>模拟链表</p>\n<ul>\n<li>单链表</li>\n</ul>\n<p><code>e[N]</code>：储存链表结点的值</p>\n<p><code>ne[N]</code>：储存结点的下一个结点下标，其中空结点下标为-1</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> head, e[N], ne[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = <span class=\"number\">-1</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">0</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在链表头插入一个数a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = head, head = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">//在a插到下标是k的结点后面</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a,<span class=\"type\">int</span> k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = a, ne[idx] = ne[k], ne[k] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将头结点删除，需要保证头结点存在</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    head = ne[head];</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 将下标为k的结点的后一个点删除</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(k)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    ne[k] = ne[ne[k]];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>双链表</li>\n</ul>\n<p>作用：优化某些问题</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个节点</span></span><br><span class=\"line\"><span class=\"type\">int</span> e[N], l[N], r[N], idx;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 初始化</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">init</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"comment\">//0是左端点，1是右端点</span></span><br><span class=\"line\">    r[<span class=\"number\">0</span>] = <span class=\"number\">1</span>, l[<span class=\"number\">1</span>] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    idx = <span class=\"number\">2</span>;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 在节点a的右边插入一个数x</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">int</span> a, <span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    e[idx] = x;</span><br><span class=\"line\">    l[idx] = a, r[idx] = r[a];</span><br><span class=\"line\">    l[r[a]] = idx, r[a] = idx ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 删除节点a</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">remove</span><span class=\"params\">(<span class=\"type\">int</span> a)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    l[r[a]] = l[a];</span><br><span class=\"line\">    r[l[a]] = r[a];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>邻接表</li>\n</ul>\n<p>N个单链表，用于存储树和图</p>\n<h2 id=\"栈\"><a href=\"#栈\" class=\"headerlink\" title=\"栈\"></a>栈</h2><p>先进后出(FILO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// tt表示栈顶</span></span><br><span class=\"line\"><span class=\"type\">int</span> stk[N], tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向栈顶插入一个数</span></span><br><span class=\"line\">stk[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从栈顶弹出一个数</span></span><br><span class=\"line\">tt -- ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 栈顶的值</span></span><br><span class=\"line\">stk[tt];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断栈是否为空，如果 tt &gt; 0，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt &gt; <span class=\"number\">0</span>)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">not</span> empty</span><br><span class=\"line\">&#125;<span class=\"keyword\">else</span></span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    empty</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"队列\"><a href=\"#队列\" class=\"headerlink\" title=\"队列\"></a>队列</h2><p>先进先出(FIFO)</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[ ++ tt] = x;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果 hh &lt;= tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh &lt;= tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>循环队列</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// hh 表示队头，tt表示队尾的后一个位置</span></span><br><span class=\"line\"><span class=\"type\">int</span> q[N], hh = <span class=\"number\">0</span>, tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 向队尾插入一个数</span></span><br><span class=\"line\">q[tt ++ ] = x;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (tt == N) tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 从队头弹出一个数</span></span><br><span class=\"line\">hh ++ ;</span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh == N) hh = <span class=\"number\">0</span>;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 队头的值</span></span><br><span class=\"line\">q[hh];</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 判断队列是否为空，如果hh != tt，则表示不为空</span></span><br><span class=\"line\"><span class=\"keyword\">if</span> (hh != tt)</span><br><span class=\"line\">&#123;</span><br><span class=\"line\"></span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调栈\"><a href=\"#单调栈\" class=\"headerlink\" title=\"单调栈\"></a>单调栈</h2><p>题型：求给定序列每一个数左&#x2F;右边离他最近的比他大&#x2F;小的数</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> tt = <span class=\"number\">0</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (tt &amp;&amp; <span class=\"built_in\">check</span>(stk[tt], i)) tt -- ;</span><br><span class=\"line\">    stk[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"单调队列\"><a href=\"#单调队列\" class=\"headerlink\" title=\"单调队列\"></a>单调队列</h2><p>题型：求滑动窗口的最大&#x2F;小值</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> hh = <span class=\"number\">0</span>, tt = <span class=\"number\">-1</span>;</span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; i &lt; n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check_out</span>(q[hh])) hh ++ ;  <span class=\"comment\">// 判断队头是否滑出窗口</span></span><br><span class=\"line\">    <span class=\"keyword\">while</span> (hh &lt;= tt &amp;&amp; <span class=\"built_in\">check</span>(q[tt], i)) tt -- ;</span><br><span class=\"line\">    q[ ++ tt] = i;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"KMP\"><a href=\"#KMP\" class=\"headerlink\" title=\"KMP\"></a>KMP</h2><p>对于字符串$s$,判断是否包含模式串$t$</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度</span></span><br><span class=\"line\"><span class=\"comment\">//求模式串的Next数组：</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">2</span>, j = <span class=\"number\">0</span>; i &lt;= m; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; p[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (p[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    ne[i] = j;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 匹配</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>, j = <span class=\"number\">0</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">&#123;</span><br><span class=\"line\">    <span class=\"keyword\">while</span> (j &amp;&amp; s[i] != p[j + <span class=\"number\">1</span>]) j = ne[j];</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (s[i] == p[j + <span class=\"number\">1</span>]) j ++ ;</span><br><span class=\"line\">    <span class=\"keyword\">if</span> (j == m)</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        j = ne[j];</span><br><span class=\"line\">        <span class=\"comment\">// 匹配成功后的逻辑</span></span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">s.<span class=\"built_in\">find</span>(t) != s.npos</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Trie树\"><a href=\"#Trie树\" class=\"headerlink\" title=\"Trie树\"></a>Trie树</h2><p>快速存储和查找字符串集合的数据结构</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"type\">int</span> son[N][<span class=\"number\">26</span>], cnt[N], idx;</span><br><span class=\"line\"><span class=\"comment\">// 0号点既是根节点，又是空节点</span></span><br><span class=\"line\"><span class=\"comment\">// son[][]存储树中每个节点的子节点</span></span><br><span class=\"line\"><span class=\"comment\">// cnt[]存储以每个节点结尾的单词数量</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 插入一个字符串</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">void</span> <span class=\"title\">insert</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) son[p][u] = ++ idx;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    cnt[p] ++ ;</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">// 查询字符串出现的次数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">query</span><span class=\"params\">(<span class=\"type\">char</span> *str)</span></span></span><br><span class=\"line\"><span class=\"function\"></span>&#123;</span><br><span class=\"line\">    <span class=\"type\">int</span> p = <span class=\"number\">0</span>;</span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">0</span>; str[i]; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        <span class=\"type\">int</span> u = str[i] - <span class=\"string\">&#x27;a&#x27;</span>;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (!son[p][u]) <span class=\"keyword\">return</span> <span class=\"number\">0</span>;</span><br><span class=\"line\">        p = son[p][u];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    <span class=\"keyword\">return</span> cnt[p];</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p><a href=\"https://www.acwing.com/problem/content/145/\">最大异或对</a></p>\n<p><a href=\"https://www.acwing.com/problem/content/144/\">前缀统计</a></p>\n<h2 id=\"并查集\"><a href=\"#并查集\" class=\"headerlink\" title=\"并查集\"></a>并查集</h2><ol>\n<li>将两个集合合并</li>\n<li>询问两个元素是否在一个集合当中</li>\n</ol>\n<p>近乎$O(1)$</p>\n<p>基本原理：每一个集合用一棵树表示，树根的编号就是整个集合的编号。每个节点存储它的父节点$p[x]$表示$x$的父节点</p>\n<ul>\n<li>如何判断是树根？</li>\n</ul>\n<p>$p[x] &#x3D; x$</p>\n<ul>\n<li>如何求$x$的集合编号</li>\n</ul>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">while</span>(p[x]!=x) x = p[x]</span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>如何合并两个区间</li>\n</ul>\n<p>设p[x]为x集合编号，p[y]是y集合编号。p[x]&#x3D;y</p>\n<p>优化：路径压缩，先搜索一遍，再将节点的父节点直接指向树根</p>\n<figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(<span class=\"number\">1</span>)朴素并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N]; <span class=\"comment\">//存储每个点的祖宗节点</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点+路径压缩</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ ) p[i] = i;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">2</span>)维护size的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], size[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, size[]只有祖宗节点的有意义，表示祖宗节点所在集合中的点的数量</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x) p[x] = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        size[i] = <span class=\"number\">1</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    size[<span class=\"built_in\">find</span>(b)] += size[<span class=\"built_in\">find</span>(a)];</span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\">(<span class=\"number\">3</span>)维护到祖宗节点距离的并查集：</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"type\">int</span> p[N], d[N];</span><br><span class=\"line\">    <span class=\"comment\">//p[]存储每个点的祖宗节点, d[x]存储x到p[x]的距离</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 返回x的祖宗节点</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"type\">int</span> <span class=\"title\">find</span><span class=\"params\">(<span class=\"type\">int</span> x)</span></span></span><br><span class=\"line\"><span class=\"function\">    </span>&#123;</span><br><span class=\"line\">        <span class=\"keyword\">if</span> (p[x] != x)</span><br><span class=\"line\">        &#123;</span><br><span class=\"line\">            <span class=\"type\">int</span> u = <span class=\"built_in\">find</span>(p[x]);</span><br><span class=\"line\">            d[x] += d[p[x]];</span><br><span class=\"line\">            p[x] = u;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        <span class=\"keyword\">return</span> p[x];</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 初始化，假定节点编号是1~n</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> (<span class=\"type\">int</span> i = <span class=\"number\">1</span>; i &lt;= n; i ++ )</span><br><span class=\"line\">    &#123;</span><br><span class=\"line\">        p[i] = i;</span><br><span class=\"line\">        d[i] = <span class=\"number\">0</span>;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"comment\">// 合并a和b所在的两个集合：</span></span><br><span class=\"line\">    p[<span class=\"built_in\">find</span>(a)] = <span class=\"built_in\">find</span>(b);</span><br><span class=\"line\">    d[<span class=\"built_in\">find</span>(a)] = distance; <span class=\"comment\">// 根据具体问题，初始化find(a)的偏移量</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"C-STL\"><a href=\"#C-STL\" class=\"headerlink\" title=\"C++ STL\"></a>C++ STL</h2><figure class=\"highlight cpp\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vector, 变长数组，倍增的思想</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()  返回元素个数</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()  返回是否为空</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()  清空</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\">    支持比较运算，按字典序</span><br><span class=\"line\"></span><br><span class=\"line\">pair&lt;<span class=\"type\">int</span>, <span class=\"type\">int</span>&gt;</span><br><span class=\"line\">    first, 第一个元素</span><br><span class=\"line\">    second, 第二个元素</span><br><span class=\"line\">    支持比较运算，以first为第一关键字，以second为第二关键字（字典序）</span><br><span class=\"line\"></span><br><span class=\"line\">string，字符串</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()/<span class=\"built_in\">length</span>()  返回字符串长度</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">substr</span>(起始下标，(子串长度))  返回子串</span><br><span class=\"line\">    <span class=\"built_in\">c_str</span>()  返回字符串所在字符数组的起始地址</span><br><span class=\"line\"></span><br><span class=\"line\">queue, 队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向队尾插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()  返回队头元素</span><br><span class=\"line\">    <span class=\"built_in\">back</span>()  返回队尾元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出队头元素</span><br><span class=\"line\"></span><br><span class=\"line\">priority_queue, 优先队列，默认是大根堆</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回堆顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出堆顶元素</span><br><span class=\"line\">    定义成小根堆的方式：priority_queue&lt;<span class=\"type\">int</span>, vector&lt;<span class=\"type\">int</span>&gt;, greater&lt;<span class=\"type\">int</span>&gt;&gt; q;</span><br><span class=\"line\"></span><br><span class=\"line\">stack, 栈</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push</span>()  向栈顶插入一个元素</span><br><span class=\"line\">    <span class=\"built_in\">top</span>()  返回栈顶元素</span><br><span class=\"line\">    <span class=\"built_in\">pop</span>()  弹出栈顶元素</span><br><span class=\"line\"></span><br><span class=\"line\">deque, 双端队列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">front</span>()/<span class=\"built_in\">back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_back</span>()/<span class=\"built_in\">pop_back</span>()</span><br><span class=\"line\">    <span class=\"built_in\">push_front</span>()/<span class=\"built_in\">pop_front</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列</span><br><span class=\"line\">    <span class=\"built_in\">size</span>()</span><br><span class=\"line\">    <span class=\"built_in\">empty</span>()</span><br><span class=\"line\">    <span class=\"built_in\">clear</span>()</span><br><span class=\"line\">    <span class=\"built_in\">begin</span>()/<span class=\"built_in\">end</span>()</span><br><span class=\"line\">    ++, -- 返回前驱和后继，时间复杂度 <span class=\"built_in\">O</span>(logn)</span><br><span class=\"line\"></span><br><span class=\"line\">    set/<span class=\"function\">multiset</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span>  查找一个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">count</span><span class=\"params\">()</span>  返回某一个数的个数</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">1</span>)</span> 输入是一个数x，删除所有x   <span class=\"title\">O</span><span class=\"params\">(k + logn)</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"params\">(<span class=\"number\">2</span>)</span> 输入一个迭代器，删除这个迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">lower_bound</span><span class=\"params\">(x)</span>  返回大于等于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">            <span class=\"title\">upper_bound</span><span class=\"params\">(x)</span>  返回大于x的最小的数的迭代器</span></span><br><span class=\"line\"><span class=\"function\">    map/multimap</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">insert</span><span class=\"params\">()</span>  插入的数是一个pair</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">erase</span><span class=\"params\">()</span>  输入的参数是pair或者迭代器</span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">find</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\">        []  注意multimap不支持此操作。 时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(logn)</span></span></span><br><span class=\"line\"><span class=\"function\">        <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span></span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">unordered_set, unordered_map, unordered_multiset, unordered_multimap, 哈希表</span></span><br><span class=\"line\"><span class=\"function\">    和上面类似，增删改查的时间复杂度是 <span class=\"title\">O</span><span class=\"params\">(<span class=\"number\">1</span>)</span></span></span><br><span class=\"line\"><span class=\"function\">    不支持 <span class=\"title\">lower_bound</span><span class=\"params\">()</span>/<span class=\"title\">upper_bound</span><span class=\"params\">()</span>， 迭代器的++，--</span></span><br><span class=\"line\"><span class=\"function\"></span></span><br><span class=\"line\"><span class=\"function\">bitset, 圧位</span></span><br><span class=\"line\"><span class=\"function\">    bitset&lt;10000&gt; s</span>;</span><br><span class=\"line\">    ~, &amp;, |, ^</span><br><span class=\"line\">    &gt;&gt;, &lt;&lt;</span><br><span class=\"line\">    ==, !=</span><br><span class=\"line\">    []</span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">count</span>()  返回有多少个<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">any</span>()  判断是否至少有一个<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">none</span>()  判断是否全为<span class=\"number\">0</span></span><br><span class=\"line\"></span><br><span class=\"line\">    <span class=\"built_in\">set</span>()  把所有位置成<span class=\"number\">1</span></span><br><span class=\"line\">    <span class=\"built_in\">set</span>(k, v)  将第k位变成v</span><br><span class=\"line\">    <span class=\"built_in\">reset</span>()  把所有位变成<span class=\"number\">0</span></span><br><span class=\"line\">    <span class=\"built_in\">flip</span>()  等价于~</span><br><span class=\"line\">    <span class=\"built_in\">flip</span>(k) 把第k位取反</span><br></pre></td></tr></table></figure>"},{"title":"Machine Learning Notes","mathjax":true,"date":"2023-11-25T12:46:25.000Z","img":"https://img2.baidu.com/it/u=1093757134,3274186314&fm=253&fmt=auto&app=120&f=JPEG?w=800&h=500","excerpt":"吴恩达机器学习视频笔记","_content":"\n# Course 1\n\n监督学习：输入特征x，输出目标y。对数据集进行预测，分为**回归**和**分类**\n\n无监督学习：输入特征x，没有目标y，对数据集进行**聚类预测**，**异常检测**，**降维**\n\n## 线性回归\n\n$$\ny^i = wx^i+b\n$$\n\n定义损失函数（成本函数），需要最小化损失函数\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2\n$$\n\n其中$y^i$为真实输出，$\\hat{y}$为预测输出\n\n- 为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差\n- 1/2是为了方便求导计算\n\nloss针对一个训练样本，cost是所有训练样本的均值\n\n### 梯度下降\n\n需要最小会损失函数，需要使用梯度下降算法\n\n定义学习率`learning_rate`为$\\alpha$,一般$\\alpha \\subseteq [0,1]$\n\n$w = w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$\n\n$b = b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$\n\n- 梯度下降时建议**同步**梯度下降，如下图\n\n![img](/img/machine-learning-notes/pic-1.png)\n\n如果$\\alpha$太小，可以得到答案，但是时间过长\n\n如果$\\alpha$太大，大交叉无法收敛，甚至发散\n\n当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解\n\n对导数项分别求导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = wx^i+b$\n\n对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值\n\n选择合适得到学习率，就可以得到$min(J(w,b))$\n\n线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例\n\n### 多元线性回归\n\n假设特征有$n$个，定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n则$f_{\\vec{w},b}=\\vec{w} \\cdot \\vec{x} +b$\n\n`·`为两个向量的点积(dot)。\n\n$\\vec{w} \\cdot \\vec{x} = w_1*x_1+w_2*x_2+....+w_n*x_n$\n\n**矢量化**：利用计算机的并行硬件，代码简洁、运行速度快\n\n```python\nf = np.dot(w, x) + b\n```\n\n**多元线性回归的梯度下降**\n\n![img](/img/machine-learning-notes/pic-2.png)\n\nPS: 正规方程：某些机器学习库在后端求$w,b$的方法，**只适用于线性回归**，而且速度慢，不要求掌握\n\n### 特征缩放\n\n不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动\n\n加快梯度下降速度\n\n避免特征的取值范围差异过大，将其进行缩放，几种常见方法：\n\n- **除以最大值**，$x_{1,scale} = \\frac{x_1}{max}$， $x \\in [0,1]$\n- **均值归一化Mean Normalization**\n  - 求均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{max-min}$\n- **`Z-score`归一化**\n  - 求标准差$\\sigma$，均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{\\sigma}$\n\n**判断梯度下降是否收敛：**\n\n1. 观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）\n\n**选择合适学习率**：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$\n\n### 特征工程\n\n利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测\n\n**多项式回归**：可以添加$x^q$项更好地拟合数据图像，$f(x)=w_1x^3+w_2x^2+w_1x^1+b$\n\n此时特征缩放尤为重要\n\n## 分类-逻辑回归\n\n解决二分类问题\n\n### sigmoid函数\n\n输出介于$(0,1)$\n\n$g(z)= \\frac{1}{1+e^{-z}},z \\subseteq R$\n\n**logistic regression**:\n\n$f_{\\vec{w},b}(\\vec{x})=g(\\vec{w} · \\vec{x}+b) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n输出值可以理解为分类为1的可能性\n\n$f_{\\vec{w},b}(\\vec{x})=P(y=1|\\vec{x};\\vec{w},b)$\n\n### 决策边界decision boundary\n\n以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b <0$，取值0\n\n$\\vec{w} · \\vec{x}+b = 0$称为决策边界\n\n多项式回归也适用于非线性的决策边界\n\n### 成本函数\n\n如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$**不是凸函数，不适用于逻辑回归**\n\n定义\n\n$$\nJ(w,b)=\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})\n$$\n其中L代表单个样例的loss，J代表总的cost\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}=1\n$$\n![img](/img/machine-learning-notes/pic-3.png)\n\n当y等于1，预测值越靠近1损失越小\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}=0\n$$\n![img](/img/machine-learning-notes/pic-4.png)\n\n当y等于0，预测值越靠近0损失越小 \n\n**简化**成本函数                                                                                                                                                                                                                                                                                          \n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n得到\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))\n$$\n\n\n成本函数是凸函数，便于实现梯度下降\n\n### 梯度下降\n\n对J求偏导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n可以使用相似方法进行特征缩放\n\n### 过拟合问题\n\n过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。\n\n![img](/img/machine-learning-notes/pic-5.png)\n\n**解决过拟合**\n\n- 收集更多训练数据\n- 特征筛选，选择特征的一个子集\n- 正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响\n\n### 正则化\n\n如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n这样使得$w_j$尽可能小，几乎为0\n\n参数$b$是否正则化无关紧要\n\n**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显\n\n- 正则化线性回归\n\n对$J(w,b)$求偏导不断同步更新w,b的值\n\n$$\n\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}\n$$\n\n$$\nw = w- \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}) = (1-\\alpha \\frac{\\lambda}{m})w+.....\n$$\n\n\n- 正则化逻辑回归\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n\n求导式和线性回归相同，只是需要注意**正则化项偏导数没有求和**\n\n$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n![img](/img/machine-learning-notes/pic-6.png)\n\n# Course 2\n\n## 神经网络\n\n起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为**深度学习**\n\n利用别人训练的神经网络参数称为推理或者预测\n\n为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数\n\n神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层->隐藏层->输出层）\n\n多层神经网络叫做多层感知机\n\n## 神经网络中的层\n\n讨论层数通常是隐藏层和输出层，不包括输入层\n\n每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层/输出结果\n\n即$a_j^{[l]} = g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$\n\n$j$表示神经元单元序号，$l$表示层数，$g(x)$为`sigmod`函数\n\n![img](/img/machine-learning-notes/pic-7.jpg)\n\n$a_j^{[l]}$构成$\\vec{a}^{[l]}$\n\n![img](/img/machine-learning-notes/pic-8.png)\n\n## 前向传播(forward prop)\n\n从输入初步传递到输出，即为前向传播\n\n**一般实现**\n\n```python\ndef dense(a_in, W, b, g):\n\tunits = W.shape[1] # 单元数等于W矩阵的列数，w_j向量是列向量\n\ta_out = np.zeros(units)\n\tfor j in range(units):\n\t\tw = W[:, j]\n\t\tz = np.dot(w, a_in) + b\n\t\ta_out[j] = g(z)\n\treturn a_out\ndef sequential(x):\n    a1 = dense(x, W1, b1)\n    a2 = dense(a1, W2, b2)\n    a3 = dense(a2, W3, b3)\n    f_x = a3\n    return f_x\n```\n\n**使用框架（TensorFlow/Pytorch)）进行矢量化加速**\n\n## 模型训练步骤\n\n1. 指定如何在给定输入X和参数的情况下计算输出(模型**结构**)\n2. 指定**损失函数**\n3. **训练**模型以最小化损失函数\n\n二元交叉熵损失\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n通过反向传播计算偏导数\n\n## 激活函数\n\n用`ReLU`函数代替`sigmoid`激活，$g(z) = max(0,z)$\n\n![img](/img/machine-learning-notes/pic-9.png)\n\n**如何选择合适的激活函数？**\n\n取决于要预测的y，对于神经网络的**输出层**：\n\n- 二分类——> sigmoid\n- y可正可负的回归——> linear\n- 回归中y大于等于0 ——> ReLU\n\n对于神经网络的**隐藏层**建议使用ReLU\n\n`ReLU`常用且更快\n\n- 不涉及指数运算\n- 当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x->-∞）,而sigmoid两端都是\n\n**为什么需要激活函数？**\n\n对于隐藏层，只使用线性的所有层等价于线性回归\n\n对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解\n\n## 多分类问题\n\n**softmax回归算法**（logistic 推广）\n\n$z_1=\\vec{w_1}·\\vec{x_1}+b_1$\n\n$a_1=\\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}} = P(y=1|\\vec{x})$\n\n即，设有N个分类\n\n$z_i=\\vec{w_1}·\\vec{x_i}+b_i$\n\n$$\na_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_i}}=P(y=i|\\vec{x})\n$$\n其中$a_1+a_2+...+a_N=1$\n\n**softmax损失函数**\n\n回顾logistic回归\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n二分类问题，设$a_1 = f_{w,b}(x^{(i)})$，即$y=1$的概率\n\n则$a_2 = 1-f_{w,b}(x^{(i)})$，即$y=0$的概率\n\n简化为\n\n$loss = -log(a_1)$ 如果$y=1$\n\n$loss = -log(a_2)$ 如果$y=0$\n\n对于softmax回归算法\n\n$$\nloss(a_1,a_2,...,a_N,y) = \\left\\{\\begin{matrix} -log(a_1) \\quad if \\quad y=1\\\\ -log(a_2) \\quad if \\quad y=2 \\\\ ... \\\\ -log(a_N) \\quad if \\quad y=N \\end{matrix}\\right.\n$$\n\n\n**神经网络中的softmax**\n\n输出层变为N个神经元\n\n注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 ... z_n$的函数\n\n**softmax改进**\n\n由于[数值溢出和精度问题](https://blog.csdn.net/muyuu/article/details/122757470)\n\n$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失\n\n不先计算出$a_i$，再带入损失函数\n\n而是**直接**\n$$\nloss_i=-log(\\frac{e^{z_i}}{e_{z_1}+...+e_{z_N}})\n$$\n\n\n此时输出层只需要`linear`即可（就是不计算$a_i$），同时开启`from_logits=True`\n\n```python\nmodel.compile(loss=SparseCategoricalCrossEntropy(from_logits=True)) #稀疏分类交叉熵损失\n```\n\n`from_logits=True`的[作用](https://blog.csdn.net/muyuu/article/details/122762442)\n\n需要概率时再调用`softmax`\n\n```python\nlogits = model(X)\nf_x = tf.nn.softmax(logits)\n```\n\n**多标签分类**\n\n![img](/img/machine-learning-notes/pic-10.png)\n\n将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。\n\n## 高级优化方法\n\n传统的梯度下降学习率固定\n\n### Adam（Adaptive Moment estimation）\n\n如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率\n\n如果看到学习率太大，某个参数值来回振荡，会自动减小学习率\n\n可以自动调整学习率$\\alpha$\n\n对于每个参数都有一个$\\alpha$\n\n选择optimizer=adam即可\n\n## 其他的网络层\n\n### 卷积层（Convolutional Layer）\n\n每个神经元只能看到前一个层输入的一部分\n\n- 加快计算速度\n- 需要更少的数据，不容易过拟合\n\n有多个卷积层，即卷积神经网络\n\n每一层的单元只查看输入的一部分 \n\n## 构建机器学习系统\n\n### 评估一个模型\n\n特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。\n\n将数据集分为训练集和测试集（73或者82开）\n\n分三步计算\n\n![img](/img/machine-learning-notes/pic-11.png)\n\n**注意计算error时不包括正则化项**\n\n过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力\n\n对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）\n\n### 如何选择模型\n\n数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$\n\n交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫**dev set**/validation set\n\n$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力\n\n数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。\n\n### **偏差和方差**\n\n![img](/img/machine-learning-notes/pic-12.png)\n\n![img](/img/machine-learning-notes/pic-13.png)\n\n高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多\n\n高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。\n\n**正则化项参数对偏差和方差的影响：**\n\n![img](/img/machine-learning-notes/pic-14.png)\n\n但是这些数值多少才算大/小呢？需要**建立基准性能标准**，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差/高偏差的问题\n\n![img](/img/machine-learning-notes/pic-15.png)\n\n**学习曲线**\n\n![img](/img/machine-learning-notes/pic-16.png)\n\n高偏差时 \n\n![img](/img/machine-learning-notes/pic-17.png)\n\n高方差时\n\n![img](/img/machine-learning-notes/pic-18.png)\n\n![img](/img/machine-learning-notes/pic-19.png)\n\n判断高方差或者高偏差决定下一步怎么做\n\n**神经网络中的偏差和方差**\n\n大型的神经网络有很小的偏差，所以只需要关注方差\n\n并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好\n\n但是大型网络计算比较昂贵\n\n```python\nlayer = Dense(unit=25, activation=\"relu\", kernel_regularizer=L2(0.01))\n```\n\n## 开发机器学习系统的迭代\n\n![img](/img/machine-learning-notes/pic-20.png)\n\n## 误差分析\n\n在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法\n\n## 添加数据\n\n由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用**数据增强**，用原有的数据样本创造新的样本\n\n例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等\n\n此外还有**数据合成**，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉\n\nAI = Code(algorithm/model) + Data\n\n## 迁移学习（Transfer Learning）\n\n对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。\n\n![img](/img/machine-learning-notes/pic-21.png)\n\n第一步叫做**监督预训练**(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做**微调**(fine tuning)，更改输出层的权重\n\n这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习\n\n通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样\n\n![img](/img/machine-learning-notes/pic-22.png)\n\n## 机器学习项目的完整周期\n\n![img](/img/machine-learning-notes/pic-23.png)\n\n部署\n\n![img](/img/machine-learning-notes/pic-24.png)\n\nMLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。\n\n## 关注公平、偏见、伦理\n\n## 倾斜数据集的误差指标\n\n某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造**混淆矩阵**，包括**真正例，假正例，真负例，假负例**\n\n常用的计算指标是**精确度(precision)**和**召回率(recall)**\n\n![img](/img/machine-learning-notes/pic-25.png)\n\n精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度\n\n权衡：\n\n当我们只有十分确信时才设置y=1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低\n\n当我们不希望错过实际上的y=1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高\n\n通过设置threshold权衡precision和recall\n\nF1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）\n\n$F1 score = \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} = 2\\frac{PR}{P+R}$\n\n## 决策树\n\n决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。\n\n![img](/img/machine-learning-notes/pic-26.png)\n\n**决策树学习**：\n\n- 如果选择每个节点选择什么特征来分类？\n\n应该最大化纯度，每一边的种类尽可能少\n\n- 什么时候停止分类？\n\n当一个节点100%是一个种类\n\n当分裂节点时会导致树超过最大高度（超参数）\n\n当提高的纯度分数低于一个门槛值\n\n当一个节点的样本数量低于一个门槛值\n\n### 衡量纯度（purity）\n\n熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 = 1 - p_1$\n\n$H(p_1)=-p_1log_2(p_1)-p_0log_2(p_0) = -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$\n\n注意：$0log(0) = 0$\n\n![img](/img/machine-learning-notes/pic-27.png)\n\n### 减小熵：信息增益（Information Gain）\n\n当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征\n\n实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值\n\n为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合\n\n![img](/img/machine-learning-notes/pic-28.png)\n\n更一般地\n\n![img](/img/machine-learning-notes/pic-29.png)\n\np是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本/上一节点样本）\n\n### 总结\n\n在根节点以所有数据样本开始\n\n计算所有特征的信息增益，选择最大的\n\n对选择的特征分裂，创建左右分支\n\n保持分裂直到遇到终止条件：\n\n- 当一个节点100%是一个类\n- 当分裂节点会导致树超过最大高度\n- 信息增益的值小于某个门槛值\n- 节点的样本数量小于某个门槛值\n\n实际上是一个递归的过程\n\n### 独热编码(One Hot Encoding)\n\n实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0/1）\n\n这样又转变为原来的左右分支分裂的情况\n\n### 连续值特征\n\n选定一个阈值，判断数据样本大于或者小于该阈值\n\n分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点\n\n对分割点分别计算信息增强来选择阈值\n\n### 回归树\n\n分裂时，改成尽量选取输出的方差(Variance)小的特征\n\nw还是从上一节点样本中选择的样本数（当前样本/上一节点样本），之后计算加权平均方差\n\n再用上一个节点所有数据的方差减去加权平均方差，选取最大的\n\n分类的结果是样本的平均值\n\n## 使用多个决策树\n\n单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮\n\n### 放回抽样\n\n从n个样本中放回地抽取n次，结果作为一个新的数据集\n\n### 随机森林（Random Forest）\n\n给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树\n\nb个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出\n\n对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k < n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k = \\sqrt{n}$\n\n### XGBoost（eXtreme Gradient Boosting）\n\n极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本\n\n思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分\n\n- 提升树的开源实现\n- 快速，有效\n- 很好的设定结束分裂的标准\n- 内置正则化\n\n### 什么时候使用决策树\n\n一个或多个决策树\n\n- 在表格化和结构化的数据上工作的很好\n- 不建议在非结构化的数据上，例如图片，音频，文本\n- 训练快速\n- 决策树是人类可以理解的（可解释性）\n\n神经网络\n\n- 对于所有类型的数据都能工作的很好\n- 比决策树更慢\n- 可以很好地使用迁移学习（预训练+微调）\n\n- 当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）\n\n# Course 3\n\n除了监督学习，机器学习还包括\n\n- 无监督学习\n  - 聚类\n  - 异常检测\n- 推荐系统\n- 强化学习\n\n## 聚类\n\n一堆数据点中自动查找相互关联或者相似的数据点\n\n### K-means\n\n首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2... \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度\n\n- 将每个点分配给离他最近的中心点（centroid质心）\n- 将中心点移动到分配的点的平均中心\n- 重复前两步，直到中心点不再移动，K-means算法收敛\n\n```bash\nRepeat{\n\tfor i = 1 to m\n\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）\n\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方\n\tfor i = 1 to k\n\t\tu_k更新为分配的点的中心（每个轴的点的平均值）\n\t\t如果簇中心点没有分配到点，就删除\n}\n```\n\n### 损失函数\n\n![img](/img/machine-learning-notes/pic-30.png)\n\n$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）\n\n$u_k$是簇k\n\n$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇\n\n损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是**欧几里得距离**\n\n也叫Distortion Function\n\n### 初始化\n\n选择$K<m$\n\n随机选择K个训练样本，将$\\mu_1 ,\\mu_2... \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点\n\n```bash\nfor i = 1 to 100{\n\t随机初始化\n\t获取c_i, u_i\n\t计算损失函数J\n}\n选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值\n```\n\n### 选择簇的个数\n\n**肘法（Elbow Method）**\n\n选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点\n\n所以K的选择还是按照之后的任务目的选择\n\n## 异常检测\n\n### 密度估计（Density estimation）\n\n根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})<\\epsilon$，认为出现了异常（anomaly）\n\n### 高斯分布\n\nGaussian Distribution，也叫正态分布(Normal Distribution)\n\n$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$\n\n其中$\\mu$是平均值，$\\sigma$是标准差\n\n![img](/img/machine-learning-notes/pic-31.png)\n\n### 算法实现\n\n对于有多个特征的输入$\\vec{x}$，$\\vec{x} = [x_1, x_2 ... x_n]$\n\n$$\np(\\vec{x}) = p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) *...* p(x_n;\\mu_n,\\sigma_n^2) = \\prod_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)\n$$\n\n\n### 开发和评估异常检测系统\n\n通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0/1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集\n\n**流程：**\n\n在训练集$x_1...x_m$上拟合模型$p(x)$\n\n在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）\n\n之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$\n\n### 权衡异常检测和监督学习\n\n异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样\n\n监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似\n\n### 特征选择\n\n监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要\n\n- 绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）\n- 检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取\n\n## 推荐系统\n\n$r(i,j) = 1$表示用户j为电影i打分\n\n$y^{(i,j)}$表示用户j为电影i打的分\n\n$w^{(j)}, b^{(j)}$是用户j的参数\n\n$x^{(i)}$是电影i的特征向量\n\n对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$\n\n$m^{(j)}$表示用户j打分的电影数量\n\n通过训练学习$w^{(j)}, b^{(j)}$\n\n$$\n\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)=\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$\n\n$$\n\\left.\\mathrm{J}\\left(\n\\begin{array}\n{cc}{w^{(1)},} & {...,w^{(n_{u})}} \\\\\n{b^{(1)},} & {...,b^{(n_{u})}}\n\\end{array}\\right.\\right)=\\frac{1}{2}\\sum_{j=1}^{n_{u}}\\sum_{i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j=1}^{n_{u}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n### 协同过滤算法\n\n在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少\n\n$$\n\\mathrm{J}(x^{(i)})=\\frac{1}{2}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n学习得到$x^{(1)},x^{(2)},...,x^{(n_m)}$\n\n$$\n\\mathrm{J}\\left(x^{(1)},x^{(2)},...,x^{(n_{m})}\\right)=\\frac{1}{2}\\sum_{i=1}^{n_{m}}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：\n\n![img](/img/machine-learning-notes/pic-32.png)\n\n梯度下降时，w，b，x都是参数\n\n![img](/img/machine-learning-notes/pic-33.png)\n\n[补充](https://blog.csdn.net/zhu_xian_gang/article/details/130243870)\n\n### 二进制标签\n\n1-用户看到物品之后参与点击，停留，添加喜欢，购买\n\n0-用户看到物品之后忽略\n\n?-用户没有看到物品\n\n预测$y^{(i,j)}=1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数\n\n![img](/img/machine-learning-notes/pic-34.png)\n\n### 均值归一化\n\n**Mean Normalization**\n\n- 求均值$\\mu$\n- $x_1 = \\frac{x_1-\\mu}{max-min}$\n\n求出每个电影的平均用户平方$\\mu_i$，构建向量$u$\n\n对于用户j，预测其在电影i的评分：\n\n$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$\n\n以至于不会当用户没有评分时认为评分接近0，而是接近平均值\n\n### 查找相关项目\n\n对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似\n\n选取小的$\\sum_{l=1}^n(x_l^{(k)} - x_l^{(i)})^2$\n\n也可以写作$||x^{(k)} - x^{(i)}||^2$\n\n### 协同过滤算法的限制\n\n**冷启动问题**\n\n- 如何对没有什么用户打分的项目评分？\n- 如何对没有对很多项目打分的用户推荐一些项目？\n\n**没有很多信息的时候利用辅助信息**\n\n### 基于内容的过滤算法\n\n协同过滤：基于用户的评分与你的评分的相似推荐项目\n\n基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目\n\n但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配\n\n对于v的获取，使用神经网络\n\n可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积\n\n也可以将两个网络合并，在内部进行点积输出结果\n\n$$\nJ=\\sum_{(i,j):r(i,j)=1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}\n$$\n\n\n为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似\n\n### Retrieval and Ranking\n\n通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名\n\n检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看\n\n排名：对这些检索出的有限个项目进行学习，根据结果进行排名\n\n权衡检索的项目数量\n\n## 强化学习\n\n强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。\n\n涉及状态，行动，奖励，折扣系数，回报，策略\n\n### 回报\n\n指的是系统获得的奖励总和\n\n折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+...$，直到终止状态\n\n### 策略\n\n状态state通过策略π实行行动a\n\n$\\pi(s) = a$，指明状态s情况下需要进行的决策a，从而最大化回报\n\n### 马尔科夫决策过程\n\nMarkov Decision Process(MDP)\n\n![img](/img/machine-learning-notes/pic-35.png)\n\n### 状态-动作价值函数\n\nState-action value function，也叫Q-function,Q*,Optimal Q function\n\n$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报\n\n在状态s的最好回报就是$max_aQ(s,a)$\n\n在状态s的最好动作的就能够提供$max_aQ(s,a)$的\n\n### Bellman方程\n\n$s$:当前状态\n\n$a$:当前状态的决策\n\n$R(s)$:当前状态的奖励\n\n$s'$:采取动作a后的状态\n\n$a'$:在状态s'采取的动作\n\n$Q(s,a) = R(s)+\\gamma max_{a'}Q(s',a')$\n\nR(s)也叫即时奖励，表示你可以立刻得到的奖励\n\n后一项是从状态s'表现得最好获得的回报\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+... = R_1 + \\gamma[R_2 + \\gamma R_3+...]$\n\n### 随机环境\n\n由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励\n\n所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望\n\n$\\text{Return} = \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+...) = \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+...)$\n\nBellman Equation变成：\n\n$Q(s,a) = R(s)+\\gamma \\text{E} [max_{a'}Q(s',a')]$\n\n### 连续状态空间\n\n状态参数可能是连续的，比如坐标，角度，速度\n\n同时状态可能有多个，比如xyz坐标，速度等\n\n此时也叫连续状态马尔科夫决策过程\n\n### 学习状态值函数\n\n![img](/img/machine-learning-notes/pic-36.png)\n\n以随机猜测$Q(s,a)$初始化神经网络\n\n重复：\n\n采取措施，得到$(s,a,R(s),s')$元组\n\n存储最近的10k个 $(s,a,R(s),s')$元组（Replay Buffer）\n\n训练网络：\n\n​\t创建10k个训练集，其中$x=(s,a)$，$y = R(s)+\\gamma max_{a'}Q(s',a')$\n\n​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$\n\n令$Q=Q_{new}$\n\n虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计\n\n**改进**\n\n- 神经网络架构\n\n可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行\n\n![img](/img/machine-learning-notes/pic-37.png)\n\n- $\\epsilon$贪心策略\n\n当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。\n\n应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）\n\n小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。\n\n- 小批量$mini-batch$\n\n将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优\n\n![img](/img/machine-learning-notes/pic-38.png)\n\n假设子集大小为1000；\n\n具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。\n\n在强化学习中，可以把10k的数据集分解训练多个模型\n\n- 软更新\n\n令$Q=Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$\n\n而是\n$$\nw = 0.01w_{new} + 0.99w\n$$\n\n$$\nb = 0.01b_{new} + 0.99b\n$$\n\n对参数进行微小调整\n","source":"_posts/machine-learning-notes.md","raw":"---\ntitle: Machine Learning Notes\nmathjax: true\ndate: 2023/11/25 20:46:25\nimg: https://img2.baidu.com/it/u=1093757134,3274186314&fm=253&fmt=auto&app=120&f=JPEG?w=800&h=500\nexcerpt: 吴恩达机器学习视频笔记\n---\n\n# Course 1\n\n监督学习：输入特征x，输出目标y。对数据集进行预测，分为**回归**和**分类**\n\n无监督学习：输入特征x，没有目标y，对数据集进行**聚类预测**，**异常检测**，**降维**\n\n## 线性回归\n\n$$\ny^i = wx^i+b\n$$\n\n定义损失函数（成本函数），需要最小化损失函数\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2\n$$\n\n其中$y^i$为真实输出，$\\hat{y}$为预测输出\n\n- 为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差\n- 1/2是为了方便求导计算\n\nloss针对一个训练样本，cost是所有训练样本的均值\n\n### 梯度下降\n\n需要最小会损失函数，需要使用梯度下降算法\n\n定义学习率`learning_rate`为$\\alpha$,一般$\\alpha \\subseteq [0,1]$\n\n$w = w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$\n\n$b = b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$\n\n- 梯度下降时建议**同步**梯度下降，如下图\n\n![img](/img/machine-learning-notes/pic-1.png)\n\n如果$\\alpha$太小，可以得到答案，但是时间过长\n\n如果$\\alpha$太大，大交叉无法收敛，甚至发散\n\n当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解\n\n对导数项分别求导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = wx^i+b$\n\n对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值\n\n选择合适得到学习率，就可以得到$min(J(w,b))$\n\n线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例\n\n### 多元线性回归\n\n假设特征有$n$个，定义$\\vec{x} = \\begin{bmatrix} x_1 & x_2 & x_3 & ... \\end{bmatrix}$，参数$\\vec{w} = \\begin{bmatrix} w_1 & w_2 & w_3 & ... \\end{bmatrix}$\n\n则$f_{\\vec{w},b}=\\vec{w} \\cdot \\vec{x} +b$\n\n`·`为两个向量的点积(dot)。\n\n$\\vec{w} \\cdot \\vec{x} = w_1*x_1+w_2*x_2+....+w_n*x_n$\n\n**矢量化**：利用计算机的并行硬件，代码简洁、运行速度快\n\n```python\nf = np.dot(w, x) + b\n```\n\n**多元线性回归的梯度下降**\n\n![img](/img/machine-learning-notes/pic-2.png)\n\nPS: 正规方程：某些机器学习库在后端求$w,b$的方法，**只适用于线性回归**，而且速度慢，不要求掌握\n\n### 特征缩放\n\n不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动\n\n加快梯度下降速度\n\n避免特征的取值范围差异过大，将其进行缩放，几种常见方法：\n\n- **除以最大值**，$x_{1,scale} = \\frac{x_1}{max}$， $x \\in [0,1]$\n- **均值归一化Mean Normalization**\n  - 求均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{max-min}$\n- **`Z-score`归一化**\n  - 求标准差$\\sigma$，均值$\\mu$\n  - $x_1 = \\frac{x_1-\\mu}{\\sigma}$\n\n**判断梯度下降是否收敛：**\n\n1. 观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）\n\n**选择合适学习率**：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$\n\n### 特征工程\n\n利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测\n\n**多项式回归**：可以添加$x^q$项更好地拟合数据图像，$f(x)=w_1x^3+w_2x^2+w_1x^1+b$\n\n此时特征缩放尤为重要\n\n## 分类-逻辑回归\n\n解决二分类问题\n\n### sigmoid函数\n\n输出介于$(0,1)$\n\n$g(z)= \\frac{1}{1+e^{-z}},z \\subseteq R$\n\n**logistic regression**:\n\n$f_{\\vec{w},b}(\\vec{x})=g(\\vec{w} · \\vec{x}+b) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n输出值可以理解为分类为1的可能性\n\n$f_{\\vec{w},b}(\\vec{x})=P(y=1|\\vec{x};\\vec{w},b)$\n\n### 决策边界decision boundary\n\n以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b <0$，取值0\n\n$\\vec{w} · \\vec{x}+b = 0$称为决策边界\n\n多项式回归也适用于非线性的决策边界\n\n### 成本函数\n\n如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$**不是凸函数，不适用于逻辑回归**\n\n定义\n\n$$\nJ(w,b)=\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})\n$$\n其中L代表单个样例的loss，J代表总的cost\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}=1\n$$\n![img](/img/machine-learning-notes/pic-3.png)\n\n当y等于1，预测值越靠近1损失越小\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}=0\n$$\n![img](/img/machine-learning-notes/pic-4.png)\n\n当y等于0，预测值越靠近0损失越小 \n\n**简化**成本函数                                                                                                                                                                                                                                                                                          \n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n得到\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))\n$$\n\n\n成本函数是凸函数，便于实现梯度下降\n\n### 梯度下降\n\n对J求偏导\n\n$\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i$\n\n$\\frac{\\partial{J(w,b)}}{\\partial{b}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)$\n\n其中$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n可以使用相似方法进行特征缩放\n\n### 过拟合问题\n\n过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。\n\n![img](/img/machine-learning-notes/pic-5.png)\n\n**解决过拟合**\n\n- 收集更多训练数据\n- 特征筛选，选择特征的一个子集\n- 正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响\n\n### 正则化\n\n如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得\n\n$$\nJ(w,b) = \\frac{1}{2m} \\sum_{i=1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n这样使得$w_j$尽可能小，几乎为0\n\n参数$b$是否正则化无关紧要\n\n**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显\n\n- 正则化线性回归\n\n对$J(w,b)$求偏导不断同步更新w,b的值\n\n$$\n\\frac{\\partial{J(w,b)}}{\\partial{w}} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}\n$$\n\n$$\nw = w- \\alpha (\\frac{1}{m} \\sum_{i=1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j=1}^{m}{w_j}) = (1-\\alpha \\frac{\\lambda}{m})w+.....\n$$\n\n\n- 正则化逻辑回归\n\n$$\nJ(w,b) = -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j=1}^{n} {w_j}^2\n$$\n\n\n\n求导式和线性回归相同，只是需要注意**正则化项偏导数没有求和**\n\n$f(x^i) = \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$\n\n![img](/img/machine-learning-notes/pic-6.png)\n\n# Course 2\n\n## 神经网络\n\n起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为**深度学习**\n\n利用别人训练的神经网络参数称为推理或者预测\n\n为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数\n\n神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层->隐藏层->输出层）\n\n多层神经网络叫做多层感知机\n\n## 神经网络中的层\n\n讨论层数通常是隐藏层和输出层，不包括输入层\n\n每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层/输出结果\n\n即$a_j^{[l]} = g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$\n\n$j$表示神经元单元序号，$l$表示层数，$g(x)$为`sigmod`函数\n\n![img](/img/machine-learning-notes/pic-7.jpg)\n\n$a_j^{[l]}$构成$\\vec{a}^{[l]}$\n\n![img](/img/machine-learning-notes/pic-8.png)\n\n## 前向传播(forward prop)\n\n从输入初步传递到输出，即为前向传播\n\n**一般实现**\n\n```python\ndef dense(a_in, W, b, g):\n\tunits = W.shape[1] # 单元数等于W矩阵的列数，w_j向量是列向量\n\ta_out = np.zeros(units)\n\tfor j in range(units):\n\t\tw = W[:, j]\n\t\tz = np.dot(w, a_in) + b\n\t\ta_out[j] = g(z)\n\treturn a_out\ndef sequential(x):\n    a1 = dense(x, W1, b1)\n    a2 = dense(a1, W2, b2)\n    a3 = dense(a2, W3, b3)\n    f_x = a3\n    return f_x\n```\n\n**使用框架（TensorFlow/Pytorch)）进行矢量化加速**\n\n## 模型训练步骤\n\n1. 指定如何在给定输入X和参数的情况下计算输出(模型**结构**)\n2. 指定**损失函数**\n3. **训练**模型以最小化损失函数\n\n二元交叉熵损失\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n通过反向传播计算偏导数\n\n## 激活函数\n\n用`ReLU`函数代替`sigmoid`激活，$g(z) = max(0,z)$\n\n![img](/img/machine-learning-notes/pic-9.png)\n\n**如何选择合适的激活函数？**\n\n取决于要预测的y，对于神经网络的**输出层**：\n\n- 二分类——> sigmoid\n- y可正可负的回归——> linear\n- 回归中y大于等于0 ——> ReLU\n\n对于神经网络的**隐藏层**建议使用ReLU\n\n`ReLU`常用且更快\n\n- 不涉及指数运算\n- 当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x->-∞）,而sigmoid两端都是\n\n**为什么需要激活函数？**\n\n对于隐藏层，只使用线性的所有层等价于线性回归\n\n对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解\n\n## 多分类问题\n\n**softmax回归算法**（logistic 推广）\n\n$z_1=\\vec{w_1}·\\vec{x_1}+b_1$\n\n$a_1=\\frac{e^{z_1}}{e^{z_1}+...+e^{z_n}} = P(y=1|\\vec{x})$\n\n即，设有N个分类\n\n$z_i=\\vec{w_1}·\\vec{x_i}+b_i$\n\n$$\na_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_i}}=P(y=i|\\vec{x})\n$$\n其中$a_1+a_2+...+a_N=1$\n\n**softmax损失函数**\n\n回顾logistic回归\n\n$$\nL(f_{w,b}(x^{(i)},y^{(i)})=-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))\n$$\n\n\n二分类问题，设$a_1 = f_{w,b}(x^{(i)})$，即$y=1$的概率\n\n则$a_2 = 1-f_{w,b}(x^{(i)})$，即$y=0$的概率\n\n简化为\n\n$loss = -log(a_1)$ 如果$y=1$\n\n$loss = -log(a_2)$ 如果$y=0$\n\n对于softmax回归算法\n\n$$\nloss(a_1,a_2,...,a_N,y) = \\left\\{\\begin{matrix} -log(a_1) \\quad if \\quad y=1\\\\ -log(a_2) \\quad if \\quad y=2 \\\\ ... \\\\ -log(a_N) \\quad if \\quad y=N \\end{matrix}\\right.\n$$\n\n\n**神经网络中的softmax**\n\n输出层变为N个神经元\n\n注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 ... z_n$的函数\n\n**softmax改进**\n\n由于[数值溢出和精度问题](https://blog.csdn.net/muyuu/article/details/122757470)\n\n$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失\n\n不先计算出$a_i$，再带入损失函数\n\n而是**直接**\n$$\nloss_i=-log(\\frac{e^{z_i}}{e_{z_1}+...+e_{z_N}})\n$$\n\n\n此时输出层只需要`linear`即可（就是不计算$a_i$），同时开启`from_logits=True`\n\n```python\nmodel.compile(loss=SparseCategoricalCrossEntropy(from_logits=True)) #稀疏分类交叉熵损失\n```\n\n`from_logits=True`的[作用](https://blog.csdn.net/muyuu/article/details/122762442)\n\n需要概率时再调用`softmax`\n\n```python\nlogits = model(X)\nf_x = tf.nn.softmax(logits)\n```\n\n**多标签分类**\n\n![img](/img/machine-learning-notes/pic-10.png)\n\n将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。\n\n## 高级优化方法\n\n传统的梯度下降学习率固定\n\n### Adam（Adaptive Moment estimation）\n\n如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率\n\n如果看到学习率太大，某个参数值来回振荡，会自动减小学习率\n\n可以自动调整学习率$\\alpha$\n\n对于每个参数都有一个$\\alpha$\n\n选择optimizer=adam即可\n\n## 其他的网络层\n\n### 卷积层（Convolutional Layer）\n\n每个神经元只能看到前一个层输入的一部分\n\n- 加快计算速度\n- 需要更少的数据，不容易过拟合\n\n有多个卷积层，即卷积神经网络\n\n每一层的单元只查看输入的一部分 \n\n## 构建机器学习系统\n\n### 评估一个模型\n\n特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。\n\n将数据集分为训练集和测试集（73或者82开）\n\n分三步计算\n\n![img](/img/machine-learning-notes/pic-11.png)\n\n**注意计算error时不包括正则化项**\n\n过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力\n\n对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）\n\n### 如何选择模型\n\n数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$\n\n交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫**dev set**/validation set\n\n$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力\n\n数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。\n\n### **偏差和方差**\n\n![img](/img/machine-learning-notes/pic-12.png)\n\n![img](/img/machine-learning-notes/pic-13.png)\n\n高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多\n\n高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。\n\n**正则化项参数对偏差和方差的影响：**\n\n![img](/img/machine-learning-notes/pic-14.png)\n\n但是这些数值多少才算大/小呢？需要**建立基准性能标准**，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差/高偏差的问题\n\n![img](/img/machine-learning-notes/pic-15.png)\n\n**学习曲线**\n\n![img](/img/machine-learning-notes/pic-16.png)\n\n高偏差时 \n\n![img](/img/machine-learning-notes/pic-17.png)\n\n高方差时\n\n![img](/img/machine-learning-notes/pic-18.png)\n\n![img](/img/machine-learning-notes/pic-19.png)\n\n判断高方差或者高偏差决定下一步怎么做\n\n**神经网络中的偏差和方差**\n\n大型的神经网络有很小的偏差，所以只需要关注方差\n\n并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好\n\n但是大型网络计算比较昂贵\n\n```python\nlayer = Dense(unit=25, activation=\"relu\", kernel_regularizer=L2(0.01))\n```\n\n## 开发机器学习系统的迭代\n\n![img](/img/machine-learning-notes/pic-20.png)\n\n## 误差分析\n\n在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法\n\n## 添加数据\n\n由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用**数据增强**，用原有的数据样本创造新的样本\n\n例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等\n\n此外还有**数据合成**，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉\n\nAI = Code(algorithm/model) + Data\n\n## 迁移学习（Transfer Learning）\n\n对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。\n\n![img](/img/machine-learning-notes/pic-21.png)\n\n第一步叫做**监督预训练**(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做**微调**(fine tuning)，更改输出层的权重\n\n这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习\n\n通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样\n\n![img](/img/machine-learning-notes/pic-22.png)\n\n## 机器学习项目的完整周期\n\n![img](/img/machine-learning-notes/pic-23.png)\n\n部署\n\n![img](/img/machine-learning-notes/pic-24.png)\n\nMLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。\n\n## 关注公平、偏见、伦理\n\n## 倾斜数据集的误差指标\n\n某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造**混淆矩阵**，包括**真正例，假正例，真负例，假负例**\n\n常用的计算指标是**精确度(precision)**和**召回率(recall)**\n\n![img](/img/machine-learning-notes/pic-25.png)\n\n精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度\n\n权衡：\n\n当我们只有十分确信时才设置y=1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低\n\n当我们不希望错过实际上的y=1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高\n\n通过设置threshold权衡precision和recall\n\nF1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）\n\n$F1 score = \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} = 2\\frac{PR}{P+R}$\n\n## 决策树\n\n决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。\n\n![img](/img/machine-learning-notes/pic-26.png)\n\n**决策树学习**：\n\n- 如果选择每个节点选择什么特征来分类？\n\n应该最大化纯度，每一边的种类尽可能少\n\n- 什么时候停止分类？\n\n当一个节点100%是一个种类\n\n当分裂节点时会导致树超过最大高度（超参数）\n\n当提高的纯度分数低于一个门槛值\n\n当一个节点的样本数量低于一个门槛值\n\n### 衡量纯度（purity）\n\n熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 = 1 - p_1$\n\n$H(p_1)=-p_1log_2(p_1)-p_0log_2(p_0) = -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$\n\n注意：$0log(0) = 0$\n\n![img](/img/machine-learning-notes/pic-27.png)\n\n### 减小熵：信息增益（Information Gain）\n\n当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征\n\n实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值\n\n为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合\n\n![img](/img/machine-learning-notes/pic-28.png)\n\n更一般地\n\n![img](/img/machine-learning-notes/pic-29.png)\n\np是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本/上一节点样本）\n\n### 总结\n\n在根节点以所有数据样本开始\n\n计算所有特征的信息增益，选择最大的\n\n对选择的特征分裂，创建左右分支\n\n保持分裂直到遇到终止条件：\n\n- 当一个节点100%是一个类\n- 当分裂节点会导致树超过最大高度\n- 信息增益的值小于某个门槛值\n- 节点的样本数量小于某个门槛值\n\n实际上是一个递归的过程\n\n### 独热编码(One Hot Encoding)\n\n实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0/1）\n\n这样又转变为原来的左右分支分裂的情况\n\n### 连续值特征\n\n选定一个阈值，判断数据样本大于或者小于该阈值\n\n分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点\n\n对分割点分别计算信息增强来选择阈值\n\n### 回归树\n\n分裂时，改成尽量选取输出的方差(Variance)小的特征\n\nw还是从上一节点样本中选择的样本数（当前样本/上一节点样本），之后计算加权平均方差\n\n再用上一个节点所有数据的方差减去加权平均方差，选取最大的\n\n分类的结果是样本的平均值\n\n## 使用多个决策树\n\n单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮\n\n### 放回抽样\n\n从n个样本中放回地抽取n次，结果作为一个新的数据集\n\n### 随机森林（Random Forest）\n\n给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树\n\nb个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出\n\n对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k < n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k = \\sqrt{n}$\n\n### XGBoost（eXtreme Gradient Boosting）\n\n极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本\n\n思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分\n\n- 提升树的开源实现\n- 快速，有效\n- 很好的设定结束分裂的标准\n- 内置正则化\n\n### 什么时候使用决策树\n\n一个或多个决策树\n\n- 在表格化和结构化的数据上工作的很好\n- 不建议在非结构化的数据上，例如图片，音频，文本\n- 训练快速\n- 决策树是人类可以理解的（可解释性）\n\n神经网络\n\n- 对于所有类型的数据都能工作的很好\n- 比决策树更慢\n- 可以很好地使用迁移学习（预训练+微调）\n\n- 当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）\n\n# Course 3\n\n除了监督学习，机器学习还包括\n\n- 无监督学习\n  - 聚类\n  - 异常检测\n- 推荐系统\n- 强化学习\n\n## 聚类\n\n一堆数据点中自动查找相互关联或者相似的数据点\n\n### K-means\n\n首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2... \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度\n\n- 将每个点分配给离他最近的中心点（centroid质心）\n- 将中心点移动到分配的点的平均中心\n- 重复前两步，直到中心点不再移动，K-means算法收敛\n\n```bash\nRepeat{\n\tfor i = 1 to m\n\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）\n\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方\n\tfor i = 1 to k\n\t\tu_k更新为分配的点的中心（每个轴的点的平均值）\n\t\t如果簇中心点没有分配到点，就删除\n}\n```\n\n### 损失函数\n\n![img](/img/machine-learning-notes/pic-30.png)\n\n$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）\n\n$u_k$是簇k\n\n$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇\n\n损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是**欧几里得距离**\n\n也叫Distortion Function\n\n### 初始化\n\n选择$K<m$\n\n随机选择K个训练样本，将$\\mu_1 ,\\mu_2... \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点\n\n```bash\nfor i = 1 to 100{\n\t随机初始化\n\t获取c_i, u_i\n\t计算损失函数J\n}\n选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值\n```\n\n### 选择簇的个数\n\n**肘法（Elbow Method）**\n\n选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点\n\n所以K的选择还是按照之后的任务目的选择\n\n## 异常检测\n\n### 密度估计（Density estimation）\n\n根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})<\\epsilon$，认为出现了异常（anomaly）\n\n### 高斯分布\n\nGaussian Distribution，也叫正态分布(Normal Distribution)\n\n$p(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$\n\n其中$\\mu$是平均值，$\\sigma$是标准差\n\n![img](/img/machine-learning-notes/pic-31.png)\n\n### 算法实现\n\n对于有多个特征的输入$\\vec{x}$，$\\vec{x} = [x_1, x_2 ... x_n]$\n\n$$\np(\\vec{x}) = p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) *...* p(x_n;\\mu_n,\\sigma_n^2) = \\prod_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)\n$$\n\n\n### 开发和评估异常检测系统\n\n通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0/1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集\n\n**流程：**\n\n在训练集$x_1...x_m$上拟合模型$p(x)$\n\n在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）\n\n之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$\n\n### 权衡异常检测和监督学习\n\n异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样\n\n监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似\n\n### 特征选择\n\n监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要\n\n- 绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）\n- 检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取\n\n## 推荐系统\n\n$r(i,j) = 1$表示用户j为电影i打分\n\n$y^{(i,j)}$表示用户j为电影i打的分\n\n$w^{(j)}, b^{(j)}$是用户j的参数\n\n$x^{(i)}$是电影i的特征向量\n\n对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$\n\n$m^{(j)}$表示用户j打分的电影数量\n\n通过训练学习$w^{(j)}, b^{(j)}$\n\n$$\n\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)=\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},...,w^{(n_u)},b^{(n_u)}$\n\n$$\n\\left.\\mathrm{J}\\left(\n\\begin{array}\n{cc}{w^{(1)},} & {...,w^{(n_{u})}} \\\\\n{b^{(1)},} & {...,b^{(n_{u})}}\n\\end{array}\\right.\\right)=\\frac{1}{2}\\sum_{j=1}^{n_{u}}\\sum_{i:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j=1}^{n_{u}}\\sum_{k=1}^{n}\\left(w_{k}^{(j)}\\right)^{2}\n$$\n\n\n### 协同过滤算法\n\n在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少\n\n$$\n\\mathrm{J}(x^{(i)})=\\frac{1}{2}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n学习得到$x^{(1)},x^{(2)},...,x^{(n_m)}$\n\n$$\n\\mathrm{J}\\left(x^{(1)},x^{(2)},...,x^{(n_{m})}\\right)=\\frac{1}{2}\\sum_{i=1}^{n_{m}}\\sum_{j:r(i,j)=1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i=1}^{n_{m}}\\sum_{k=1}^{n}\\left(x_{k}^{(i)}\\right)^{2}\n$$\n\n\n将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：\n\n![img](/img/machine-learning-notes/pic-32.png)\n\n梯度下降时，w，b，x都是参数\n\n![img](/img/machine-learning-notes/pic-33.png)\n\n[补充](https://blog.csdn.net/zhu_xian_gang/article/details/130243870)\n\n### 二进制标签\n\n1-用户看到物品之后参与点击，停留，添加喜欢，购买\n\n0-用户看到物品之后忽略\n\n?-用户没有看到物品\n\n预测$y^{(i,j)}=1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数\n\n![img](/img/machine-learning-notes/pic-34.png)\n\n### 均值归一化\n\n**Mean Normalization**\n\n- 求均值$\\mu$\n- $x_1 = \\frac{x_1-\\mu}{max-min}$\n\n求出每个电影的平均用户平方$\\mu_i$，构建向量$u$\n\n对于用户j，预测其在电影i的评分：\n\n$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$\n\n以至于不会当用户没有评分时认为评分接近0，而是接近平均值\n\n### 查找相关项目\n\n对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似\n\n选取小的$\\sum_{l=1}^n(x_l^{(k)} - x_l^{(i)})^2$\n\n也可以写作$||x^{(k)} - x^{(i)}||^2$\n\n### 协同过滤算法的限制\n\n**冷启动问题**\n\n- 如何对没有什么用户打分的项目评分？\n- 如何对没有对很多项目打分的用户推荐一些项目？\n\n**没有很多信息的时候利用辅助信息**\n\n### 基于内容的过滤算法\n\n协同过滤：基于用户的评分与你的评分的相似推荐项目\n\n基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目\n\n但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配\n\n对于v的获取，使用神经网络\n\n可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积\n\n也可以将两个网络合并，在内部进行点积输出结果\n\n$$\nJ=\\sum_{(i,j):r(i,j)=1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}\n$$\n\n\n为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似\n\n### Retrieval and Ranking\n\n通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名\n\n检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看\n\n排名：对这些检索出的有限个项目进行学习，根据结果进行排名\n\n权衡检索的项目数量\n\n## 强化学习\n\n强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。\n\n涉及状态，行动，奖励，折扣系数，回报，策略\n\n### 回报\n\n指的是系统获得的奖励总和\n\n折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+...$，直到终止状态\n\n### 策略\n\n状态state通过策略π实行行动a\n\n$\\pi(s) = a$，指明状态s情况下需要进行的决策a，从而最大化回报\n\n### 马尔科夫决策过程\n\nMarkov Decision Process(MDP)\n\n![img](/img/machine-learning-notes/pic-35.png)\n\n### 状态-动作价值函数\n\nState-action value function，也叫Q-function,Q*,Optimal Q function\n\n$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报\n\n在状态s的最好回报就是$max_aQ(s,a)$\n\n在状态s的最好动作的就能够提供$max_aQ(s,a)$的\n\n### Bellman方程\n\n$s$:当前状态\n\n$a$:当前状态的决策\n\n$R(s)$:当前状态的奖励\n\n$s'$:采取动作a后的状态\n\n$a'$:在状态s'采取的动作\n\n$Q(s,a) = R(s)+\\gamma max_{a'}Q(s',a')$\n\nR(s)也叫即时奖励，表示你可以立刻得到的奖励\n\n后一项是从状态s'表现得最好获得的回报\n\n$\\text{Return} = R_1 + \\gamma R_2 + \\gamma^2R_3+... = R_1 + \\gamma[R_2 + \\gamma R_3+...]$\n\n### 随机环境\n\n由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励\n\n所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望\n\n$\\text{Return} = \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+...) = \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+...)$\n\nBellman Equation变成：\n\n$Q(s,a) = R(s)+\\gamma \\text{E} [max_{a'}Q(s',a')]$\n\n### 连续状态空间\n\n状态参数可能是连续的，比如坐标，角度，速度\n\n同时状态可能有多个，比如xyz坐标，速度等\n\n此时也叫连续状态马尔科夫决策过程\n\n### 学习状态值函数\n\n![img](/img/machine-learning-notes/pic-36.png)\n\n以随机猜测$Q(s,a)$初始化神经网络\n\n重复：\n\n采取措施，得到$(s,a,R(s),s')$元组\n\n存储最近的10k个 $(s,a,R(s),s')$元组（Replay Buffer）\n\n训练网络：\n\n​\t创建10k个训练集，其中$x=(s,a)$，$y = R(s)+\\gamma max_{a'}Q(s',a')$\n\n​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$\n\n令$Q=Q_{new}$\n\n虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计\n\n**改进**\n\n- 神经网络架构\n\n可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行\n\n![img](/img/machine-learning-notes/pic-37.png)\n\n- $\\epsilon$贪心策略\n\n当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。\n\n应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）\n\n小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。\n\n- 小批量$mini-batch$\n\n将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优\n\n![img](/img/machine-learning-notes/pic-38.png)\n\n假设子集大小为1000；\n\n具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。\n\n在强化学习中，可以把10k的数据集分解训练多个模型\n\n- 软更新\n\n令$Q=Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$\n\n而是\n$$\nw = 0.01w_{new} + 0.99w\n$$\n\n$$\nb = 0.01b_{new} + 0.99b\n$$\n\n对参数进行微小调整\n","slug":"machine-learning-notes","published":1,"updated":"2025-02-27T16:18:57.885Z","comments":1,"layout":"post","photos":[],"_id":"cm7o4qpo200072o99h99p4kid","content":"<h1 id=\"Course-1\"><a href=\"#Course-1\" class=\"headerlink\" title=\"Course 1\"></a>Course 1</h1><p>监督学习：输入特征x，输出目标y。对数据集进行预测，分为<strong>回归</strong>和<strong>分类</strong></p>\n<p>无监督学习：输入特征x，没有目标y，对数据集进行<strong>聚类预测</strong>，<strong>异常检测</strong>，<strong>降维</strong></p>\n<h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><p>$$<br>y^i &#x3D; wx^i+b<br>$$</p>\n<p>定义损失函数（成本函数），需要最小化损失函数</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2<br>$$</p>\n<p>其中$y^i$为真实输出，$\\hat{y}$为预测输出</p>\n<ul>\n<li>为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差</li>\n<li>1&#x2F;2是为了方便求导计算</li>\n</ul>\n<p>loss针对一个训练样本，cost是所有训练样本的均值</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>需要最小会损失函数，需要使用梯度下降算法</p>\n<p>定义学习率<code>learning_rate</code>为$\\alpha$,一般$\\alpha \\subseteq [0,1]$</p>\n<p>$w &#x3D; w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$</p>\n<p>$b &#x3D; b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$</p>\n<ul>\n<li>梯度下降时建议<strong>同步</strong>梯度下降，如下图</li>\n</ul>\n<p><img src=\"/img/machine-learning-notes/pic-1.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-1.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>如果$\\alpha$太小，可以得到答案，但是时间过长</p>\n<p>如果$\\alpha$太大，大交叉无法收敛，甚至发散</p>\n<p>当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解</p>\n<p>对导数项分别求导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; wx^i+b$</p>\n<p>对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值</p>\n<p>选择合适得到学习率，就可以得到$min(J(w,b))$</p>\n<p>线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例</p>\n<h3 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h3><p>假设特征有$n$个，定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>则$f_{\\vec{w},b}&#x3D;\\vec{w} \\cdot \\vec{x} +b$</p>\n<p><code>·</code>为两个向量的点积(dot)。</p>\n<p>$\\vec{w} \\cdot \\vec{x} &#x3D; w_1<em>x_1+w_2</em>x_2+….+w_n*x_n$</p>\n<p><strong>矢量化</strong>：利用计算机的并行硬件，代码简洁、运行速度快</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>\n\n<p><strong>多元线性回归的梯度下降</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-2.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-2.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>PS: 正规方程：某些机器学习库在后端求$w,b$的方法，<strong>只适用于线性回归</strong>，而且速度慢，不要求掌握</p>\n<h3 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h3><p>不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动</p>\n<p>加快梯度下降速度</p>\n<p>避免特征的取值范围差异过大，将其进行缩放，几种常见方法：</p>\n<ul>\n<li><strong>除以最大值</strong>，$x_{1,scale} &#x3D; \\frac{x_1}{max}$， $x \\in [0,1]$</li>\n<li><strong>均值归一化Mean Normalization</strong><ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n</li>\n<li><strong><code>Z-score</code>归一化</strong><ul>\n<li>求标准差$\\sigma$，均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{\\sigma}$</li>\n</ul>\n</li>\n</ul>\n<p><strong>判断梯度下降是否收敛：</strong></p>\n<ol>\n<li>观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）</li>\n</ol>\n<p><strong>选择合适学习率</strong>：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$</p>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测</p>\n<p><strong>多项式回归</strong>：可以添加$x^q$项更好地拟合数据图像，$f(x)&#x3D;w_1x^3+w_2x^2+w_1x^1+b$</p>\n<p>此时特征缩放尤为重要</p>\n<h2 id=\"分类-逻辑回归\"><a href=\"#分类-逻辑回归\" class=\"headerlink\" title=\"分类-逻辑回归\"></a>分类-逻辑回归</h2><p>解决二分类问题</p>\n<h3 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h3><p>输出介于$(0,1)$</p>\n<p>$g(z)&#x3D; \\frac{1}{1+e^{-z}},z \\subseteq R$</p>\n<p><strong>logistic regression</strong>:</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;g(\\vec{w} · \\vec{x}+b) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>输出值可以理解为分类为1的可能性</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;P(y&#x3D;1|\\vec{x};\\vec{w},b)$</p>\n<h3 id=\"决策边界decision-boundary\"><a href=\"#决策边界decision-boundary\" class=\"headerlink\" title=\"决策边界decision boundary\"></a>决策边界decision boundary</h3><p>以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b &lt;0$，取值0</p>\n<p>$\\vec{w} · \\vec{x}+b &#x3D; 0$称为决策边界</p>\n<p>多项式回归也适用于非线性的决策边界</p>\n<h3 id=\"成本函数\"><a href=\"#成本函数\" class=\"headerlink\" title=\"成本函数\"></a>成本函数</h3><p>如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$<strong>不是凸函数，不适用于逻辑回归</strong></p>\n<p>定义</p>\n<p>$$<br>J(w,b)&#x3D;\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})<br>$$<br>其中L代表单个样例的loss，J代表总的cost</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}&#x3D;1<br>$$<br><img src=\"/img/machine-learning-notes/pic-3.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-3.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>当y等于1，预测值越靠近1损失越小</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}&#x3D;0<br>$$<br><img src=\"/img/machine-learning-notes/pic-4.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-4.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>当y等于0，预测值越靠近0损失越小 </p>\n<p><strong>简化</strong>成本函数                                                                                                                                                                                                                                                                                          </p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>得到</p>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))<br>$$</p>\n<p>成本函数是凸函数，便于实现梯度下降</p>\n<h3 id=\"梯度下降-1\"><a href=\"#梯度下降-1\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>对J求偏导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>可以使用相似方法进行特征缩放</p>\n<h3 id=\"过拟合问题\"><a href=\"#过拟合问题\" class=\"headerlink\" title=\"过拟合问题\"></a>过拟合问题</h3><p>过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。</p>\n<p><img src=\"/img/machine-learning-notes/pic-5.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-5.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>解决过拟合</strong></p>\n<ul>\n<li>收集更多训练数据</li>\n<li>特征筛选，选择特征的一个子集</li>\n<li>正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响</li>\n</ul>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><p>如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>这样使得$w_j$尽可能小，几乎为0</p>\n<p>参数$b$是否正则化无关紧要</p>\n<p>**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显</p>\n<ul>\n<li>正则化线性回归</li>\n</ul>\n<p>对$J(w,b)$求偏导不断同步更新w,b的值</p>\n<p>$$<br>\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}<br>$$</p>\n<p>$$<br>w &#x3D; w- \\alpha (\\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}) &#x3D; (1-\\alpha \\frac{\\lambda}{m})w+…..<br>$$</p>\n<ul>\n<li>正则化逻辑回归</li>\n</ul>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>求导式和线性回归相同，只是需要注意<strong>正则化项偏导数没有求和</strong></p>\n<p>$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-6.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-6.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h1 id=\"Course-2\"><a href=\"#Course-2\" class=\"headerlink\" title=\"Course 2\"></a>Course 2</h1><h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><p>起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为<strong>深度学习</strong></p>\n<p>利用别人训练的神经网络参数称为推理或者预测</p>\n<p>为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数</p>\n<p>神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层-&gt;隐藏层-&gt;输出层）</p>\n<p>多层神经网络叫做多层感知机</p>\n<h2 id=\"神经网络中的层\"><a href=\"#神经网络中的层\" class=\"headerlink\" title=\"神经网络中的层\"></a>神经网络中的层</h2><p>讨论层数通常是隐藏层和输出层，不包括输入层</p>\n<p>每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层&#x2F;输出结果</p>\n<p>即$a_j^{[l]} &#x3D; g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$</p>\n<p>$j$表示神经元单元序号，$l$表示层数，$g(x)$为<code>sigmod</code>函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-7.jpg\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-7.jpg\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>$a_j^{[l]}$构成$\\vec{a}^{[l]}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-8.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-8.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"前向传播-forward-prop\"><a href=\"#前向传播-forward-prop\" class=\"headerlink\" title=\"前向传播(forward prop)\"></a>前向传播(forward prop)</h2><p>从输入初步传递到输出，即为前向传播</p>\n<p><strong>一般实现</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dense</span>(<span class=\"params\">a_in, W, b, g</span>):</span><br><span class=\"line\">\tunits = W.shape[<span class=\"number\">1</span>] <span class=\"comment\"># 单元数等于W矩阵的列数，w_j向量是列向量</span></span><br><span class=\"line\">\ta_out = np.zeros(units)</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(units):</span><br><span class=\"line\">\t\tw = W[:, j]</span><br><span class=\"line\">\t\tz = np.dot(w, a_in) + b</span><br><span class=\"line\">\t\ta_out[j] = g(z)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a_out</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sequential</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    a1 = dense(x, W1, b1)</span><br><span class=\"line\">    a2 = dense(a1, W2, b2)</span><br><span class=\"line\">    a3 = dense(a2, W3, b3)</span><br><span class=\"line\">    f_x = a3</span><br><span class=\"line\">    <span class=\"keyword\">return</span> f_x</span><br></pre></td></tr></table></figure>\n\n<p><strong>使用框架（TensorFlow&#x2F;Pytorch)）进行矢量化加速</strong></p>\n<h2 id=\"模型训练步骤\"><a href=\"#模型训练步骤\" class=\"headerlink\" title=\"模型训练步骤\"></a>模型训练步骤</h2><ol>\n<li>指定如何在给定输入X和参数的情况下计算输出(模型<strong>结构</strong>)</li>\n<li>指定<strong>损失函数</strong></li>\n<li><strong>训练</strong>模型以最小化损失函数</li>\n</ol>\n<p>二元交叉熵损失</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$<br>通过反向传播计算偏导数</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>用<code>ReLU</code>函数代替<code>sigmoid</code>激活，$g(z) &#x3D; max(0,z)$</p>\n<p><img src=\"/img/machine-learning-notes/pic-9.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-9.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>如何选择合适的激活函数？</strong></p>\n<p>取决于要预测的y，对于神经网络的<strong>输出层</strong>：</p>\n<ul>\n<li>二分类——&gt; sigmoid</li>\n<li>y可正可负的回归——&gt; linear</li>\n<li>回归中y大于等于0 ——&gt; ReLU</li>\n</ul>\n<p>对于神经网络的<strong>隐藏层</strong>建议使用ReLU</p>\n<p><code>ReLU</code>常用且更快</p>\n<ul>\n<li>不涉及指数运算</li>\n<li>当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x-&gt;-∞）,而sigmoid两端都是</li>\n</ul>\n<p><strong>为什么需要激活函数？</strong></p>\n<p>对于隐藏层，只使用线性的所有层等价于线性回归</p>\n<p>对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解</p>\n<h2 id=\"多分类问题\"><a href=\"#多分类问题\" class=\"headerlink\" title=\"多分类问题\"></a>多分类问题</h2><p><strong>softmax回归算法</strong>（logistic 推广）</p>\n<p>$z_1&#x3D;\\vec{w_1}·\\vec{x_1}+b_1$</p>\n<p>$a_1&#x3D;\\frac{e^{z_1}}{e^{z_1}+…+e^{z_n}} &#x3D; P(y&#x3D;1|\\vec{x})$</p>\n<p>即，设有N个分类</p>\n<p>$z_i&#x3D;\\vec{w_1}·\\vec{x_i}+b_i$</p>\n<p>$$<br>a_i &#x3D; \\frac{e^{z_i}}{\\sum_{k&#x3D;1}^{N} e^{z_i}}&#x3D;P(y&#x3D;i|\\vec{x})<br>$$<br>其中$a_1+a_2+…+a_N&#x3D;1$</p>\n<p><strong>softmax损失函数</strong></p>\n<p>回顾logistic回归</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>二分类问题，设$a_1 &#x3D; f_{w,b}(x^{(i)})$，即$y&#x3D;1$的概率</p>\n<p>则$a_2 &#x3D; 1-f_{w,b}(x^{(i)})$，即$y&#x3D;0$的概率</p>\n<p>简化为</p>\n<p>$loss &#x3D; -log(a_1)$ 如果$y&#x3D;1$</p>\n<p>$loss &#x3D; -log(a_2)$ 如果$y&#x3D;0$</p>\n<p>对于softmax回归算法</p>\n<p>$$<br>loss(a_1,a_2,…,a_N,y) &#x3D; \\left{\\begin{matrix} -log(a_1) \\quad if \\quad y&#x3D;1\\ -log(a_2) \\quad if \\quad y&#x3D;2 \\ … \\ -log(a_N) \\quad if \\quad y&#x3D;N \\end{matrix}\\right.<br>$$</p>\n<p><strong>神经网络中的softmax</strong></p>\n<p>输出层变为N个神经元</p>\n<p>注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 … z_n$的函数</p>\n<p><strong>softmax改进</strong></p>\n<p>由于<a href=\"https://blog.csdn.net/muyuu/article/details/122757470\">数值溢出和精度问题</a></p>\n<p>$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失</p>\n<p>不先计算出$a_i$，再带入损失函数</p>\n<p>而是<strong>直接</strong><br>$$<br>loss_i&#x3D;-log(\\frac{e^{z_i}}{e_{z_1}+…+e_{z_N}})<br>$$</p>\n<p>此时输出层只需要<code>linear</code>即可（就是不计算$a_i$），同时开启<code>from_logits=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.<span class=\"built_in\">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class=\"literal\">True</span>)) <span class=\"comment\">#稀疏分类交叉熵损失</span></span><br></pre></td></tr></table></figure>\n\n<p><code>from_logits=True</code>的<a href=\"https://blog.csdn.net/muyuu/article/details/122762442\">作用</a></p>\n<p>需要概率时再调用<code>softmax</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logits = model(X)</span><br><span class=\"line\">f_x = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>\n\n<p><strong>多标签分类</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-10.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-10.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。</p>\n<h2 id=\"高级优化方法\"><a href=\"#高级优化方法\" class=\"headerlink\" title=\"高级优化方法\"></a>高级优化方法</h2><p>传统的梯度下降学习率固定</p>\n<h3 id=\"Adam（Adaptive-Moment-estimation）\"><a href=\"#Adam（Adaptive-Moment-estimation）\" class=\"headerlink\" title=\"Adam（Adaptive Moment estimation）\"></a>Adam（Adaptive Moment estimation）</h3><p>如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率</p>\n<p>如果看到学习率太大，某个参数值来回振荡，会自动减小学习率</p>\n<p>可以自动调整学习率$\\alpha$</p>\n<p>对于每个参数都有一个$\\alpha$</p>\n<p>选择optimizer&#x3D;adam即可</p>\n<h2 id=\"其他的网络层\"><a href=\"#其他的网络层\" class=\"headerlink\" title=\"其他的网络层\"></a>其他的网络层</h2><h3 id=\"卷积层（Convolutional-Layer）\"><a href=\"#卷积层（Convolutional-Layer）\" class=\"headerlink\" title=\"卷积层（Convolutional Layer）\"></a>卷积层（Convolutional Layer）</h3><p>每个神经元只能看到前一个层输入的一部分</p>\n<ul>\n<li>加快计算速度</li>\n<li>需要更少的数据，不容易过拟合</li>\n</ul>\n<p>有多个卷积层，即卷积神经网络</p>\n<p>每一层的单元只查看输入的一部分 </p>\n<h2 id=\"构建机器学习系统\"><a href=\"#构建机器学习系统\" class=\"headerlink\" title=\"构建机器学习系统\"></a>构建机器学习系统</h2><h3 id=\"评估一个模型\"><a href=\"#评估一个模型\" class=\"headerlink\" title=\"评估一个模型\"></a>评估一个模型</h3><p>特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。</p>\n<p>将数据集分为训练集和测试集（73或者82开）</p>\n<p>分三步计算</p>\n<p><img src=\"/img/machine-learning-notes/pic-11.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-11.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>注意计算error时不包括正则化项</strong></p>\n<p>过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力</p>\n<p>对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）</p>\n<h3 id=\"如何选择模型\"><a href=\"#如何选择模型\" class=\"headerlink\" title=\"如何选择模型\"></a>如何选择模型</h3><p>数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$</p>\n<p>交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫<strong>dev set</strong>&#x2F;validation set</p>\n<p>$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力</p>\n<p>数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。</p>\n<h3 id=\"偏差和方差\"><a href=\"#偏差和方差\" class=\"headerlink\" title=\"偏差和方差\"></a><strong>偏差和方差</strong></h3><p><img src=\"/img/machine-learning-notes/pic-12.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-12.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-13.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-13.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多</p>\n<p>高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。</p>\n<p><strong>正则化项参数对偏差和方差的影响：</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-14.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-14.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>但是这些数值多少才算大&#x2F;小呢？需要<strong>建立基准性能标准</strong>，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差&#x2F;高偏差的问题</p>\n<p><img src=\"/img/machine-learning-notes/pic-15.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-15.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>学习曲线</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-16.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-16.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高偏差时 </p>\n<p><img src=\"/img/machine-learning-notes/pic-17.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-17.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>高方差时</p>\n<p><img src=\"/img/machine-learning-notes/pic-18.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-18.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-19.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-19.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>判断高方差或者高偏差决定下一步怎么做</p>\n<p><strong>神经网络中的偏差和方差</strong></p>\n<p>大型的神经网络有很小的偏差，所以只需要关注方差</p>\n<p>并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好</p>\n<p>但是大型网络计算比较昂贵</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Dense(unit=<span class=\"number\">25</span>, activation=<span class=\"string\">&quot;relu&quot;</span>, kernel_regularizer=L2(<span class=\"number\">0.01</span>))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"开发机器学习系统的迭代\"><a href=\"#开发机器学习系统的迭代\" class=\"headerlink\" title=\"开发机器学习系统的迭代\"></a>开发机器学习系统的迭代</h2><p><img src=\"/img/machine-learning-notes/pic-20.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-20.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h2><p>在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法</p>\n<h2 id=\"添加数据\"><a href=\"#添加数据\" class=\"headerlink\" title=\"添加数据\"></a>添加数据</h2><p>由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用<strong>数据增强</strong>，用原有的数据样本创造新的样本</p>\n<p>例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等</p>\n<p>此外还有<strong>数据合成</strong>，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉</p>\n<p>AI &#x3D; Code(algorithm&#x2F;model) + Data</p>\n<h2 id=\"迁移学习（Transfer-Learning）\"><a href=\"#迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"迁移学习（Transfer Learning）\"></a>迁移学习（Transfer Learning）</h2><p>对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。</p>\n<p><img src=\"/img/machine-learning-notes/pic-21.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-21.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>第一步叫做<strong>监督预训练</strong>(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做<strong>微调</strong>(fine tuning)，更改输出层的权重</p>\n<p>这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习</p>\n<p>通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样</p>\n<p><img src=\"/img/machine-learning-notes/pic-22.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-22.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h2 id=\"机器学习项目的完整周期\"><a href=\"#机器学习项目的完整周期\" class=\"headerlink\" title=\"机器学习项目的完整周期\"></a>机器学习项目的完整周期</h2><p><img src=\"/img/machine-learning-notes/pic-23.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-23.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>部署</p>\n<p><img src=\"/img/machine-learning-notes/pic-24.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-24.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>MLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。</p>\n<h2 id=\"关注公平、偏见、伦理\"><a href=\"#关注公平、偏见、伦理\" class=\"headerlink\" title=\"关注公平、偏见、伦理\"></a>关注公平、偏见、伦理</h2><h2 id=\"倾斜数据集的误差指标\"><a href=\"#倾斜数据集的误差指标\" class=\"headerlink\" title=\"倾斜数据集的误差指标\"></a>倾斜数据集的误差指标</h2><p>某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造<strong>混淆矩阵</strong>，包括<strong>真正例，假正例，真负例，假负例</strong></p>\n<p>常用的计算指标是<strong>精确度(precision)<strong>和</strong>召回率(recall)</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-25.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-25.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度</p>\n<p>权衡：</p>\n<p>当我们只有十分确信时才设置y&#x3D;1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低</p>\n<p>当我们不希望错过实际上的y&#x3D;1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高</p>\n<p>通过设置threshold权衡precision和recall</p>\n<p>F1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）</p>\n<p>$F1 score &#x3D; \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} &#x3D; 2\\frac{PR}{P+R}$</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>\n<p><img src=\"/img/machine-learning-notes/pic-26.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-26.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><strong>决策树学习</strong>：</p>\n<ul>\n<li>如果选择每个节点选择什么特征来分类？</li>\n</ul>\n<p>应该最大化纯度，每一边的种类尽可能少</p>\n<ul>\n<li>什么时候停止分类？</li>\n</ul>\n<p>当一个节点100%是一个种类</p>\n<p>当分裂节点时会导致树超过最大高度（超参数）</p>\n<p>当提高的纯度分数低于一个门槛值</p>\n<p>当一个节点的样本数量低于一个门槛值</p>\n<h3 id=\"衡量纯度（purity）\"><a href=\"#衡量纯度（purity）\" class=\"headerlink\" title=\"衡量纯度（purity）\"></a>衡量纯度（purity）</h3><p>熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 &#x3D; 1 - p_1$</p>\n<p>$H(p_1)&#x3D;-p_1log_2(p_1)-p_0log_2(p_0) &#x3D; -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$</p>\n<p>注意：$0log(0) &#x3D; 0$</p>\n<p><img src=\"/img/machine-learning-notes/pic-27.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-27.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"减小熵：信息增益（Information-Gain）\"><a href=\"#减小熵：信息增益（Information-Gain）\" class=\"headerlink\" title=\"减小熵：信息增益（Information Gain）\"></a>减小熵：信息增益（Information Gain）</h3><p>当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征</p>\n<p>实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值</p>\n<p>为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合</p>\n<p><img src=\"/img/machine-learning-notes/pic-28.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-28.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>更一般地</p>\n<p><img src=\"/img/machine-learning-notes/pic-29.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-29.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>p是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本）</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在根节点以所有数据样本开始</p>\n<p>计算所有特征的信息增益，选择最大的</p>\n<p>对选择的特征分裂，创建左右分支</p>\n<p>保持分裂直到遇到终止条件：</p>\n<ul>\n<li>当一个节点100%是一个类</li>\n<li>当分裂节点会导致树超过最大高度</li>\n<li>信息增益的值小于某个门槛值</li>\n<li>节点的样本数量小于某个门槛值</li>\n</ul>\n<p>实际上是一个递归的过程</p>\n<h3 id=\"独热编码-One-Hot-Encoding\"><a href=\"#独热编码-One-Hot-Encoding\" class=\"headerlink\" title=\"独热编码(One Hot Encoding)\"></a>独热编码(One Hot Encoding)</h3><p>实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0&#x2F;1）</p>\n<p>这样又转变为原来的左右分支分裂的情况</p>\n<h3 id=\"连续值特征\"><a href=\"#连续值特征\" class=\"headerlink\" title=\"连续值特征\"></a>连续值特征</h3><p>选定一个阈值，判断数据样本大于或者小于该阈值</p>\n<p>分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点</p>\n<p>对分割点分别计算信息增强来选择阈值</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>分裂时，改成尽量选取输出的方差(Variance)小的特征</p>\n<p>w还是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本），之后计算加权平均方差</p>\n<p>再用上一个节点所有数据的方差减去加权平均方差，选取最大的</p>\n<p>分类的结果是样本的平均值</p>\n<h2 id=\"使用多个决策树\"><a href=\"#使用多个决策树\" class=\"headerlink\" title=\"使用多个决策树\"></a>使用多个决策树</h2><p>单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮</p>\n<h3 id=\"放回抽样\"><a href=\"#放回抽样\" class=\"headerlink\" title=\"放回抽样\"></a>放回抽样</h3><p>从n个样本中放回地抽取n次，结果作为一个新的数据集</p>\n<h3 id=\"随机森林（Random-Forest）\"><a href=\"#随机森林（Random-Forest）\" class=\"headerlink\" title=\"随机森林（Random Forest）\"></a>随机森林（Random Forest）</h3><p>给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树</p>\n<p>b个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出</p>\n<p>对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k &lt; n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k &#x3D; \\sqrt{n}$</p>\n<h3 id=\"XGBoost（eXtreme-Gradient-Boosting）\"><a href=\"#XGBoost（eXtreme-Gradient-Boosting）\" class=\"headerlink\" title=\"XGBoost（eXtreme Gradient Boosting）\"></a>XGBoost（eXtreme Gradient Boosting）</h3><p>极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本</p>\n<p>思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分</p>\n<ul>\n<li>提升树的开源实现</li>\n<li>快速，有效</li>\n<li>很好的设定结束分裂的标准</li>\n<li>内置正则化</li>\n</ul>\n<h3 id=\"什么时候使用决策树\"><a href=\"#什么时候使用决策树\" class=\"headerlink\" title=\"什么时候使用决策树\"></a>什么时候使用决策树</h3><p>一个或多个决策树</p>\n<ul>\n<li>在表格化和结构化的数据上工作的很好</li>\n<li>不建议在非结构化的数据上，例如图片，音频，文本</li>\n<li>训练快速</li>\n<li>决策树是人类可以理解的（可解释性）</li>\n</ul>\n<p>神经网络</p>\n<ul>\n<li><p>对于所有类型的数据都能工作的很好</p>\n</li>\n<li><p>比决策树更慢</p>\n</li>\n<li><p>可以很好地使用迁移学习（预训练+微调）</p>\n</li>\n<li><p>当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）</p>\n</li>\n</ul>\n<h1 id=\"Course-3\"><a href=\"#Course-3\" class=\"headerlink\" title=\"Course 3\"></a>Course 3</h1><p>除了监督学习，机器学习还包括</p>\n<ul>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>异常检测</li>\n</ul>\n</li>\n<li>推荐系统</li>\n<li>强化学习</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><p>一堆数据点中自动查找相互关联或者相似的数据点</p>\n<h3 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means\"></a>K-means</h3><p>首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2… \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度</p>\n<ul>\n<li>将每个点分配给离他最近的中心点（centroid质心）</li>\n<li>将中心点移动到分配的点的平均中心</li>\n<li>重复前两步，直到中心点不再移动，K-means算法收敛</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Repeat&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to m</span><br><span class=\"line\">\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）</span><br><span class=\"line\">\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to k</span><br><span class=\"line\">\t\tu_k更新为分配的点的中心（每个轴的点的平均值）</span><br><span class=\"line\">\t\t如果簇中心点没有分配到点，就删除</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p><img src=\"/img/machine-learning-notes/pic-30.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-30.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）</p>\n<p>$u_k$是簇k</p>\n<p>$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇</p>\n<p>损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是<strong>欧几里得距离</strong></p>\n<p>也叫Distortion Function</p>\n<h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>选择$K&lt;m$</p>\n<p>随机选择K个训练样本，将$\\mu_1 ,\\mu_2… \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i = 1 to 100&#123;</span><br><span class=\"line\">\t随机初始化</span><br><span class=\"line\">\t获取c_i, u_i</span><br><span class=\"line\">\t计算损失函数J</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"选择簇的个数\"><a href=\"#选择簇的个数\" class=\"headerlink\" title=\"选择簇的个数\"></a>选择簇的个数</h3><p><strong>肘法（Elbow Method）</strong></p>\n<p>选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点</p>\n<p>所以K的选择还是按照之后的任务目的选择</p>\n<h2 id=\"异常检测\"><a href=\"#异常检测\" class=\"headerlink\" title=\"异常检测\"></a>异常检测</h2><h3 id=\"密度估计（Density-estimation）\"><a href=\"#密度估计（Density-estimation）\" class=\"headerlink\" title=\"密度估计（Density estimation）\"></a>密度估计（Density estimation）</h3><p>根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})&lt;\\epsilon$，认为出现了异常（anomaly）</p>\n<h3 id=\"高斯分布\"><a href=\"#高斯分布\" class=\"headerlink\" title=\"高斯分布\"></a>高斯分布</h3><p>Gaussian Distribution，也叫正态分布(Normal Distribution)</p>\n<p>$p(x)&#x3D;\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$</p>\n<p>其中$\\mu$是平均值，$\\sigma$是标准差</p>\n<p><img src=\"/img/machine-learning-notes/pic-31.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-31.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>对于有多个特征的输入$\\vec{x}$，$\\vec{x} &#x3D; [x_1, x_2 … x_n]$</p>\n<p>$$<br>p(\\vec{x}) &#x3D; p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) <em>…</em> p(x_n;\\mu_n,\\sigma_n^2) &#x3D; \\prod_{j&#x3D;1}^np(x_j;\\mu_j,\\sigma_j^2)<br>$$</p>\n<h3 id=\"开发和评估异常检测系统\"><a href=\"#开发和评估异常检测系统\" class=\"headerlink\" title=\"开发和评估异常检测系统\"></a>开发和评估异常检测系统</h3><p>通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0&#x2F;1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集</p>\n<p><strong>流程：</strong></p>\n<p>在训练集$x_1…x_m$上拟合模型$p(x)$</p>\n<p>在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）</p>\n<p>之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$</p>\n<h3 id=\"权衡异常检测和监督学习\"><a href=\"#权衡异常检测和监督学习\" class=\"headerlink\" title=\"权衡异常检测和监督学习\"></a>权衡异常检测和监督学习</h3><p>异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样</p>\n<p>监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><p>监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要</p>\n<ul>\n<li>绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）</li>\n<li>检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取</li>\n</ul>\n<h2 id=\"推荐系统\"><a href=\"#推荐系统\" class=\"headerlink\" title=\"推荐系统\"></a>推荐系统</h2><p>$r(i,j) &#x3D; 1$表示用户j为电影i打分</p>\n<p>$y^{(i,j)}$表示用户j为电影i打的分</p>\n<p>$w^{(j)}, b^{(j)}$是用户j的参数</p>\n<p>$x^{(i)}$是电影i的特征向量</p>\n<p>对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$</p>\n<p>$m^{(j)}$表示用户j打分的电影数量</p>\n<p>通过训练学习$w^{(j)}, b^{(j)}$</p>\n<p>$$<br>\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)&#x3D;\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<p>对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},…,w^{(n_u)},b^{(n_u)}$</p>\n<p>$$<br>\\left.\\mathrm{J}\\left(<br>\\begin{array}<br>{cc}{w^{(1)},} &amp; {…,w^{(n_{u})}} \\<br>{b^{(1)},} &amp; {…,b^{(n_{u})}}<br>\\end{array}\\right.\\right)&#x3D;\\frac{1}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<h3 id=\"协同过滤算法\"><a href=\"#协同过滤算法\" class=\"headerlink\" title=\"协同过滤算法\"></a>协同过滤算法</h3><p>在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少</p>\n<p>$$<br>\\mathrm{J}(x^{(i)})&#x3D;\\frac{1}{2}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>学习得到$x^{(1)},x^{(2)},…,x^{(n_m)}$</p>\n<p>$$<br>\\mathrm{J}\\left(x^{(1)},x^{(2)},…,x^{(n_{m})}\\right)&#x3D;\\frac{1}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：</p>\n<p><img src=\"/img/machine-learning-notes/pic-32.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-32.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>梯度下降时，w，b，x都是参数</p>\n<p><img src=\"/img/machine-learning-notes/pic-33.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-33.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p><a href=\"https://blog.csdn.net/zhu_xian_gang/article/details/130243870\">补充</a></p>\n<h3 id=\"二进制标签\"><a href=\"#二进制标签\" class=\"headerlink\" title=\"二进制标签\"></a>二进制标签</h3><p>1-用户看到物品之后参与点击，停留，添加喜欢，购买</p>\n<p>0-用户看到物品之后忽略</p>\n<p>?-用户没有看到物品</p>\n<p>预测$y^{(i,j)}&#x3D;1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-34.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-34.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"均值归一化\"><a href=\"#均值归一化\" class=\"headerlink\" title=\"均值归一化\"></a>均值归一化</h3><p><strong>Mean Normalization</strong></p>\n<ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n<p>求出每个电影的平均用户平方$\\mu_i$，构建向量$u$</p>\n<p>对于用户j，预测其在电影i的评分：</p>\n<p>$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$</p>\n<p>以至于不会当用户没有评分时认为评分接近0，而是接近平均值</p>\n<h3 id=\"查找相关项目\"><a href=\"#查找相关项目\" class=\"headerlink\" title=\"查找相关项目\"></a>查找相关项目</h3><p>对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似</p>\n<p>选取小的$\\sum_{l&#x3D;1}^n(x_l^{(k)} - x_l^{(i)})^2$</p>\n<p>也可以写作$||x^{(k)} - x^{(i)}||^2$</p>\n<h3 id=\"协同过滤算法的限制\"><a href=\"#协同过滤算法的限制\" class=\"headerlink\" title=\"协同过滤算法的限制\"></a>协同过滤算法的限制</h3><p><strong>冷启动问题</strong></p>\n<ul>\n<li>如何对没有什么用户打分的项目评分？</li>\n<li>如何对没有对很多项目打分的用户推荐一些项目？</li>\n</ul>\n<p><strong>没有很多信息的时候利用辅助信息</strong></p>\n<h3 id=\"基于内容的过滤算法\"><a href=\"#基于内容的过滤算法\" class=\"headerlink\" title=\"基于内容的过滤算法\"></a>基于内容的过滤算法</h3><p>协同过滤：基于用户的评分与你的评分的相似推荐项目</p>\n<p>基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目</p>\n<p>但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配</p>\n<p>对于v的获取，使用神经网络</p>\n<p>可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积</p>\n<p>也可以将两个网络合并，在内部进行点积输出结果</p>\n<p>$$<br>J&#x3D;\\sum_{(i,j):r(i,j)&#x3D;1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}<br>$$</p>\n<p>为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似</p>\n<h3 id=\"Retrieval-and-Ranking\"><a href=\"#Retrieval-and-Ranking\" class=\"headerlink\" title=\"Retrieval and Ranking\"></a>Retrieval and Ranking</h3><p>通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名</p>\n<p>检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看</p>\n<p>排名：对这些检索出的有限个项目进行学习，根据结果进行排名</p>\n<p>权衡检索的项目数量</p>\n<h2 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h2><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。</p>\n<p>涉及状态，行动，奖励，折扣系数，回报，策略</p>\n<h3 id=\"回报\"><a href=\"#回报\" class=\"headerlink\" title=\"回报\"></a>回报</h3><p>指的是系统获得的奖励总和</p>\n<p>折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+…$，直到终止状态</p>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>状态state通过策略π实行行动a</p>\n<p>$\\pi(s) &#x3D; a$，指明状态s情况下需要进行的决策a，从而最大化回报</p>\n<h3 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h3><p>Markov Decision Process(MDP)</p>\n<p><img src=\"/img/machine-learning-notes/pic-35.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-35.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<h3 id=\"状态-动作价值函数\"><a href=\"#状态-动作价值函数\" class=\"headerlink\" title=\"状态-动作价值函数\"></a>状态-动作价值函数</h3><p>State-action value function，也叫Q-function,Q*,Optimal Q function</p>\n<p>$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报</p>\n<p>在状态s的最好回报就是$max_aQ(s,a)$</p>\n<p>在状态s的最好动作的就能够提供$max_aQ(s,a)$的</p>\n<h3 id=\"Bellman方程\"><a href=\"#Bellman方程\" class=\"headerlink\" title=\"Bellman方程\"></a>Bellman方程</h3><p>$s$:当前状态</p>\n<p>$a$:当前状态的决策</p>\n<p>$R(s)$:当前状态的奖励</p>\n<p>$s’$:采取动作a后的状态</p>\n<p>$a’$:在状态s’采取的动作</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>R(s)也叫即时奖励，表示你可以立刻得到的奖励</p>\n<p>后一项是从状态s’表现得最好获得的回报</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+… &#x3D; R_1 + \\gamma[R_2 + \\gamma R_3+…]$</p>\n<h3 id=\"随机环境\"><a href=\"#随机环境\" class=\"headerlink\" title=\"随机环境\"></a>随机环境</h3><p>由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励</p>\n<p>所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望</p>\n<p>$\\text{Return} &#x3D; \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+…) &#x3D; \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+…)$</p>\n<p>Bellman Equation变成：</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma \\text{E} [max_{a’}Q(s’,a’)]$</p>\n<h3 id=\"连续状态空间\"><a href=\"#连续状态空间\" class=\"headerlink\" title=\"连续状态空间\"></a>连续状态空间</h3><p>状态参数可能是连续的，比如坐标，角度，速度</p>\n<p>同时状态可能有多个，比如xyz坐标，速度等</p>\n<p>此时也叫连续状态马尔科夫决策过程</p>\n<h3 id=\"学习状态值函数\"><a href=\"#学习状态值函数\" class=\"headerlink\" title=\"学习状态值函数\"></a>学习状态值函数</h3><p><img src=\"/img/machine-learning-notes/pic-36.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-36.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>以随机猜测$Q(s,a)$初始化神经网络</p>\n<p>重复：</p>\n<p>采取措施，得到$(s,a,R(s),s’)$元组</p>\n<p>存储最近的10k个 $(s,a,R(s),s’)$元组（Replay Buffer）</p>\n<p>训练网络：</p>\n<p>​\t创建10k个训练集，其中$x&#x3D;(s,a)$，$y &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$</p>\n<p>令$Q&#x3D;Q_{new}$</p>\n<p>虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计</p>\n<p><strong>改进</strong></p>\n<ul>\n<li>神经网络架构</li>\n</ul>\n<p>可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行</p>\n<p><img src=\"/img/machine-learning-notes/pic-37.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-37.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<ul>\n<li>$\\epsilon$贪心策略</li>\n</ul>\n<p>当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。</p>\n<p>应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）</p>\n<p>小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。</p>\n<ul>\n<li>小批量$mini-batch$</li>\n</ul>\n<p>将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优</p>\n<p><img src=\"/img/machine-learning-notes/pic-38.png\" class=\"lazyload placeholder\" data-srcset=\"/img/machine-learning-notes/pic-38.png\" srcset=\"https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp\" alt=\"img\"></p>\n<p>假设子集大小为1000；</p>\n<p>具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。</p>\n<p>在强化学习中，可以把10k的数据集分解训练多个模型</p>\n<ul>\n<li>软更新</li>\n</ul>\n<p>令$Q&#x3D;Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$</p>\n<p>而是<br>$$<br>w &#x3D; 0.01w_{new} + 0.99w<br>$$</p>\n<p>$$<br>b &#x3D; 0.01b_{new} + 0.99b<br>$$</p>\n<p>对参数进行微小调整</p>\n","more":"<h1 id=\"Course-1\"><a href=\"#Course-1\" class=\"headerlink\" title=\"Course 1\"></a>Course 1</h1><p>监督学习：输入特征x，输出目标y。对数据集进行预测，分为<strong>回归</strong>和<strong>分类</strong></p>\n<p>无监督学习：输入特征x，没有目标y，对数据集进行<strong>聚类预测</strong>，<strong>异常检测</strong>，<strong>降维</strong></p>\n<h2 id=\"线性回归\"><a href=\"#线性回归\" class=\"headerlink\" title=\"线性回归\"></a>线性回归</h2><p>$$<br>y^i &#x3D; wx^i+b<br>$$</p>\n<p>定义损失函数（成本函数），需要最小化损失函数</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2<br>$$</p>\n<p>其中$y^i$为真实输出，$\\hat{y}$为预测输出</p>\n<ul>\n<li>为了不让数据集变大而损失也变大，故采用平均平方误差而不是总平方误差</li>\n<li>1&#x2F;2是为了方便求导计算</li>\n</ul>\n<p>loss针对一个训练样本，cost是所有训练样本的均值</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>需要最小会损失函数，需要使用梯度下降算法</p>\n<p>定义学习率<code>learning_rate</code>为$\\alpha$,一般$\\alpha \\subseteq [0,1]$</p>\n<p>$w &#x3D; w- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{w}}$</p>\n<p>$b &#x3D; b- \\alpha \\frac{\\partial{J(w,b)}}{\\partial{b}}$</p>\n<ul>\n<li>梯度下降时建议<strong>同步</strong>梯度下降，如下图</li>\n</ul>\n<p><img src=\"/img/machine-learning-notes/pic-1.png\" alt=\"img\"></p>\n<p>如果$\\alpha$太小，可以得到答案，但是时间过长</p>\n<p>如果$\\alpha$太大，大交叉无法收敛，甚至发散</p>\n<p>当参数值每次更新时，$J(w,b)$变小，导数项（斜率）也会变小，对于固定学习率$\\alpha$，步长也会变小，从而达到局部最优解</p>\n<p>对导数项分别求导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; wx^i+b$</p>\n<p>对于线性回归损失，他的损失函数图像是一个凸函数，只有一个全局最小值，没有局部最小值</p>\n<p>选择合适得到学习率，就可以得到$min(J(w,b))$</p>\n<p>线性回归的梯度下降也是batch gradient descent，批次梯度下降每次更新关心整批的训练样例</p>\n<h3 id=\"多元线性回归\"><a href=\"#多元线性回归\" class=\"headerlink\" title=\"多元线性回归\"></a>多元线性回归</h3><p>假设特征有$n$个，定义$\\vec{x} &#x3D; \\begin{bmatrix} x_1 &amp; x_2 &amp; x_3 &amp; … \\end{bmatrix}$，参数$\\vec{w} &#x3D; \\begin{bmatrix} w_1 &amp; w_2 &amp; w_3 &amp; … \\end{bmatrix}$</p>\n<p>则$f_{\\vec{w},b}&#x3D;\\vec{w} \\cdot \\vec{x} +b$</p>\n<p><code>·</code>为两个向量的点积(dot)。</p>\n<p>$\\vec{w} \\cdot \\vec{x} &#x3D; w_1<em>x_1+w_2</em>x_2+….+w_n*x_n$</p>\n<p><strong>矢量化</strong>：利用计算机的并行硬件，代码简洁、运行速度快</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">f = np.dot(w, x) + b</span><br></pre></td></tr></table></figure>\n\n<p><strong>多元线性回归的梯度下降</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-2.png\" alt=\"img\"></p>\n<p>PS: 正规方程：某些机器学习库在后端求$w,b$的方法，<strong>只适用于线性回归</strong>，而且速度慢，不要求掌握</p>\n<h3 id=\"特征缩放\"><a href=\"#特征缩放\" class=\"headerlink\" title=\"特征缩放\"></a>特征缩放</h3><p>不同特征的估计值范围差异很大，梯度下降等高线图可能某些轴范围宽某些窄，梯度下降过程中可能波 动</p>\n<p>加快梯度下降速度</p>\n<p>避免特征的取值范围差异过大，将其进行缩放，几种常见方法：</p>\n<ul>\n<li><strong>除以最大值</strong>，$x_{1,scale} &#x3D; \\frac{x_1}{max}$， $x \\in [0,1]$</li>\n<li><strong>均值归一化Mean Normalization</strong><ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n</li>\n<li><strong><code>Z-score</code>归一化</strong><ul>\n<li>求标准差$\\sigma$，均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{\\sigma}$</li>\n</ul>\n</li>\n</ul>\n<p><strong>判断梯度下降是否收敛：</strong></p>\n<ol>\n<li>观察iteration-loss曲线是否平稳 2. 自动收敛测试，当loss小于一个很小的值时停止（难用）</li>\n</ol>\n<p><strong>选择合适学习率</strong>：从0.001开始，每次乘以3，对比$J(w,b)$与迭代次数的关系，选择合适的$\\alpha$</p>\n<h3 id=\"特征工程\"><a href=\"#特征工程\" class=\"headerlink\" title=\"特征工程\"></a>特征工程</h3><p>利用知识和直觉设计新特征，通常通过转化与组合，使模型做出更准确的预测</p>\n<p><strong>多项式回归</strong>：可以添加$x^q$项更好地拟合数据图像，$f(x)&#x3D;w_1x^3+w_2x^2+w_1x^1+b$</p>\n<p>此时特征缩放尤为重要</p>\n<h2 id=\"分类-逻辑回归\"><a href=\"#分类-逻辑回归\" class=\"headerlink\" title=\"分类-逻辑回归\"></a>分类-逻辑回归</h2><p>解决二分类问题</p>\n<h3 id=\"sigmoid函数\"><a href=\"#sigmoid函数\" class=\"headerlink\" title=\"sigmoid函数\"></a>sigmoid函数</h3><p>输出介于$(0,1)$</p>\n<p>$g(z)&#x3D; \\frac{1}{1+e^{-z}},z \\subseteq R$</p>\n<p><strong>logistic regression</strong>:</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;g(\\vec{w} · \\vec{x}+b) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>输出值可以理解为分类为1的可能性</p>\n<p>$f_{\\vec{w},b}(\\vec{x})&#x3D;P(y&#x3D;1|\\vec{x};\\vec{w},b)$</p>\n<h3 id=\"决策边界decision-boundary\"><a href=\"#决策边界decision-boundary\" class=\"headerlink\" title=\"决策边界decision boundary\"></a>决策边界decision boundary</h3><p>以0.5作为阈值，当$\\vec{w} · \\vec{x}+b \\ge 0$，取值1；当$\\vec{w} · \\vec{x}+b &lt;0$，取值0</p>\n<p>$\\vec{w} · \\vec{x}+b &#x3D; 0$称为决策边界</p>\n<p>多项式回归也适用于非线性的决策边界</p>\n<h3 id=\"成本函数\"><a href=\"#成本函数\" class=\"headerlink\" title=\"成本函数\"></a>成本函数</h3><p>如果使用平方误差成本函数，有多个局部最小值，$J(w,b)$<strong>不是凸函数，不适用于逻辑回归</strong></p>\n<p>定义</p>\n<p>$$<br>J(w,b)&#x3D;\\frac{1}{m}\\sum_{i-1}^{m}L(f_{w,b}(x^{(i)},y^{(i)})<br>$$<br>其中L代表单个样例的loss，J代表总的cost</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(f_{w,b}(x^{(i)})) \\quad if\\quad y^{(i)}&#x3D;1<br>$$<br><img src=\"/img/machine-learning-notes/pic-3.png\" alt=\"img\"></p>\n<p>当y等于1，预测值越靠近1损失越小</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-log(1-f_{w,b}(x^{(i)})) \\quad if \\quad y^{(i)}&#x3D;0<br>$$<br><img src=\"/img/machine-learning-notes/pic-4.png\" alt=\"img\"></p>\n<p>当y等于0，预测值越靠近0损失越小 </p>\n<p><strong>简化</strong>成本函数                                                                                                                                                                                                                                                                                          </p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>得到</p>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))<br>$$</p>\n<p>成本函数是凸函数，便于实现梯度下降</p>\n<h3 id=\"梯度下降-1\"><a href=\"#梯度下降-1\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>对J求偏导</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i$</p>\n<p>$\\frac{\\partial{J(w,b)}}{\\partial{b}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)$</p>\n<p>其中$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p>可以使用相似方法进行特征缩放</p>\n<h3 id=\"过拟合问题\"><a href=\"#过拟合问题\" class=\"headerlink\" title=\"过拟合问题\"></a>过拟合问题</h3><p>过拟合虽然可能完美通过训练集，但是有高方差，泛化能力差。应该避免欠拟合（高偏差high bias）和过拟合（高方差high variance）。</p>\n<p><img src=\"/img/machine-learning-notes/pic-5.png\" alt=\"img\"></p>\n<p><strong>解决过拟合</strong></p>\n<ul>\n<li>收集更多训练数据</li>\n<li>特征筛选，选择特征的一个子集</li>\n<li>正则化(Regularization)：在维持多项式回归的基础上，减小参数$w_j$的值，减小一些特征的影响</li>\n</ul>\n<h3 id=\"正则化\"><a href=\"#正则化\" class=\"headerlink\" title=\"正则化\"></a>正则化</h3><p>如果不知道哪个特征是重要的，一般惩罚所有特征，防止过拟合</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{\\alpha m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>其中$\\lambda$为正则化参数，$\\alpha$为学习率，缩放得</p>\n<p>$$<br>J(w,b) &#x3D; \\frac{1}{2m} \\sum_{i&#x3D;1}^{m} {(y^i-\\hat{y})}^2 + \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>这样使得$w_j$尽可能小，几乎为0</p>\n<p>参数$b$是否正则化无关紧要</p>\n<p>**需要选择合适的$\\lambda$**，太大趋于直线，太小惩罚效果不明显</p>\n<ul>\n<li>正则化线性回归</li>\n</ul>\n<p>对$J(w,b)$求偏导不断同步更新w,b的值</p>\n<p>$$<br>\\frac{\\partial{J(w,b)}}{\\partial{w}} &#x3D; \\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}<br>$$</p>\n<p>$$<br>w &#x3D; w- \\alpha (\\frac{1}{m} \\sum_{i&#x3D;1}^{m} (f(x^i)-y^i)x^i+\\frac{\\lambda}{m}\\sum_{j&#x3D;1}^{m}{w_j}) &#x3D; (1-\\alpha \\frac{\\lambda}{m})w+…..<br>$$</p>\n<ul>\n<li>正则化逻辑回归</li>\n</ul>\n<p>$$<br>J(w,b) &#x3D; -\\frac{1}{m} (y^{(i)} log(f_{w,b}(x^{(i)})) + (1-y^{(i)})log(1-f_{w,b}(x^{(i)})))+ \\frac{\\lambda}{2 m}\\sum_{j&#x3D;1}^{n} {w_j}^2<br>$$</p>\n<p>求导式和线性回归相同，只是需要注意<strong>正则化项偏导数没有求和</strong></p>\n<p>$f(x^i) &#x3D; \\frac{1}{1+e^{-(\\vec{w} · \\vec{x}+b)}}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-6.png\" alt=\"img\"></p>\n<h1 id=\"Course-2\"><a href=\"#Course-2\" class=\"headerlink\" title=\"Course 2\"></a>Course 2</h1><h2 id=\"神经网络\"><a href=\"#神经网络\" class=\"headerlink\" title=\"神经网络\"></a>神经网络</h2><p>起源于设计算法来模拟人脑活动（但无需过于重视深度学习的生物动机），21世纪定义为<strong>深度学习</strong></p>\n<p>利用别人训练的神经网络参数称为推理或者预测</p>\n<p>为了简化表达使用全连接，一层可以使用上一层的所有特征，对于不重要的特征可以选择适当的参数</p>\n<p>神经网络不需要手动设计它可以学习的功能，在隐藏层自动提取特征（输入层-&gt;隐藏层-&gt;输出层）</p>\n<p>多层神经网络叫做多层感知机</p>\n<h2 id=\"神经网络中的层\"><a href=\"#神经网络中的层\" class=\"headerlink\" title=\"神经网络中的层\"></a>神经网络中的层</h2><p>讨论层数通常是隐藏层和输出层，不包括输入层</p>\n<p>每一层输入向量$\\vec{x}$或$\\vec{a}_{i-1}$，经过当前层中多个神经元的逻辑回归处理，输出新的向量$\\vec{a}^{[l]}$，进入到下一层&#x2F;输出结果</p>\n<p>即$a_j^{[l]} &#x3D; g(\\vec{w}_j^{[l]} \\cdot \\vec{a}^{[l-1]} + b_j^{[l]})$</p>\n<p>$j$表示神经元单元序号，$l$表示层数，$g(x)$为<code>sigmod</code>函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-7.jpg\" alt=\"img\"></p>\n<p>$a_j^{[l]}$构成$\\vec{a}^{[l]}$</p>\n<p><img src=\"/img/machine-learning-notes/pic-8.png\" alt=\"img\"></p>\n<h2 id=\"前向传播-forward-prop\"><a href=\"#前向传播-forward-prop\" class=\"headerlink\" title=\"前向传播(forward prop)\"></a>前向传播(forward prop)</h2><p>从输入初步传递到输出，即为前向传播</p>\n<p><strong>一般实现</strong></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">dense</span>(<span class=\"params\">a_in, W, b, g</span>):</span><br><span class=\"line\">\tunits = W.shape[<span class=\"number\">1</span>] <span class=\"comment\"># 单元数等于W矩阵的列数，w_j向量是列向量</span></span><br><span class=\"line\">\ta_out = np.zeros(units)</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(units):</span><br><span class=\"line\">\t\tw = W[:, j]</span><br><span class=\"line\">\t\tz = np.dot(w, a_in) + b</span><br><span class=\"line\">\t\ta_out[j] = g(z)</span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a_out</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sequential</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    a1 = dense(x, W1, b1)</span><br><span class=\"line\">    a2 = dense(a1, W2, b2)</span><br><span class=\"line\">    a3 = dense(a2, W3, b3)</span><br><span class=\"line\">    f_x = a3</span><br><span class=\"line\">    <span class=\"keyword\">return</span> f_x</span><br></pre></td></tr></table></figure>\n\n<p><strong>使用框架（TensorFlow&#x2F;Pytorch)）进行矢量化加速</strong></p>\n<h2 id=\"模型训练步骤\"><a href=\"#模型训练步骤\" class=\"headerlink\" title=\"模型训练步骤\"></a>模型训练步骤</h2><ol>\n<li>指定如何在给定输入X和参数的情况下计算输出(模型<strong>结构</strong>)</li>\n<li>指定<strong>损失函数</strong></li>\n<li><strong>训练</strong>模型以最小化损失函数</li>\n</ol>\n<p>二元交叉熵损失</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$<br>通过反向传播计算偏导数</p>\n<h2 id=\"激活函数\"><a href=\"#激活函数\" class=\"headerlink\" title=\"激活函数\"></a>激活函数</h2><p>用<code>ReLU</code>函数代替<code>sigmoid</code>激活，$g(z) &#x3D; max(0,z)$</p>\n<p><img src=\"/img/machine-learning-notes/pic-9.png\" alt=\"img\"></p>\n<p><strong>如何选择合适的激活函数？</strong></p>\n<p>取决于要预测的y，对于神经网络的<strong>输出层</strong>：</p>\n<ul>\n<li>二分类——&gt; sigmoid</li>\n<li>y可正可负的回归——&gt; linear</li>\n<li>回归中y大于等于0 ——&gt; ReLU</li>\n</ul>\n<p>对于神经网络的<strong>隐藏层</strong>建议使用ReLU</p>\n<p><code>ReLU</code>常用且更快</p>\n<ul>\n<li>不涉及指数运算</li>\n<li>当一个函数在许多地方都是平的，梯度下降会很慢，ReLU只有一端（x-&gt;-∞）,而sigmoid两端都是</li>\n</ul>\n<p><strong>为什么需要激活函数？</strong></p>\n<p>对于隐藏层，只使用线性的所有层等价于线性回归</p>\n<p>对于输出层，得到的结果显然可以仅仅使用线性回归（输出层用线性）或者逻辑回归（输出层用sigmoid）求解</p>\n<h2 id=\"多分类问题\"><a href=\"#多分类问题\" class=\"headerlink\" title=\"多分类问题\"></a>多分类问题</h2><p><strong>softmax回归算法</strong>（logistic 推广）</p>\n<p>$z_1&#x3D;\\vec{w_1}·\\vec{x_1}+b_1$</p>\n<p>$a_1&#x3D;\\frac{e^{z_1}}{e^{z_1}+…+e^{z_n}} &#x3D; P(y&#x3D;1|\\vec{x})$</p>\n<p>即，设有N个分类</p>\n<p>$z_i&#x3D;\\vec{w_1}·\\vec{x_i}+b_i$</p>\n<p>$$<br>a_i &#x3D; \\frac{e^{z_i}}{\\sum_{k&#x3D;1}^{N} e^{z_i}}&#x3D;P(y&#x3D;i|\\vec{x})<br>$$<br>其中$a_1+a_2+…+a_N&#x3D;1$</p>\n<p><strong>softmax损失函数</strong></p>\n<p>回顾logistic回归</p>\n<p>$$<br>L(f_{w,b}(x^{(i)},y^{(i)})&#x3D;-y^{(i)} log(f_{w,b}(x^{(i)})) - (1-y^{(i)})log(1-f_{w,b}(x^{(i)}))<br>$$</p>\n<p>二分类问题，设$a_1 &#x3D; f_{w,b}(x^{(i)})$，即$y&#x3D;1$的概率</p>\n<p>则$a_2 &#x3D; 1-f_{w,b}(x^{(i)})$，即$y&#x3D;0$的概率</p>\n<p>简化为</p>\n<p>$loss &#x3D; -log(a_1)$ 如果$y&#x3D;1$</p>\n<p>$loss &#x3D; -log(a_2)$ 如果$y&#x3D;0$</p>\n<p>对于softmax回归算法</p>\n<p>$$<br>loss(a_1,a_2,…,a_N,y) &#x3D; \\left{\\begin{matrix} -log(a_1) \\quad if \\quad y&#x3D;1\\ -log(a_2) \\quad if \\quad y&#x3D;2 \\ … \\ -log(a_N) \\quad if \\quad y&#x3D;N \\end{matrix}\\right.<br>$$</p>\n<p><strong>神经网络中的softmax</strong></p>\n<p>输出层变为N个神经元</p>\n<p>注意：之前的激活函数$g(z_1)$只是$z_1$的函数，但是softmax是$z_1 … z_n$的函数</p>\n<p><strong>softmax改进</strong></p>\n<p>由于<a href=\"https://blog.csdn.net/muyuu/article/details/122757470\">数值溢出和精度问题</a></p>\n<p>$log$函数当x趋于0变化一些都会影响很大，所以尽量不舍入$a_i$，得到精确得到损失</p>\n<p>不先计算出$a_i$，再带入损失函数</p>\n<p>而是<strong>直接</strong><br>$$<br>loss_i&#x3D;-log(\\frac{e^{z_i}}{e_{z_1}+…+e_{z_N}})<br>$$</p>\n<p>此时输出层只需要<code>linear</code>即可（就是不计算$a_i$），同时开启<code>from_logits=True</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">model.<span class=\"built_in\">compile</span>(loss=SparseCategoricalCrossEntropy(from_logits=<span class=\"literal\">True</span>)) <span class=\"comment\">#稀疏分类交叉熵损失</span></span><br></pre></td></tr></table></figure>\n\n<p><code>from_logits=True</code>的<a href=\"https://blog.csdn.net/muyuu/article/details/122762442\">作用</a></p>\n<p>需要概率时再调用<code>softmax</code></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">logits = model(X)</span><br><span class=\"line\">f_x = tf.nn.softmax(logits)</span><br></pre></td></tr></table></figure>\n\n<p><strong>多标签分类</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-10.png\" alt=\"img\"></p>\n<p>将每个标签看做一个二分类问题，输出层n个logistic函数，输出的y是一个向量。</p>\n<h2 id=\"高级优化方法\"><a href=\"#高级优化方法\" class=\"headerlink\" title=\"高级优化方法\"></a>高级优化方法</h2><p>传统的梯度下降学习率固定</p>\n<h3 id=\"Adam（Adaptive-Moment-estimation）\"><a href=\"#Adam（Adaptive-Moment-estimation）\" class=\"headerlink\" title=\"Adam（Adaptive Moment estimation）\"></a>Adam（Adaptive Moment estimation）</h3><p>如果看到学习率太小，而多次向同一个方向下降，会自动加大学习率</p>\n<p>如果看到学习率太大，某个参数值来回振荡，会自动减小学习率</p>\n<p>可以自动调整学习率$\\alpha$</p>\n<p>对于每个参数都有一个$\\alpha$</p>\n<p>选择optimizer&#x3D;adam即可</p>\n<h2 id=\"其他的网络层\"><a href=\"#其他的网络层\" class=\"headerlink\" title=\"其他的网络层\"></a>其他的网络层</h2><h3 id=\"卷积层（Convolutional-Layer）\"><a href=\"#卷积层（Convolutional-Layer）\" class=\"headerlink\" title=\"卷积层（Convolutional Layer）\"></a>卷积层（Convolutional Layer）</h3><p>每个神经元只能看到前一个层输入的一部分</p>\n<ul>\n<li>加快计算速度</li>\n<li>需要更少的数据，不容易过拟合</li>\n</ul>\n<p>有多个卷积层，即卷积神经网络</p>\n<p>每一层的单元只查看输入的一部分 </p>\n<h2 id=\"构建机器学习系统\"><a href=\"#构建机器学习系统\" class=\"headerlink\" title=\"构建机器学习系统\"></a>构建机器学习系统</h2><h3 id=\"评估一个模型\"><a href=\"#评估一个模型\" class=\"headerlink\" title=\"评估一个模型\"></a>评估一个模型</h3><p>特征只有一到二个还可以通过画图判断过拟合或者欠拟合，但是再多的特征就不适用了。</p>\n<p>将数据集分为训练集和测试集（73或者82开）</p>\n<p>分三步计算</p>\n<p><img src=\"/img/machine-learning-notes/pic-11.png\" alt=\"img\"></p>\n<p><strong>注意计算error时不包括正则化项</strong></p>\n<p>过拟合$J_{train}$很低，$J_{test}$很高，很好地评估模型的泛化能力</p>\n<p>对于分类问题，error就不再用交叉熵损失，直接用算法正确或者错误分类的个数（准确率accurate rate）</p>\n<h3 id=\"如何选择模型\"><a href=\"#如何选择模型\" class=\"headerlink\" title=\"如何选择模型\"></a>如何选择模型</h3><p>数据集分为三个子集，训练集$J_{train}$，交叉验证集$J_{cv}$，测试集$J_{test}$</p>\n<p>交叉验证集交叉检查不同模型的有效性和准确性，cross validation也叫<strong>dev set</strong>&#x2F;validation set</p>\n<p>$J_{train}$优化参数，$J_{cv}$选择模型，也叫优化超参数，$J_{test}$评估模型的泛化能力</p>\n<p>数据样本不够时622开可以，但是数据样本够的时候后两者不宜太多。</p>\n<h3 id=\"偏差和方差\"><a href=\"#偏差和方差\" class=\"headerlink\" title=\"偏差和方差\"></a><strong>偏差和方差</strong></h3><p><img src=\"/img/machine-learning-notes/pic-12.png\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-13.png\" alt=\"img\"></p>\n<p>高偏差意味着在训练集上表现不好，高方差意味着在交叉验证集表现比训练集上差得多</p>\n<p>高方差和高偏差同时存在是有可能的，大部分在神经网络中，线性回归不太可能。</p>\n<p><strong>正则化项参数对偏差和方差的影响：</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-14.png\" alt=\"img\"></p>\n<p>但是这些数值多少才算大&#x2F;小呢？需要<strong>建立基准性能标准</strong>，通常是衡量人类在这项任务做的有多好。另一种估计性能基线水平的方法是，是否有一些前人实现的算法来建立性能的基线水平。通过自己的模型效果和基准的比较判断是否有高方差&#x2F;高偏差的问题</p>\n<p><img src=\"/img/machine-learning-notes/pic-15.png\" alt=\"img\"></p>\n<p><strong>学习曲线</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-16.png\" alt=\"img\"></p>\n<p>高偏差时 </p>\n<p><img src=\"/img/machine-learning-notes/pic-17.png\" alt=\"img\"></p>\n<p>高方差时</p>\n<p><img src=\"/img/machine-learning-notes/pic-18.png\" alt=\"img\"></p>\n<p><img src=\"/img/machine-learning-notes/pic-19.png\" alt=\"img\"></p>\n<p>判断高方差或者高偏差决定下一步怎么做</p>\n<p><strong>神经网络中的偏差和方差</strong></p>\n<p>大型的神经网络有很小的偏差，所以只需要关注方差</p>\n<p>并且在合适的正则化下，大型神经网络也会和更小的神经网络工作的一样好甚至更好</p>\n<p>但是大型网络计算比较昂贵</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">layer = Dense(unit=<span class=\"number\">25</span>, activation=<span class=\"string\">&quot;relu&quot;</span>, kernel_regularizer=L2(<span class=\"number\">0.01</span>))</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"开发机器学习系统的迭代\"><a href=\"#开发机器学习系统的迭代\" class=\"headerlink\" title=\"开发机器学习系统的迭代\"></a>开发机器学习系统的迭代</h2><p><img src=\"/img/machine-learning-notes/pic-20.png\" alt=\"img\"></p>\n<h2 id=\"误差分析\"><a href=\"#误差分析\" class=\"headerlink\" title=\"误差分析\"></a>误差分析</h2><p>在交叉验证集手动选出几个（几百个）分类错误的例子，计数，归类几个原因，找到比较多的错误分类类型，更新学习算法</p>\n<h2 id=\"添加数据\"><a href=\"#添加数据\" class=\"headerlink\" title=\"添加数据\"></a>添加数据</h2><p>由误差分析，可以针对性地选择一些特定的数据，对于图像和语音识别，常用<strong>数据增强</strong>，用原有的数据样本创造新的样本</p>\n<p>例如旋转，放大，缩小图片，更改图片对比度，扭曲图片，对输入的x施加失真或变换。对语音添加背景噪声等</p>\n<p>此外还有<strong>数据合成</strong>，对于OCR文字识别，可以在真实图片基础上，更改字体，生成新的数据。一般在计算机视觉</p>\n<p>AI &#x3D; Code(algorithm&#x2F;model) + Data</p>\n<h2 id=\"迁移学习（Transfer-Learning）\"><a href=\"#迁移学习（Transfer-Learning）\" class=\"headerlink\" title=\"迁移学习（Transfer Learning）\"></a>迁移学习（Transfer Learning）</h2><p>对于神经网络，假设要进行0-9分类，但是数据集很小，可以借用有一个很大数据集的猫狗等1000类分类的神经网络，使用其中除了输出层以外的所有参数。</p>\n<p><img src=\"/img/machine-learning-notes/pic-21.png\" alt=\"img\"></p>\n<p>第一步叫做<strong>监督预训练</strong>(supervised pretraining)，获得除了输出层以外的层的权重；第二步叫做<strong>微调</strong>(fine tuning)，更改输出层的权重</p>\n<p>这样就可以在一个只有很小数据集的训练中，通过别的有很大数据集的不太相关的任务中学习</p>\n<p>通常下载别人预训练好并开源的神经网络，微调输出层参数来很好地学习自己的任务，但是输入x的类型（图片、音频、文本）也要和预训练模型一样</p>\n<p><img src=\"/img/machine-learning-notes/pic-22.png\" alt=\"img\"></p>\n<h2 id=\"机器学习项目的完整周期\"><a href=\"#机器学习项目的完整周期\" class=\"headerlink\" title=\"机器学习项目的完整周期\"></a>机器学习项目的完整周期</h2><p><img src=\"/img/machine-learning-notes/pic-23.png\" alt=\"img\"></p>\n<p>部署</p>\n<p><img src=\"/img/machine-learning-notes/pic-24.png\" alt=\"img\"></p>\n<p>MLOps(Machine Learning operations)：机器学习运维，系统构建，部署，维护机器学习系统的实践活动来确保机器学习系统可靠，监测损耗和及时更新。</p>\n<h2 id=\"关注公平、偏见、伦理\"><a href=\"#关注公平、偏见、伦理\" class=\"headerlink\" title=\"关注公平、偏见、伦理\"></a>关注公平、偏见、伦理</h2><h2 id=\"倾斜数据集的误差指标\"><a href=\"#倾斜数据集的误差指标\" class=\"headerlink\" title=\"倾斜数据集的误差指标\"></a>倾斜数据集的误差指标</h2><p>某个系统的正例和负例不一定都是对半开，例如判断某个稀有的病，构造<strong>混淆矩阵</strong>，包括<strong>真正例，假正例，真负例，假负例</strong></p>\n<p>常用的计算指标是<strong>精确度(precision)<strong>和</strong>召回率(recall)</strong></p>\n<p><img src=\"/img/machine-learning-notes/pic-25.png\" alt=\"img\"></p>\n<p>精确度展示预测出的的真实精确程度，召回率展示实际真实中预测出的精确程度</p>\n<p>权衡：</p>\n<p>当我们只有十分确信时才设置y&#x3D;1，设置logistic门槛为大于0.5，会导致精确度提高，召回率降低</p>\n<p>当我们不希望错过实际上的y&#x3D;1，设置logistic门槛为小于0.5，导致精确度降低，召回率提高</p>\n<p>通过设置threshold权衡precision和recall</p>\n<p>F1 score：自动组合精确度和召回率，选择最佳值，强调有比较低的值的算法（可能效果不好）</p>\n<p>$F1 score &#x3D; \\frac{1}{\\frac{1}{2}(\\frac{1}{P}+\\frac{1}{R})} &#x3D; 2\\frac{PR}{P+R}$</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><p>决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。</p>\n<p><img src=\"/img/machine-learning-notes/pic-26.png\" alt=\"img\"></p>\n<p><strong>决策树学习</strong>：</p>\n<ul>\n<li>如果选择每个节点选择什么特征来分类？</li>\n</ul>\n<p>应该最大化纯度，每一边的种类尽可能少</p>\n<ul>\n<li>什么时候停止分类？</li>\n</ul>\n<p>当一个节点100%是一个种类</p>\n<p>当分裂节点时会导致树超过最大高度（超参数）</p>\n<p>当提高的纯度分数低于一个门槛值</p>\n<p>当一个节点的样本数量低于一个门槛值</p>\n<h3 id=\"衡量纯度（purity）\"><a href=\"#衡量纯度（purity）\" class=\"headerlink\" title=\"衡量纯度（purity）\"></a>衡量纯度（purity）</h3><p>熵是对一组数据杂质的度量，$p_1$是目标种类数量在总数量得到占比，$p_0 &#x3D; 1 - p_1$</p>\n<p>$H(p_1)&#x3D;-p_1log_2(p_1)-p_0log_2(p_0) &#x3D; -p_1log_2(p_1)-(1-p_1)log_2(1-p_1)$</p>\n<p>注意：$0log(0) &#x3D; 0$</p>\n<p><img src=\"/img/machine-learning-notes/pic-27.png\" alt=\"img\"></p>\n<h3 id=\"减小熵：信息增益（Information-Gain）\"><a href=\"#减小熵：信息增益（Information-Gain）\" class=\"headerlink\" title=\"减小熵：信息增益（Information Gain）\"></a>减小熵：信息增益（Information Gain）</h3><p>当选择一个节点选择什么特征时，计算左右分支的熵，并进行加权平均计算，选择有最小结果的特征</p>\n<p>实际上是测量熵的减小量，由根节点原来的熵值$H(p)$减去左右分支的加权平均熵，此时选择更大的值</p>\n<p>为什么？当熵减小的量很小时，可以选择不分裂，而避免过拟合</p>\n<p><img src=\"/img/machine-learning-notes/pic-28.png\" alt=\"img\"></p>\n<p>更一般地</p>\n<p><img src=\"/img/machine-learning-notes/pic-29.png\" alt=\"img\"></p>\n<p>p是当前节点样本中正例的个数，w是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本）</p>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a>总结</h3><p>在根节点以所有数据样本开始</p>\n<p>计算所有特征的信息增益，选择最大的</p>\n<p>对选择的特征分裂，创建左右分支</p>\n<p>保持分裂直到遇到终止条件：</p>\n<ul>\n<li>当一个节点100%是一个类</li>\n<li>当分裂节点会导致树超过最大高度</li>\n<li>信息增益的值小于某个门槛值</li>\n<li>节点的样本数量小于某个门槛值</li>\n</ul>\n<p>实际上是一个递归的过程</p>\n<h3 id=\"独热编码-One-Hot-Encoding\"><a href=\"#独热编码-One-Hot-Encoding\" class=\"headerlink\" title=\"独热编码(One Hot Encoding)\"></a>独热编码(One Hot Encoding)</h3><p>实现有两个以上离散值的特征：如果一个类的特征有k个离散值，创建k个二元特征（0&#x2F;1）</p>\n<p>这样又转变为原来的左右分支分裂的情况</p>\n<h3 id=\"连续值特征\"><a href=\"#连续值特征\" class=\"headerlink\" title=\"连续值特征\"></a>连续值特征</h3><p>选定一个阈值，判断数据样本大于或者小于该阈值</p>\n<p>分割点将训练样本排序后取每对的中间值，10个样本就有9个分割点</p>\n<p>对分割点分别计算信息增强来选择阈值</p>\n<h3 id=\"回归树\"><a href=\"#回归树\" class=\"headerlink\" title=\"回归树\"></a>回归树</h3><p>分裂时，改成尽量选取输出的方差(Variance)小的特征</p>\n<p>w还是从上一节点样本中选择的样本数（当前样本&#x2F;上一节点样本），之后计算加权平均方差</p>\n<p>再用上一个节点所有数据的方差减去加权平均方差，选取最大的</p>\n<p>分类的结果是样本的平均值</p>\n<h2 id=\"使用多个决策树\"><a href=\"#使用多个决策树\" class=\"headerlink\" title=\"使用多个决策树\"></a>使用多个决策树</h2><p>单一决策树对数据中的微小变化十分敏感，所以要建立多个决策树（Tree Ensemble），并进行投票，使得算法更加健壮</p>\n<h3 id=\"放回抽样\"><a href=\"#放回抽样\" class=\"headerlink\" title=\"放回抽样\"></a>放回抽样</h3><p>从n个样本中放回地抽取n次，结果作为一个新的数据集</p>\n<h3 id=\"随机森林（Random-Forest）\"><a href=\"#随机森林（Random-Forest）\" class=\"headerlink\" title=\"随机森林（Random Forest）\"></a>随机森林（Random Forest）</h3><p>给定一个训练样本数m，进行b次的训练（一般不超过100），每次放回抽样创建一个新的大小为m的数据集，在此基础上训练一个决策树</p>\n<p>b个决策树构成袋状决策树（Bagged Decision Tree），输出结果进行投票决定最终输出</p>\n<p>对于每个节点，当要选择一个特征来分裂的时候，如果有n个特征可用，随机选择一个$k &lt; n$大小子集，使得算法只从这个子集里的特征选择信息增益最高得到特征进行分裂，当n很大时，经验做法是取$k &#x3D; \\sqrt{n}$</p>\n<h3 id=\"XGBoost（eXtreme-Gradient-Boosting）\"><a href=\"#XGBoost（eXtreme-Gradient-Boosting）\" class=\"headerlink\" title=\"XGBoost（eXtreme Gradient Boosting）\"></a>XGBoost（eXtreme Gradient Boosting）</h3><p>极端梯度提升树，与前面不同的是，进行放回抽样的时候，不是让每个样本有$\\frac{1}{m}$的概率被抽中，而是更可能抽到前面训练的树错误匹配的样本</p>\n<p>思想：关注我们已经训练好的树做的不好的地方，在之后刻意地尝试优化这部分</p>\n<ul>\n<li>提升树的开源实现</li>\n<li>快速，有效</li>\n<li>很好的设定结束分裂的标准</li>\n<li>内置正则化</li>\n</ul>\n<h3 id=\"什么时候使用决策树\"><a href=\"#什么时候使用决策树\" class=\"headerlink\" title=\"什么时候使用决策树\"></a>什么时候使用决策树</h3><p>一个或多个决策树</p>\n<ul>\n<li>在表格化和结构化的数据上工作的很好</li>\n<li>不建议在非结构化的数据上，例如图片，音频，文本</li>\n<li>训练快速</li>\n<li>决策树是人类可以理解的（可解释性）</li>\n</ul>\n<p>神经网络</p>\n<ul>\n<li><p>对于所有类型的数据都能工作的很好</p>\n</li>\n<li><p>比决策树更慢</p>\n</li>\n<li><p>可以很好地使用迁移学习（预训练+微调）</p>\n</li>\n<li><p>当建立一个有多个模型一起工作的系统，链接神经网络会更简单（输出都是光滑的，连在一起仍然可微，决策树一次只能训练一个）</p>\n</li>\n</ul>\n<h1 id=\"Course-3\"><a href=\"#Course-3\" class=\"headerlink\" title=\"Course 3\"></a>Course 3</h1><p>除了监督学习，机器学习还包括</p>\n<ul>\n<li>无监督学习<ul>\n<li>聚类</li>\n<li>异常检测</li>\n</ul>\n</li>\n<li>推荐系统</li>\n<li>强化学习</li>\n</ul>\n<h2 id=\"聚类\"><a href=\"#聚类\" class=\"headerlink\" title=\"聚类\"></a>聚类</h2><p>一堆数据点中自动查找相互关联或者相似的数据点</p>\n<h3 id=\"K-means\"><a href=\"#K-means\" class=\"headerlink\" title=\"K-means\"></a>K-means</h3><p>首先随机初始化K个簇中心点$\\mu_1 ,\\mu_2… \\mu_k$，$\\mu$应该是一个向量，与输入有相同的维度</p>\n<ul>\n<li>将每个点分配给离他最近的中心点（centroid质心）</li>\n<li>将中心点移动到分配的点的平均中心</li>\n<li>重复前两步，直到中心点不再移动，K-means算法收敛</li>\n</ul>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Repeat&#123;</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to m</span><br><span class=\"line\">\t\tc_i 是距离x_i点最近得到簇中心点的下标（从1-k）</span><br><span class=\"line\">\t\t//其中距离为 min_k ||x_i - u_k||，可以加平方</span><br><span class=\"line\">\t<span class=\"keyword\">for</span> i = 1 to k</span><br><span class=\"line\">\t\tu_k更新为分配的点的中心（每个轴的点的平均值）</span><br><span class=\"line\">\t\t如果簇中心点没有分配到点，就删除</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"损失函数\"><a href=\"#损失函数\" class=\"headerlink\" title=\"损失函数\"></a>损失函数</h3><p><img src=\"/img/machine-learning-notes/pic-30.png\" alt=\"img\"></p>\n<p>$c^{(i)}$是$x^{(i)}$被分配到的簇的下标（1-k）</p>\n<p>$u_k$是簇k</p>\n<p>$\\mu _{c^{(i)}}$是$x^{(i)}$被分配到的簇</p>\n<p>损失函数就是每个点到其分配到的簇的距离平方的平均值，其中距离是<strong>欧几里得距离</strong></p>\n<p>也叫Distortion Function</p>\n<h3 id=\"初始化\"><a href=\"#初始化\" class=\"headerlink\" title=\"初始化\"></a>初始化</h3><p>选择$K&lt;m$</p>\n<p>随机选择K个训练样本，将$\\mu_1 ,\\mu_2… \\mu_k$设定为这几个点，每次运行容易得到局部最小值，所以运行多次，找到效果最好的点</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> i = 1 to 100&#123;</span><br><span class=\"line\">\t随机初始化</span><br><span class=\"line\">\t获取c_i, u_i</span><br><span class=\"line\">\t计算损失函数J</span><br><span class=\"line\">&#125;</span><br><span class=\"line\">选择J最小的初始化参数，i可以从50到1000，充分避免局部最小值</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"选择簇的个数\"><a href=\"#选择簇的个数\" class=\"headerlink\" title=\"选择簇的个数\"></a>选择簇的个数</h3><p><strong>肘法（Elbow Method）</strong></p>\n<p>选取不同的K，绘制损失函数曲线，选择肘点，但是这个方法不通用，不是每一次都有肘点</p>\n<p>所以K的选择还是按照之后的任务目的选择</p>\n<h2 id=\"异常检测\"><a href=\"#异常检测\" class=\"headerlink\" title=\"异常检测\"></a>异常检测</h2><h3 id=\"密度估计（Density-estimation）\"><a href=\"#密度估计（Density-estimation）\" class=\"headerlink\" title=\"密度估计（Density estimation）\"></a>密度估计（Density estimation）</h3><p>根据数据集建立模型$p(x)$，其中特征向量x的概率，对于$x_{test}$，求得$p$，若$p(x_{test})&lt;\\epsilon$，认为出现了异常（anomaly）</p>\n<h3 id=\"高斯分布\"><a href=\"#高斯分布\" class=\"headerlink\" title=\"高斯分布\"></a>高斯分布</h3><p>Gaussian Distribution，也叫正态分布(Normal Distribution)</p>\n<p>$p(x)&#x3D;\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)}{2\\sigma^2}^2}$</p>\n<p>其中$\\mu$是平均值，$\\sigma$是标准差</p>\n<p><img src=\"/img/machine-learning-notes/pic-31.png\" alt=\"img\"></p>\n<h3 id=\"算法实现\"><a href=\"#算法实现\" class=\"headerlink\" title=\"算法实现\"></a>算法实现</h3><p>对于有多个特征的输入$\\vec{x}$，$\\vec{x} &#x3D; [x_1, x_2 … x_n]$</p>\n<p>$$<br>p(\\vec{x}) &#x3D; p(x_1;\\mu_1,\\sigma_1^2) * p(x_2;\\mu_2,\\sigma_2^2) <em>…</em> p(x_n;\\mu_n,\\sigma_n^2) &#x3D; \\prod_{j&#x3D;1}^np(x_j;\\mu_j,\\sigma_j^2)<br>$$</p>\n<h3 id=\"开发和评估异常检测系统\"><a href=\"#开发和评估异常检测系统\" class=\"headerlink\" title=\"开发和评估异常检测系统\"></a>开发和评估异常检测系统</h3><p>通常在训练集训练（无标签），在cv集加入异常的样本，打上标签0&#x2F;1，选择合适的$\\epsilon$使得在cv集可以很好地工作，对于异常样本很多的情况下，可以再使用测试集</p>\n<p><strong>流程：</strong></p>\n<p>在训练集$x_1…x_m$上拟合模型$p(x)$</p>\n<p>在交叉验证集或者测试集上，预测y（如果小于epsilon为1否则为0）</p>\n<p>之后计算真正例，精确度Precision，召回率Recall和F1分数等指标衡量模型，并且选择更好的参数$\\epsilon$</p>\n<h3 id=\"权衡异常检测和监督学习\"><a href=\"#权衡异常检测和监督学习\" class=\"headerlink\" title=\"权衡异常检测和监督学习\"></a>权衡异常检测和监督学习</h3><p>异常检测：有很多种异常，对于算法来说很难从已知的异常中学习，因为未来的异常可能与当前的完全不一样</p>\n<p>监督学习：有足够的正例使得算法学会识别正例，未来的正例也是与当前训练集里的类似</p>\n<h3 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h3><p>监督学习中，特征如果不重要可以让参数变得小一点，但在异常检测中，特征的选择更加重要</p>\n<ul>\n<li>绘制直方图，转换保证特征符合高斯分布，注意cv集和测试集也要同样转换（开根号，取对数）</li>\n<li>检查是否在cv集效果不好，分析原因，看看有没有新的特征可以选取</li>\n</ul>\n<h2 id=\"推荐系统\"><a href=\"#推荐系统\" class=\"headerlink\" title=\"推荐系统\"></a>推荐系统</h2><p>$r(i,j) &#x3D; 1$表示用户j为电影i打分</p>\n<p>$y^{(i,j)}$表示用户j为电影i打的分</p>\n<p>$w^{(j)}, b^{(j)}$是用户j的参数</p>\n<p>$x^{(i)}$是电影i的特征向量</p>\n<p>对于用户j和电影i，预测评分$w^{(j)} \\cdot x^{(i)}+b^{(j)}$</p>\n<p>$m^{(j)}$表示用户j打分的电影数量</p>\n<p>通过训练学习$w^{(j)}, b^{(j)}$</p>\n<p>$$<br>\\min_{w^{(j)}b^{(j)}}J\\left(w^{(j)},b^{(j)}\\right)&#x3D;\\frac{1}{2m^{(j)}}\\sum_{(i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2m^{(j)}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<p>对所有用户都要学习参数$w^{(1)},b^{(1)},w^{(2)},b^{(2)},…,w^{(n_u)},b^{(n_u)}$</p>\n<p>$$<br>\\left.\\mathrm{J}\\left(<br>\\begin{array}<br>{cc}{w^{(1)},} &amp; {…,w^{(n_{u})}} \\<br>{b^{(1)},} &amp; {…,b^{(n_{u})}}<br>\\end{array}\\right.\\right)&#x3D;\\frac{1}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{i:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}\\quad+\\frac{\\lambda}{2}\\sum_{j&#x3D;1}^{n_{u}}\\sum_{k&#x3D;1}^{n}\\left(w_{k}^{(j)}\\right)^{2}<br>$$</p>\n<h3 id=\"协同过滤算法\"><a href=\"#协同过滤算法\" class=\"headerlink\" title=\"协同过滤算法\"></a>协同过滤算法</h3><p>在上面的例子中，我们已经得到了每部电影的特征的值是多少，可以使用线性回归，但是当不知道的时候，需要使用$w^{(j)}, b^{(j)}$来推测每部电影的特征值是多少</p>\n<p>$$<br>\\mathrm{J}(x^{(i)})&#x3D;\\frac{1}{2}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-{y^{(i,j)}}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>学习得到$x^{(1)},x^{(2)},…,x^{(n_m)}$</p>\n<p>$$<br>\\mathrm{J}\\left(x^{(1)},x^{(2)},…,x^{(n_{m})}\\right)&#x3D;\\frac{1}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{j:r(i,j)&#x3D;1}\\left(w^{(j)}\\cdot x^{(i)}+b^{(j)}-y^{(i,j)}\\right)^{2}+\\frac{\\lambda}{2}\\sum_{i&#x3D;1}^{n_{m}}\\sum_{k&#x3D;1}^{n}\\left(x_{k}^{(i)}\\right)^{2}<br>$$</p>\n<p>将这里与上面提到求w,b的算法结合起来，构成协同过滤算法：</p>\n<p><img src=\"/img/machine-learning-notes/pic-32.png\" alt=\"img\"></p>\n<p>梯度下降时，w，b，x都是参数</p>\n<p><img src=\"/img/machine-learning-notes/pic-33.png\" alt=\"img\"></p>\n<p><a href=\"https://blog.csdn.net/zhu_xian_gang/article/details/130243870\">补充</a></p>\n<h3 id=\"二进制标签\"><a href=\"#二进制标签\" class=\"headerlink\" title=\"二进制标签\"></a>二进制标签</h3><p>1-用户看到物品之后参与点击，停留，添加喜欢，购买</p>\n<p>0-用户看到物品之后忽略</p>\n<p>?-用户没有看到物品</p>\n<p>预测$y^{(i,j)}&#x3D;1$的概率，由$g(w^{(j)} \\cdot x^{(i)}+ b^{(i)})$，g是logistic函数</p>\n<p><img src=\"/img/machine-learning-notes/pic-34.png\" alt=\"img\"></p>\n<h3 id=\"均值归一化\"><a href=\"#均值归一化\" class=\"headerlink\" title=\"均值归一化\"></a>均值归一化</h3><p><strong>Mean Normalization</strong></p>\n<ul>\n<li>求均值$\\mu$</li>\n<li>$x_1 &#x3D; \\frac{x_1-\\mu}{max-min}$</li>\n</ul>\n<p>求出每个电影的平均用户平方$\\mu_i$，构建向量$u$</p>\n<p>对于用户j，预测其在电影i的评分：</p>\n<p>$w^{(j)} \\cdot x^{(i)}+ b^{(i)} + \\mu_i$</p>\n<p>以至于不会当用户没有评分时认为评分接近0，而是接近平均值</p>\n<h3 id=\"查找相关项目\"><a href=\"#查找相关项目\" class=\"headerlink\" title=\"查找相关项目\"></a>查找相关项目</h3><p>对于项目$i$的特征$x^{(i)}$，为了找到相关的项目$k$，需要找到$x^{(k)}$与$x^{(i)}$相似</p>\n<p>选取小的$\\sum_{l&#x3D;1}^n(x_l^{(k)} - x_l^{(i)})^2$</p>\n<p>也可以写作$||x^{(k)} - x^{(i)}||^2$</p>\n<h3 id=\"协同过滤算法的限制\"><a href=\"#协同过滤算法的限制\" class=\"headerlink\" title=\"协同过滤算法的限制\"></a>协同过滤算法的限制</h3><p><strong>冷启动问题</strong></p>\n<ul>\n<li>如何对没有什么用户打分的项目评分？</li>\n<li>如何对没有对很多项目打分的用户推荐一些项目？</li>\n</ul>\n<p><strong>没有很多信息的时候利用辅助信息</strong></p>\n<h3 id=\"基于内容的过滤算法\"><a href=\"#基于内容的过滤算法\" class=\"headerlink\" title=\"基于内容的过滤算法\"></a>基于内容的过滤算法</h3><p>协同过滤：基于用户的评分与你的评分的相似推荐项目</p>\n<p>基于内容过滤：基于用户和项目特征的匹配良好程度推荐项目</p>\n<p>但是电影的特征数和用户的特征数大概率不一样多，所以需要提取出$v^{(j)}$和$v^{(i)}$（相同维度）进行匹配</p>\n<p>对于v的获取，使用神经网络</p>\n<p>可以分别建立user network和movie network，使用相同维度的输出层，将结果进行点积</p>\n<p>也可以将两个网络合并，在内部进行点积输出结果</p>\n<p>$$<br>J&#x3D;\\sum_{(i,j):r(i,j)&#x3D;1}\\left(v_{u}^{(j)}\\cdot v_{m}^{(i)}-y^{(i,j)}\\right)^{2}+\\text{NN regularization term}<br>$$</p>\n<p>为了找到电影i的相似电影，找$||v^{(k)} - v^{(i)}||^2$小的电影，最为相似</p>\n<h3 id=\"Retrieval-and-Ranking\"><a href=\"#Retrieval-and-Ranking\" class=\"headerlink\" title=\"Retrieval and Ranking\"></a>Retrieval and Ranking</h3><p>通常样本有几百万或者几千几万，不可能对每个样本构造神经网络，所以采用检索和排名</p>\n<p>检索：生成可能得项目列表，比如从用户最近观看的10个电影中找到相似的，从最常看的3个类别中选出其中的top10，用户所在国家的top20。将检索的项目列表，去除重复项目和用户已经观看</p>\n<p>排名：对这些检索出的有限个项目进行学习，根据结果进行排名</p>\n<p>权衡检索的项目数量</p>\n<h2 id=\"强化学习\"><a href=\"#强化学习\" class=\"headerlink\" title=\"强化学习\"></a>强化学习</h2><p>强化学习（Reinforcement Learning，简称RL）是一种机器学习范式，它通过让智能体（Agent）与环境（Environment）进行交互，学习如何做出最优决策，以最大化累积奖励（Reward）。强化学习的核心思想是通过试错（Trial and Error）的方式，让智能体逐步探索环境，找到最优的行为策略。</p>\n<p>涉及状态，行动，奖励，折扣系数，回报，策略</p>\n<h3 id=\"回报\"><a href=\"#回报\" class=\"headerlink\" title=\"回报\"></a>回报</h3><p>指的是系统获得的奖励总和</p>\n<p>折扣系数$\\gamma$，是一个无限接近1的数字，例如0.9,0.99</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+…$，直到终止状态</p>\n<h3 id=\"策略\"><a href=\"#策略\" class=\"headerlink\" title=\"策略\"></a>策略</h3><p>状态state通过策略π实行行动a</p>\n<p>$\\pi(s) &#x3D; a$，指明状态s情况下需要进行的决策a，从而最大化回报</p>\n<h3 id=\"马尔科夫决策过程\"><a href=\"#马尔科夫决策过程\" class=\"headerlink\" title=\"马尔科夫决策过程\"></a>马尔科夫决策过程</h3><p>Markov Decision Process(MDP)</p>\n<p><img src=\"/img/machine-learning-notes/pic-35.png\" alt=\"img\"></p>\n<h3 id=\"状态-动作价值函数\"><a href=\"#状态-动作价值函数\" class=\"headerlink\" title=\"状态-动作价值函数\"></a>状态-动作价值函数</h3><p>State-action value function，也叫Q-function,Q*,Optimal Q function</p>\n<p>$Q(s,a)$的值等于你从状态s开始执行一个动作a之后，表现的最好所获得的回报</p>\n<p>在状态s的最好回报就是$max_aQ(s,a)$</p>\n<p>在状态s的最好动作的就能够提供$max_aQ(s,a)$的</p>\n<h3 id=\"Bellman方程\"><a href=\"#Bellman方程\" class=\"headerlink\" title=\"Bellman方程\"></a>Bellman方程</h3><p>$s$:当前状态</p>\n<p>$a$:当前状态的决策</p>\n<p>$R(s)$:当前状态的奖励</p>\n<p>$s’$:采取动作a后的状态</p>\n<p>$a’$:在状态s’采取的动作</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>R(s)也叫即时奖励，表示你可以立刻得到的奖励</p>\n<p>后一项是从状态s’表现得最好获得的回报</p>\n<p>$\\text{Return} &#x3D; R_1 + \\gamma R_2 + \\gamma^2R_3+… &#x3D; R_1 + \\gamma[R_2 + \\gamma R_3+…]$</p>\n<h3 id=\"随机环境\"><a href=\"#随机环境\" class=\"headerlink\" title=\"随机环境\"></a>随机环境</h3><p>由于不可控因素，强化学习问题是随机的，不一定会按照某个序列，而是有很多个可能得序列，得到不同的奖励</p>\n<p>所以问题不是最大化回报，而是最大化奖励之和得到平均值，也就是期望</p>\n<p>$\\text{Return} &#x3D; \\text{Average}(R_1 + \\gamma R_2 + \\gamma^2R_3+…) &#x3D; \\text{E}(R_1 + \\gamma R_2 + \\gamma^2R_3+…)$</p>\n<p>Bellman Equation变成：</p>\n<p>$Q(s,a) &#x3D; R(s)+\\gamma \\text{E} [max_{a’}Q(s’,a’)]$</p>\n<h3 id=\"连续状态空间\"><a href=\"#连续状态空间\" class=\"headerlink\" title=\"连续状态空间\"></a>连续状态空间</h3><p>状态参数可能是连续的，比如坐标，角度，速度</p>\n<p>同时状态可能有多个，比如xyz坐标，速度等</p>\n<p>此时也叫连续状态马尔科夫决策过程</p>\n<h3 id=\"学习状态值函数\"><a href=\"#学习状态值函数\" class=\"headerlink\" title=\"学习状态值函数\"></a>学习状态值函数</h3><p><img src=\"/img/machine-learning-notes/pic-36.png\" alt=\"img\"></p>\n<p>以随机猜测$Q(s,a)$初始化神经网络</p>\n<p>重复：</p>\n<p>采取措施，得到$(s,a,R(s),s’)$元组</p>\n<p>存储最近的10k个 $(s,a,R(s),s’)$元组（Replay Buffer）</p>\n<p>训练网络：</p>\n<p>​\t创建10k个训练集，其中$x&#x3D;(s,a)$，$y &#x3D; R(s)+\\gamma max_{a’}Q(s’,a’)$</p>\n<p>​\t训练$Q_{new}$使得$Q_{new}(s,a) \\approx y$</p>\n<p>令$Q&#x3D;Q_{new}$</p>\n<p>虽然刚开始Q是随机猜测的，但是随着训练迭代，Q的值会变成真实值的良好估计</p>\n<p><strong>改进</strong></p>\n<ul>\n<li>神经网络架构</li>\n</ul>\n<p>可以直接将输出层改成每种决策的结果输出，就不用分别计算多次不同决策，只用计算一次就行</p>\n<p><img src=\"/img/machine-learning-notes/pic-37.png\" alt=\"img\"></p>\n<ul>\n<li>$\\epsilon$贪心策略</li>\n</ul>\n<p>当正在学习时如何选择决策，不应该都选择能最大化Q的a，因为当Q时随机初始化的，大的不一定好。</p>\n<p>应该选择大概率例如0.95选择最大化的Q，也是贪心greedy，或者exploitation。再0.05概率随机选择别的策略（探索exploration）</p>\n<p>小概率的值就是epsilon，这个策略也叫做epsilon贪心策略，开始的e比较大，逐渐减小。</p>\n<ul>\n<li>小批量$mini-batch$</li>\n</ul>\n<p>将数据集分成几个小的集合，每次迭代查看一个小数据集，梯度下降最开始虽然不是朝最优方向，但是越来越优</p>\n<p><img src=\"/img/machine-learning-notes/pic-38.png\" alt=\"img\"></p>\n<p>假设子集大小为1000；</p>\n<p>具体过程，是先取出1000个数据，前向计算出结果，再反向传导计算出代价函数对w和b的偏导数；接着计算出代价函数的和，然后取这1000次的平均值，进行优化；然后再拿出1000个数据，再次计算代价函数与导数，再次优化，重复进行直到全部数据集取完即可。</p>\n<p>在强化学习中，可以把10k的数据集分解训练多个模型</p>\n<ul>\n<li>软更新</li>\n</ul>\n<p>令$Q&#x3D;Q_{new}$时，不直接把$w,b$换成$w_{new},b_{new}$</p>\n<p>而是<br>$$<br>w &#x3D; 0.01w_{new} + 0.99w<br>$$</p>\n<p>$$<br>b &#x3D; 0.01b_{new} + 0.99b<br>$$</p>\n<p>对参数进行微小调整</p>\n"}],"PostAsset":[],"PostCategory":[],"PostTag":[],"Tag":[]}}