---
title: AI面经
mathjax: true
date: 2025/01/26 20:46:25
img: https://img1.baidu.com/it/u=4282277671,2501328998&fm=253&fmt=auto&app=138&f=JPEG?w=449&h=252
excerpt: 人工智能经典面试问题
---

### 1 机器学习、深度学习、与人工智能对比

- **机器学习是一种实现人工智能的方法，深度学习是一种实现机器学习的技术**

> **人工智能**（人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器）
>
> **分类**： 弱人工智能（当前）、强人工智能、超人工智能
>
> **机器学习**（MachineLearning）简称ML：机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。
>
> **深度学习**（DeepLearning）简称DL。**最初**的深度学习是**利用深度神经网络**来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于**建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本**。

#### 1.1 人工智能

> **人工智能（Artificial intelligence）简称AI**。人工智能是计算机科学的一个分支，它企图了解智能的本质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器，是研究、开发用于模拟、延伸和扩展人的智能的理论、方法、技术及应用系统的一门新的技术科学。
>
> - 人工智能目前分为弱人工智能和强人工智能和超人工智能：
> - **弱人工智能**：弱人工智能（Artificial Narrow Intelligence /ANI),只专注于完成某个特定的任务，例如语音识别、图象识别和翻译等，是擅长于单个方面的人工智能。它们只是用于解决特定的具体类的任务问题而存在，大都是统计数据，以此从中归纳出模型。由于弱人工智能智能处理较为单一的问题，且发展程度并没有达到模拟人脑思维的程度，所以弱人工智能仍然属于“工具”的范畴，与传统的“产品”在本质上并无区别。
>
> - **强人工智能**：强人工智能（Artificial General lnteligence /AGI),属于人类级别的人工智能，在各方面都能和人类比肩，它能够进行思考、计划、解决问题、抽象思维、理解复杂理念、快速学习和从经验中学习等操作，并且和人类一样得心应手。
>
> - **超人工智能**：超人工智能（Artificial Superintelligence/ASI），在几乎所有领域都比最聪明的人类大脑都聪明许多，包括科学创新、通识和社交技能。在超人工智能阶段，人工智能已经跨过“奇点”，其计算和思维能力已经远超人脑。此时的人工智能已经不是人类可以理解和想象。人工智能将打破人脑受到的维度限制，其所观察和思考的内容，人脑已经无法理解，人工智能将形成一个新的社会。
>
> **目前我们仍处于弱人工智能阶段**。

#### 1.2 机器学习

> **机器学习（Machine Learning）简称ML**。机器学习属于人工智能的一个分支，也是人工智能的和核心。机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法。

#### 1.3 深度学习

> **深度学习（Deep Learning）简称DL**。最初的深度学习是利用深度神经网络来解决特征表达的一种学习过程。深度神经网络本身并不是一个全新的概念，可大致理解为包含多个隐含层的神经网络结构。为了提高深层神经网络的训练效果，人们对神经元的连接方法和激活函数等方面做出相应的调整。深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，如图象、声音、文本。
>
> 注意：你可能在接触深度学习的时候也听到过**监督学习**、**非监督学习**、**半监督学习**等概念，下面就顺便对这三个名词解析下：
>
> 1. **监督学习：**用一部分已知分类、有标记的样本来训练机器后，让它用学到的特征，对没有还分类、无标记的样本进行分类、贴标签。多用于分类。
> 2. **非监督学习：**所有的数据没有标记，类别未知，让它自己学习样本之间的相似性来进行分类。多用于聚类。
> 3. **半监督学习：**有两个样本集，一个有标记，一个没有标记。综合利用有类标的样本（ labeled sample）和没有类标的样本（ unlabeled sample），来生成合适的分类。

#### 1.4 区别与联系

![img](\img\ai-mianjing\pic-1.png)

- 人工智能的研究领域包括 **专家系统**(Expert Systems)、**机器学习**(Machine Learning)、**模式识别**、**进化计算**(Evolutionary Computation)、**模糊逻辑**(Fussy Logic)、**计算机视觉**(Computer Vision)、**自然语言处理**(NLP)、**推荐系统**(Recommender Systems)
- 深度学习，一种**实现机器学习**的技术。是机器学习中的一个方法或者分支。
- **深度学习是用于建立、模拟人脑进行分析学习的神经网络，并模仿人脑的机制来解释数据的一种机器学习技术**。
- 它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。最显著的应用是计算机视觉和[自然语言处理](https://so.csdn.net/so/search?q=自然语言处理&spm=1001.2101.3001.7020)(NLP)领域。显然，**“深度学习”是与机器学习中的“神经网络”是强相关**，“神经网络”也是其主要的算法和手段；或者我们可以将“深度学习”称之为“改良版的神经网络”算法。
- **总结：人工智能是一个很老的概念，机器学习是人工智能的一个子集，深度学习又是机器学习的一个子集。机器学习与深度学习都是需要大量数据来“喂”的，是大数据技术上的一个应用，同时深度学习还需要更高的运算能力支撑，如GPU**

![img](\img\ai-mianjing\pic-2.png)

### 2 AI的分类

- **模式识别**：

  - **模式识别是指用计算机对物体进行识别。**

    **物体指文字、符号、图形、图像、语音、声音等实体对象；**

    •**不包括概念、思想、意识等虚拟对象，它们属于认知和哲学研究范畴。**

  - **模式识别应用：**

    • **文字识别，车牌识别，人脸识别，视网膜识别，指纹识别，掌纹识别等**

- **自然语言处理**：用自然语言同计算机进行通讯的一种技术

  - 机器翻译
  - 语音识别（**可以单列**）
  - 文本分类/预测

- **机器学习**（重点是深度学习）：机器学习理论主要是设计和分析一些让计算机可以自动”学习“的算法

- **计算机视觉**：用摄影机和电脑代替人眼对目标进行识别、跟踪和测量等机器视觉，并进一步做图形处理，使电脑处理成为更适合人眼观察或传送给仪器检测的图像

- **智能机器人**：给机器人装上“大脑芯片”，从而使其智能性更强，在认知学 习、自动组织、对模糊信

  息的综合处理等方面将会前进一大步

- **自动程序设计**：自动程序设计是指根据给定问题的原始描述，自动生成满足要求的程序

- **数据挖掘**：从大量的数据中搜索隐藏在其中信息的过程

- **专家系统**等

### 3 一些深度学习概念

#### **监督学习基础定义**：

**有监督学习 (Supervised Learning)**: 使用**已经标注的数据**训练模型，目的是让模型学会如何将输入映射到期望的输出。例如，使用一组已知类别的图片来训练一个图像**分类模型**，模型学习如何根据输入的图片预测正确的类别 。

**无监督学习 (Unsupervised Learning)**: **不使用任何标注数据**，让模型自己发现数据中的结构和模式。例如，**聚类**算法可以将数据分为几个不同的组，而不事先告诉模型应该有多少组，或者各组的特征 。

**半监督学习 (Semi-Supervised Learning)**: 结合**有监督和无监督学习**的方法，使用**少量标注数据和大量未标注数据**共同训练模型。这种方法在标注数据难以获得但未标注数据丰富的场景下特别有用 。 例如：**医学影像**，大部分没有标签，人工打标较为昂贵。可以用

- 使用已标注数据训练一个分类器。
- 用该分类器对未标注数据进行预测，生成伪标签。
- 选择置信度最高的伪标签来扩充训练集。
- 结合原始的标注数据和带有伪标签的数据重新训练分类器

**自监督学习 (Self-Supervised Learning)**: 一种特殊的无监督学习方法，通过**自动生成伪标签**来训练模型。这种方法通过创建预测任务（如预测未来的帧、填充缺失的部分、一个自监督学习模型可以通过尝试预测句子中缺失的单词来学习语言的语法和词汇等），使模型能够从输入数据本身学习有用的表示 。 通常是**预训练+下游任务**的方式。

**对比**：

> - **有监督学习**通常需要大量的标注数据，适用于标注数据充足的任务。
> - **无监督学习**不需要标签，适用于探索数据内在结构和关系的任务。
> - **半监督学习**结合了有监督和无监督的优点，适用于标注数据有限但无标签数据多的情况。
> - **自监督学习**则是一种新兴的方法，它能够从未标注的数据中学习到有用的特征表示，适用于标注数据难以获得但需要丰富特征表示的任务。

**半监督学习举例**

- 使用已标注数据训练一个分类器。
- 用该分类器对未标注数据进行预测，生成伪标签。
- 选择置信度最高的伪标签来扩充训练集。
- 结合原始的标注数据和带有伪标签的数据重新训练分类器

#### **对比预训练**：

> 对比预训练是一种自监督学习方法，其核心目标是通过学习**区分不同数据之间的相似性和差异性来提升模型的特征表示能力**。这种方法不依赖于人工标注的数据，而是通过**定义一种或多种对比任务**（比如区分不同图像、文本或声音等），使得模型能够自动学习到数据的**内在特征和结构**。对比预训练通过比较正样本对（相似）和负样本对（不相似）来优化模型，使模型能够捕捉到数据中有用的信息和模式，从而在没有大量标注数据的情况下也能有效提升模型性能 。
>
> **在NLP和CV等领域，对比学习已经成为一种重要的自监督学习方法**，**通过自监督预训练模式**，模型可以从数据本身的先验知识分布中吸取图像或文本的特征，得到一个能够更好地适应不同任务和领域的预训练模 。这种方法的优势在于**无需大量标注数据**，同时可以**显著提高模型的泛化性能和学习能力** 。

#### **平行语料库**：

> **平行语料库是收录了某一源语言文本及其对应的目标语文本的语料库**。具体来说，平行语料库包括以下内容：
>
> - **源语言和目标语言的对应文本**：这些文本在语义上是相同的，只是用不同的语言表达，如英文原版和中文翻译版。
> - **对齐的层面**：平行对齐指的是源语文本和目标语文本之间的具体单位的对应关系或翻译关系，可以细分为词汇、语句和段落等层面的对齐。
> - **类型**：根据所涉及的语种数量和方向，平行语料库可以分为单向平行语料库、双向平行语料库和多向平行语料库。
> - **应用**：平行语料库对于语言对比、双语词典编纂、机器翻译、翻译策略与规范研究等都具有很高的应用价值。
>
> 总的来说，平行语料库不仅为机器翻译提供了必要的数据支持，也是推动相关领域发展和进步的重要驱动力。随着科技的不断进步和全球化的深入发展，平行语料库将在未来发挥更加重要的作用

#### **联合学习joint study**：

> **联合学习（Joint Learning）是一种机器学习范式，它涉及同时训练多个模型或任务，以便它们可以共享知识或参数，从而提高整体的学习效率和泛化能力**。
>
> 在联合学习中，不同的模型或任务之间存在一定的关联性，这种关联性可以是因为它们处理相似的数据类型，或者因为它们解决的是相关的子问题。通过联合学习，这些模型可以相互促进，提高各自的性能。以下是一些关键点：
>
> - **模型组合**：在联合学习中，一个大模型由多个小模型组成，这些小模型可以独立训练，也可以与其他模型一起联合训练。
> - **多任务学习**：联合学习与多任务学习（Multi-Task Learning）有相似之处，多任务学习也是让一个模型同时学习多个任务，并且在学习过程中共享参数。这样做的好处是可以利用任务之间的相关性来提升模型的泛化能力。
> - **应用领域**：联合学习在自然语言处理（NLP）领域有广泛的应用，例如，可以通过联合模型来同时处理语法分析和语义理解等任务。
> - **端到端学习**：在某些情况下，联合学习还涉及到端到端的训练方法，这意味着从输入到输出的整个流程都被整合在一个学习过程中，以此来优化整体性能。
>
> 总的来说，联合学习是一种强大的学习方法，它通过整合多个相关任务或模型的学习过程，能够提高学习效率和模型的泛化能力。这种方法在处理复杂问题时特别有用，因为它可以有效地利用不同任务之间的共同信息。

### 4 衡量模型好坏的一些指标

#### F1 score 精确率precision 召回率re-call

![img](\img\ai-mianjing\pic-3.png)

![img](\img\ai-mianjing\pic-4.png)

#### IOU mIOU

![img](\img\ai-mianjing\pic-5.png)

### 机器学习20道

1. 网络配置时batchsize的大小怎样设置？过小和过大分别有什么特点？ **小的不稳定 大的速度快**

   > - 大的batchsize **减少训练时间，提高稳定性**。 这是肯定的，同样的epoch数目，在性能允许情况下，大的batchsize需要的batch数目减少了，所以可以**减少训练时间**。另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑。
   > - 过大的batchsize 泛化能力下降。而**过大的批量大小会导致收敛速度变慢**，并且可能需要更多的内存 在一定范围内，**增加batchsize有助于收敛的稳定性**，但是随着batchsize的增加，**模型的性能会下降**。
   > - 同样是通过对训练步数的影响，小的batch_size使模型迭代次数增多，**提前到达拟合点**，但是epoch没结束，继续学习训练数据，容易导致过拟合于原始数据

2. 设置学习率衰减的原因？

   > 学习率衰减是为了让学习率随着训练过程逐渐减小，有助于模型**在接近全局最优解时减小步长**，避免在优化过程中跳过最佳点或产生不必要的波动

3. 有哪些分类算法？

   > 朴素贝叶斯、逻辑回归、K 最近邻 （KNN）、支持向量机 （SVM）、决策树、随机森林和神经网络

4. 分类和回归的区别？

   > 分类预测**离散标签**，将数据分类为两个或多个类别。回归预测**连续量**，估计变量之间的关系

5. 请描述下k-means聚类的过程？

   > k-means聚类是一种**迭代算法**，首先随机选取k个初始中心点，然后**将每个点分配到最近的中心点形成的簇**中，接着重新计算每个簇的中心点，重复此过程直到中心点不再变化或达到预设的迭代次数。

6. 训练集、测试集、验证集的作用？

   > 数据集的作用：**训练集用于训练模型**，**验证集用于调整超参数**并在训练过程中提供无偏评估，**测试集**用于评估模型的**泛化性能**
   >
   > 1. **评估偏差**: 如果我们**直接根据测试集调整超参数，那么测试集就不再是独立的评估数据集**。这意味着我们无法准确地估计模型在新数据上的性能，因为测试集已经被用来调整模型了。

7. 请讲解一下k折交叉验证？

   > 在 k 折叠交叉验证中，数据被划分为 k 个子集。每次，将 k 个子集中的一个用作测试集，并将其他 k-1 子集放在一起形成一个训练集。**该过程重复 k 次，每个 k 个子集恰好用作测试集一次**。

8. 分类和聚类的区别？

   > 分类是**监督学习任务**，需要**标注数据**来训练模型；聚类是**无监督学习**任务，不需要标注数据，目的是发现数据**内在的结构或模式。**

9. 描述一下梯度的概念？

   > 梯度是导数的多变量推广，表示**函数最快增长的方向和速率**。在优化中，它用于查找函数减小最快的方向。
   >
   > eg:
   >
   > 例如，考虑二元函数 f(x, y) = x^2 + y^2。我们可以计算它的梯度：
   >
   > 1. 计算偏导数：∂f/∂x = 2x，∂f/∂y = 2y
   > 2. 在点 (x0, y0) = (1, 1) 处，梯度向量为：∇f(1, 1) = [2*1, 2*1] = [2, 2]
   >
   > 因此，在点 (1, 1) 处，梯度向量指向 [2, 2] 方向，且函数在该方向上的增长率最大，为 √(2^2 + 2^2) = √8 ≈ 2.83。这意味着在点 (1, 1) 处，函数 f(x, y) = x^2 + y^2 沿着 [2, 2] 方向增长最快，且每单位距离增长 2.83。

10. 有监督学习、无监督学习和半监督学习的区别？

    > 见 1.1 节

11. 带核的SVM为什么能分类非线性问题？

    > 非线性问题的 SVM：SVM 可以通过使用内核技巧**将输入空间转换为更高维的空间**来对非线性问题进行分类，从而更容易线性分离数据。

12. 请描述常见的梯度下降方法？

    > 常用的方法包括批量梯度下降、随机梯度下降 （SGD） 和小批量梯度下降，用于计算损失函数梯度的数据量不同。
    >
    > 批量梯度下降、随机梯度下降（SGD）和小批量梯度下降是梯度下降优化算法的三种不同实现方式，它们在机器学习和深度学习中被广泛使用来优化模 型的参数。**具体介绍如下**：
    >
    > - **批量梯度下降**：它在每次迭代时**使用所有的训练样本来计算梯度**并更新模型参数。这种方法可以确保每次迭代都沿着全局最优方向更新，因此收敛曲线通常比较平滑。然而，当数据集非常大时，每次迭代的计算成本会非常高，导致训练速度变慢。
    > - **随机梯度下降**：它在**每次迭代时只使用一个训练样本来计算梯度并更新模型参数**。这种方法的优点是计算速度快，可以快速进行模型更新，特别适用于大数据集。但是，由于每次更新只基于一个样本，可能会导致收敛过程波动较大，不一定能收敛到全局最优解。
    > - **小批量梯度下降**：它**介于批量梯度下降和随机梯度下降之间**，每次迭代**使用一小部分训练样本**（即小批量）来计算梯度并更新模型参数。这种方法既保留了批量梯度下降的稳定收敛特性，又具有随机梯度下降的快速计算优势。因此，小批量梯度下降在实践中被广泛应用，尤其是在深度学习模型的训练中。
    >
    > 总的来说，这三种方法都是梯度下降的变体，目的是通过迭代更新模型参数以最小化损失函数。批量梯度下降使用所有数据，收敛稳定但计算成本高；随机梯度下降使用单个样本，计算快但可能波动大；小批量梯度下降折中了前两者的特点，使用部分数据，既稳定又相对高效。

13. Adam、RMSprop、Adagrad、Momentum优化算法？

    > **Adam** 将 **RMSprop 和 Stochastic Gradient Descent 的思想与动量相结合**。它动态调整每个参数的学习率。RMSprop 在训练期间调整学习率，而 Adagrad 通过将学习率缩放为与梯度的所有过去平方值的平方根成反比来调整学习率。动量有助于加速新元朝相关方向发展。
    >
    > 此方法是在**梯度下降的基础上加入了“惯性”的概念**，即考虑了历史梯度对当前梯度的影响，使得更新过程具有一定的连续性，并能够加快学习速度，减少振荡。Momentum 通过使用动量项来对网络参数进行平滑处理，有助于使梯度的摆动幅度变小，从而加快收敛速度。

14. 什么是过拟合？怎么解决过拟合问题？

    > 过拟合是指模型在训练数据上表现良好但在新数据上泛化能力差的现象。
    >
    > **解决过拟合的方法**包括增加**数据量**、减少模型**复杂度**、使用**正则化**技术和集成学习方法等。

15. 怎样解决梯度消失/爆炸问题？

    > **含义**：
    >
    > 1. **梯度消失**：在深度神经网络的反向传播过程中，梯度可能会因为多层连续的乘法操作而变得非常小，以至于权重几乎不会被更新，导致训练过程提前停止。这种现象通常发生在激活函数选择不当（如使用sigmoid或tanh函数）且网络层次较深的情况下。
    > 2. **梯度爆炸**：与梯度消失相反，梯度爆炸是指梯度值变得非常大，以至于导致模型参数更新过于频繁和巨大，从而使得模型无法收敛到一个稳定的解。梯度爆炸通常是由于梯度值在反向传播过程中连续乘以较大的数而累积起来的。
    >
    > 如何解决？
    >
    > - **选择合适的激活函数**：**避免**使用**容易饱和**的激活函数，如sigmoid或tanh，转而使用ReLU（Rectified Linear Units）及其变种Leaky ReLU、Parametric ReLU等，它们在输入值较大时不会饱和，有助于缓解梯度消失问题。
    > - **批量归一化（Batch Normalization）**：通过对每一层的输入进行归一化处理，保持输入数据的均值为0、方差为1，有助于稳定梯度变化，防止梯度消失和爆炸。
    > - **残差连接（Residual Connections）**：在深度网络中使用残差连接，即让前一层的输出直接加到后续某层的输入上，可以保证梯度能够绕过一些层直接传回，从而缓解梯度消失问题。
    > - **梯度裁剪（Gradient Clipping）**：通过设置一个阈值来限制梯度的最大值，可以有效防止梯度爆炸。
    > - **优化器选择**：使用具有自适应学习率调整能力的优化器，如Adam、RMSprop等，可以在不同情况下自动调整学习率，减少梯度消失或爆炸的风险。
    > - **短程记忆结构**：对于循环神经网络（RNN），可以**使用长短时记忆（LSTM）**或门控循环单元（GRU）这类结构来代替传统的RNN结构，因为它们能够更好地捕捉长距离依赖关系，从而缓解梯度消失问题。
    > - **正则化技术**：如L1/L2正则化，也可以帮助控制模型参数的规模，间接地减小梯度爆炸的可能性。

16. 讲解下神经网络反向传播算法？

    > **反向传播算法**：反向传播是人工神经网络中使用的一种方法，用于计算单个输入输出示例的损失函数相对于网络权重的梯度，从而使用梯度下降更新权重。

17. 激活函数的作用是什么？有哪些激活函数？它们的表达式分别是？

    > 激活函数作用：引入**非线性**因素，使得神经网络可以学习和模拟任何复杂的函数和数据分布，这样网络就能处理非线性问题。如果**没有激活函数，无论神经网络有多少层，输出都只是输入的线性组合**，这大大限制了网络的表达能力和复杂度。
    >
    > 常见的激活函数包括 Sigmoid、Tanh、ReLU：（需要去了解一下基本原理）

18. 请讲解一下正则化的概念？L1正则化是什么？L2正则化是什么？

    > 正则化是机器学习和统计学中用于**防止模型过拟合**的一种技术。过拟合是指模型在训练数据上表现得很好，但在未见过的新数据上表现不佳的现象，通常是因为模型过于复杂，学习到了训练数据中的噪声和细节，而不是潜在的数据生成规律。**正则化通过在模型训练的损失函数中添加一个额外的惩罚项来解决这个问题，该惩罚项会惩罚模型的复杂度**
    >
    > - **L1正则化**（也称为Lasso正则化）通过向损失函数添加参数的绝对值之和作为惩罚项来工作。L1正则化倾向于产生稀疏参数，即许多参数会变成零，这可以用于特征选择。
    > - **L2正则化**（也称为Ridge正则化）通过向损失函数添加参数的平方和作为惩罚项来工作。与L1正则化不同，L2正则化倾向于使参数非常小但不完全为零，从而保持所有特征但减少其影响。
    > - 在某些情况下，人们可能会**同时使用L1和L2正则化**，这种方法被称为**弹性网络**正则化。
    > - 正则化技术不仅限于线性模型，它们也广泛应用于神经网络、决策树和其他类型的机器学习模型中。在神经网络中，除了L1和L2正则化外，还有如**Dropout**正则化等特殊形式的正则化技术，它通过在训练过程中随机丢弃一部分神经元来防止过拟合。

19. 除了BatchNormalization还有其他什么方法来加速模型训练吗？

    > 1. 使用**半字训练（半精度浮点数）**
    > 2. **合理的超参数设计**：选择合适的batch size、epoch数量和学习率策略对于加速模型训练至关重要。较小的batch size可以加快单次迭代的速度，而适当的epoch数量和学习率调整可以帮助模型更快地收敛。
    > 3. **增大batch size**：虽然增大batch size可以提高训练速度，但过大的batch size可能会影响模型的泛化性能和收敛速度。因此，需要根据具体情况选择合适的batch size大小。
    > 4. **多GPU分布式训练**：通过使用多个GPU进行模型并行或数据并行训练，可以显著提高模型的训练速度。模型并行是将模型的不同部分分配给不同的计算设备，而数据并行则是将数据集分割成多个子集，每个子集在不同的计算设备上进行处理。
    > 5. **模型压缩与加速技术**：包括参数剪枝、参数量化、紧凑网络设计、知识蒸馏、低秩分解、参数共享等方法。这些技术旨在减少模型的大小和复杂性，从而提高训练和推理的速度。
    > 6. **使用预训练模型**：在相似任务上使用预训练模型可以显著减少训练时间，因为预训练模型已经学习了大量的特征，只需要对顶层进行微调即可。

20. 机器学习中，为何要经常对数据进行预处理？

    > 处理噪声、缺失值、降维、结构化非结构化处理
    >
    > 1. **处理缺失值与噪声**：数据中可能存在缺失值，预处理可以包括填补或删除这些缺失值，以确保数据的完整性。
    > 2. **无量纲化**：不同特征可能具有不同的量纲和规模，无量纲化处理可以消除这种差异，使得模型能够更公平地评估每个特征的重要性。
    > 3. **数据规范化**：通过最值归一化或Z-Score规范化等方法，将数据缩放到特定的范围内，有助于加快模型的收敛速度。
    > 4. **特征工程**：数据预处理是特征工程的一部分，它包括数据清洗、特征提取、特征选择和特征构造等子问题。通过这些步骤，可以创建出能够使机器学习算法达到最佳性能的特征。
    > 5. **提高模型性能**：干净且经过恰当处理的数据可以帮助算法更准确地学习模式和关系，从而提高模型的性能。
    > 6. **增强模型泛化能力**：通过正则化等预处理方法，可以防止模型过拟合，提高其在新数据上的泛化能力。